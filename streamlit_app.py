#!/usr/bin/env python3
"""
ğŸ¢ ä¼æ¥­ãƒ“ã‚¸ãƒã‚¹åˆ†æã‚·ã‚¹ãƒ†ãƒ  - Streamlit Webç‰ˆ
EVPæ©Ÿèƒ½ã‚’å‰Šé™¤ã—ã€ä¼æ¥­ã®ãƒ“ã‚¸ãƒã‚¹åˆ†æã«ç‰¹åŒ–ã—ãŸAIã‚·ã‚¹ãƒ†ãƒ 
"""

import streamlit as st
import os
import json
import time
import requests
from pathlib import Path
from openai import OpenAI
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from datetime import datetime, timedelta
import PyPDF2
import pdfplumber
import io

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ğŸ¢ ä¼æ¥­ãƒ“ã‚¸ãƒã‚¹åˆ†æã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# è¨­å®šå®šæ•°
CONFIG = {
    'MAX_CRAWL_DEPTH': 4,  # 4éšå±¤ç¶­æŒï¼ˆé‡è¦è³‡æ–™ã‚¢ã‚¯ã‚»ã‚¹ï¼‰
    'DATE_LIMIT_YEARS': 3,
    'MAX_SOURCES': 12,  # 10â†’12ã«å¾®å¢—
    'MAX_CONTENT_LENGTH': 50000,  # 100000â†’50000æ–‡å­—ï¼ˆãƒãƒ©ãƒ³ã‚¹å‹ï¼‰
    'TIME_LIMIT_SECONDS': 180,  # 3åˆ†åˆ¶é™ã‚’è¿½åŠ 
    'PDF_PAGES_LIMIT': 10,  # PDFå‡¦ç†ãƒšãƒ¼ã‚¸åˆ¶é™
    'USER_AGENT': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
}

class SearchBasedIRCollector:
    """SerpAPIæ¤œç´¢ãƒ™ãƒ¼ã‚¹ã®IRæƒ…å ±åé›†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, company_name):
        self.company_name = company_name
        self.session = requests.Session()
        self.session.headers.update({'User-Agent': CONFIG['USER_AGENT']})
        self.start_time = None  # å‡¦ç†æ™‚é–“åˆ¶é™ç”¨
    
    def smart_content_filter(self, content):
        """é‡è¦æƒ…å ±ã‚’å„ªå…ˆçš„ã«æŠ½å‡ºã™ã‚‹ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚£ãƒ«ã‚¿"""
        if not content:
            return content
            
        # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå„ªå…ˆåº¦é †ï¼‰
        priority_keywords = [
            # è²¡å‹™ãƒ»æ¥­ç¸¾é–¢é€£ï¼ˆæœ€é‡è¦ï¼‰
            'å£²ä¸Šé«˜', 'å–¶æ¥­åˆ©ç›Š', 'ç´”åˆ©ç›Š', 'å½“æœŸç´”åˆ©ç›Š', 'å£²ä¸Š', 'åˆ©ç›Š', 'åç›Š',
            'æ¥­ç¸¾', 'æ±ºç®—', 'è²¡å‹™', 'æç›Š', 'EBITDA', 'ROE', 'ROA',
            
            # å¸‚å ´ãƒ»äº‹æ¥­é–¢é€£
            'å¸‚å ´ã‚·ã‚§ã‚¢', 'å¸‚å ´è¦æ¨¡', 'ç«¶åˆ', 'äº‹æ¥­ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ', 'äº‹æ¥­ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ª',
            'æˆé•·ç‡', 'å‰å¹´åŒæœŸæ¯”', 'å‰å¹´æ¯”', 'YoY', 'QoQ',
            
            # æˆ¦ç•¥ãƒ»å±•æœ›é–¢é€£
            'æˆ¦ç•¥', 'æ–¹é‡', 'è¨ˆç”»', 'å±•æœ›', 'äºˆæƒ³', 'è¦‹é€šã—', 'ç›®æ¨™',
            'DX', 'ãƒ‡ã‚¸ã‚¿ãƒ«å¤‰é©', 'AI', 'ãƒ‡ãƒ¼ã‚¿æ´»ç”¨'
        ]
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ®µè½ã«åˆ†å‰²
        paragraphs = content.split('\n')
        
        # å„æ®µè½ã«ã‚¹ã‚³ã‚¢ã‚’ä»˜ä¸
        scored_paragraphs = []
        for paragraph in paragraphs:
            if len(paragraph.strip()) < 20:  # çŸ­ã™ãã‚‹æ®µè½ã¯é™¤å¤–
                continue
                
            score = 0
            paragraph_lower = paragraph.lower()
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ã§ã‚¹ã‚³ã‚¢è¨ˆç®—
            for i, keyword in enumerate(priority_keywords):
                if keyword in paragraph_lower:
                    # æ—©æœŸã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã»ã©é«˜ã‚¹ã‚³ã‚¢
                    score += (len(priority_keywords) - i) * 2
            
            # æ•°å€¤ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹æ®µè½ã¯è¿½åŠ ãƒã‚¤ãƒ³ãƒˆ
            if any(char.isdigit() for char in paragraph):
                score += 10
            
            # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã‚„å††è¡¨è¨˜ãŒã‚ã‚‹å ´åˆã¯è¿½åŠ ãƒã‚¤ãƒ³ãƒˆ
            if '%' in paragraph or 'å††' in paragraph or 'å„„' in paragraph or 'å…†' in paragraph:
                score += 15
                
            scored_paragraphs.append((score, paragraph))
        
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        scored_paragraphs.sort(key=lambda x: x[0], reverse=True)
        
        # ä¸Šä½ã®æ®µè½ã‚’çµåˆã—ã¦è¿”ã™
        filtered_content = '\n'.join([para for score, para in scored_paragraphs])
        
        # æ–‡å­—æ•°åˆ¶é™ã‚’é©ç”¨
        if len(filtered_content) > CONFIG['MAX_CONTENT_LENGTH']:
            filtered_content = filtered_content[:CONFIG['MAX_CONTENT_LENGTH']] + '...'
            
        return filtered_content
    
    def format_text_for_display(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿ã‚„ã™ãæ•´å½¢ï¼ˆæ”¹è¡Œé‡è¦–ãƒ»æ§‹é€ åŒ–ï¼‰"""
        if not text or len(text.strip()) == 0:
            return text
            
        # å¥ç‚¹ã§ã®åˆ†å‰²ã‚’åŸºæœ¬ã«ã—ã¦æ®µè½ã‚’ä½œæˆ
        sentences = text.split('ã€‚')
        formatted_sentences = []
        
        for i, sentence in enumerate(sentences):
            sentence = sentence.strip()
            if len(sentence) == 0:
                continue
                
            # å¥ç‚¹ã‚’å¾©å…ƒ
            if not sentence.endswith('ã€‚') and i < len(sentences) - 1:
                sentence += 'ã€‚'
            
            formatted_sentences.append(sentence)
            
            # 2-3æ–‡ã”ã¨ã«æ”¹è¡Œã‚’æŒ¿å…¥ï¼ˆèª­ã¿ã‚„ã™ã•é‡è¦–ï¼‰
            if (i + 1) % 2 == 0 and i < len(sentences) - 2:
                formatted_sentences.append('\n')
        
        # é‡è¦ãªæ•°å€¤ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ
        formatted_text = '\n'.join(formatted_sentences)
        return self.highlight_important_info(formatted_text)
    
    def highlight_important_info(self, text):
        """é‡è¦ãªæ•°å€¤ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å¤ªå­—ã§ãƒã‚¤ãƒ©ã‚¤ãƒˆ"""
        import re
        
        # æ•°å€¤é–¢é€£ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆï¼ˆæ”¹è¡Œè€ƒæ…®ã‚’å¼·åŒ–ï¼‰
        text = re.sub(r'(\d+(?:,\d{3})*å„„å††)', r'**\1**', text)  # é‡‘é¡
        text = re.sub(r'(\d+(?:,\d{3})*å…†å††)', r'**\1**', text)  # å¤§ããªé‡‘é¡
        text = re.sub(r'(\d+\.?\d*%)', r'**\1**', text)  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸
        text = re.sub(r'(å‰å¹´(?:åŒæœŸ)?æ¯”[+-]?\d+\.?\d*%)', r'**\1**', text)  # æˆé•·ç‡
        text = re.sub(r'(å£²ä¸Šé«˜\d+)', r'**\1**', text)  # å£²ä¸Š
        
        # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆ
        important_keywords = [
            'å£²ä¸Šé«˜', 'å–¶æ¥­åˆ©ç›Š', 'ç´”åˆ©ç›Š', 'å½“æœŸç´”åˆ©ç›Š', 'EBITDA',
            'å¸‚å ´ã‚·ã‚§ã‚¢', 'ã‚·ã‚§ã‚¢', 'å¸‚å ´è¦æ¨¡', 'æˆé•·ç‡',
            'å¾“æ¥­å“¡æ•°', 'å£²ä¸Šæ§‹æˆæ¯”', 'åˆ©ç›Šç‡', 'ROE', 'ROA'
        ]
        
        for keyword in important_keywords:
            # å˜èªå¢ƒç•Œã‚’è€ƒæ…®ã—ã¦ãƒã‚¤ãƒ©ã‚¤ãƒˆ
            text = re.sub(f'({re.escape(keyword)})', r'**\1**', text)
        
        return text
    
    def display_formatted_analysis(self, analysis_data):
        """åˆ†æçµæœã‚’æ§‹é€ åŒ–ã—ã¦ç¾ã—ãè¡¨ç¤º"""
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³å®šç¾©ï¼ˆã‚¢ã‚¤ã‚³ãƒ³ + æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«ï¼‰
        sections = [
            ("ğŸ“Š", "æ¥­ç•Œãƒ»å¸‚å ´åˆ†æ", "industry_market", "å¸‚å ´ç’°å¢ƒã€æ¥­ç•Œå‹•å‘ã€æˆé•·æ€§ã«é–¢ã™ã‚‹åˆ†æ"),
            ("ğŸ¯", "å¸‚å ´ãƒã‚¸ã‚·ãƒ§ãƒ³", "market_position", "ç«¶åˆæ¯”è¼ƒã€å¸‚å ´ã‚·ã‚§ã‚¢ã€ç«¶äº‰å„ªä½æ€§"),  
            ("ğŸ’¡", "å·®åˆ¥åŒ–è¦å› ", "differentiation", "ç‹¬è‡ªã®å¼·ã¿ã€æŠ€è¡“å„ªä½æ€§ã€ãƒ–ãƒ©ãƒ³ãƒ‰ä¾¡å€¤"),
            ("ğŸ¢", "äº‹æ¥­ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ª", "business_portfolio", "äº‹æ¥­æ§‹æˆã€åç›Šæ§‹é€ ã€æˆé•·æˆ¦ç•¥")
        ]
        
        for icon, title, key, description in sections:
            content = analysis_data.get(key, '')
            
            if content and len(content.strip()) > 0:
                # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼
                st.markdown(f"## {icon} {title}")
                st.markdown(f"*{description}*")
                st.markdown("")  # ç©ºè¡Œè¿½åŠ 
                
                # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ•´å½¢ã—ã¦è¡¨ç¤º
                formatted_content = self.format_text_for_display(content)
                st.markdown(formatted_content)
                
                # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åŒºåˆ‡ã‚Š
                st.markdown("---")
                st.markdown("")  # åŒºåˆ‡ã‚Šå¾Œã®ç©ºè¡Œ
            else:
                # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒãªã„å ´åˆ
                st.markdown(f"## {icon} {title}")
                st.info(f"{title}ã®æƒ…å ±ã¯åé›†ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                st.markdown("---")
                st.markdown("")
    
    def get_serpapi_key(self):
        """SerpAPIã‚­ãƒ¼å–å¾—ï¼ˆæœ¬ç•ªç’°å¢ƒå¯¾å¿œï¼‰"""
        # ç’°å¢ƒå¤‰æ•°ã‚’æœ€å„ªå…ˆã§ãƒã‚§ãƒƒã‚¯
        env_key = os.getenv("SERPAPI_KEY")
        if env_key and len(env_key) > 10:
            return env_key
        
        # Streamlit Cloud ã®Secretsæ©Ÿèƒ½
        if hasattr(st, 'secrets') and "SERPAPI_KEY" in st.secrets:
            key = st.secrets["SERPAPI_KEY"]
            # ãƒ†ã‚¹ãƒˆå€¤ã‚„ç„¡åŠ¹ãªå€¤ã§ãªã„ã“ã¨ã‚’ç¢ºèª
            if key and key != "your-actual-serpapi-key-here" and len(key) > 10 and not key.startswith("test"):
                return key
        
        # SerpAPIæœªè¨­å®šæ™‚ã®æ˜ç¢ºãªé€šçŸ¥ï¼ˆã‚¨ãƒ©ãƒ¼ã§ã¯ãªãæƒ…å ±ï¼‰
        return None
    
    def search_with_serpapi(self, query, api_key):
        """SerpAPIã‚’ä½¿ç”¨ã—ãŸæ¤œç´¢å®Ÿè¡Œ"""
        url = "https://serpapi.com/search"
        params = {
            "q": query,
            "api_key": api_key,
            "engine": "google",
            "num": 5,  # ç„¡æ–™æ ç¯€ç´„
            "hl": "ja",  # æ—¥æœ¬èª
            "gl": "jp"   # æ—¥æœ¬åœ°åŸŸ
        }
        
        try:
            response = requests.get(url, params=params, timeout=15)
            
            if response.status_code == 200:
                result = response.json()
                # ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯
                if 'error' in result:
                    st.warning(f"SerpAPI Error: {result['error']}")
                    return {'error': result['error']}
                return result
            elif response.status_code == 401:
                st.error("âŒ SerpAPIã‚­ãƒ¼ãŒç„¡åŠ¹ã§ã™ã€‚Secretsè¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                return {'error': 'Invalid API Key'}
            elif response.status_code == 429:
                st.warning("âš ï¸ SerpAPIä½¿ç”¨åˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚")
                return {'error': 'Rate limit exceeded'}
            else:
                st.warning(f"SerpAPI HTTP Error: {response.status_code}")
                return {'error': f'HTTP {response.status_code}'}
                
        except requests.exceptions.Timeout:
            st.warning("âš ï¸ SerpAPIæ¥ç¶šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
            return {'error': 'Timeout'}
        except requests.exceptions.ConnectionError:
            st.warning("âš ï¸ SerpAPIã«æ¥ç¶šã§ãã¾ã›ã‚“")
            return {'error': 'Connection Error'}
        except requests.exceptions.RequestException as e:
            st.warning(f"âš ï¸ SerpAPIãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {'error': str(e)}
        except Exception as e:
            st.warning(f"âš ï¸ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {'error': str(e)}
    
    def search_ir_information(self):
        """IRé–¢é€£æƒ…å ±ã‚’æ¤œç´¢ãƒ™ãƒ¼ã‚¹ã§åé›†"""
        import time
        self.start_time = time.time()  # å‡¦ç†é–‹å§‹æ™‚é–“ã‚’è¨˜éŒ²
        
        serpapi_key = self.get_serpapi_key()
        if not serpapi_key:
            st.info("ğŸ” SerpAPIã‚­ãƒ¼ãŒæœªè¨­å®šã®ãŸã‚ã€OpenAI APIã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã§åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™")
            return []
        
        # IRé–¢é€£æ¤œç´¢ã‚¯ã‚¨ãƒª
        search_queries = [
            f"{self.company_name} IR æŠ•è³‡å®¶å‘ã‘æƒ…å ±",
            f"{self.company_name} æ±ºç®— æ¥­ç¸¾ è²¡å‹™",
            f"{self.company_name} æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸",
            f"{self.company_name} äº‹æ¥­å ±å‘Š å¹´æ¬¡å ±å‘Šæ›¸"
        ]
        
        collected_data = []
        successful_searches = 0
        
        for query in search_queries:
            # æ™‚é–“åˆ¶é™ãƒã‚§ãƒƒã‚¯
            if time.time() - self.start_time > CONFIG['TIME_LIMIT_SECONDS']:
                st.warning(f"â±ï¸ æ™‚é–“åˆ¶é™({CONFIG['TIME_LIMIT_SECONDS']}ç§’)ã«é”ã—ãŸãŸã‚å‡¦ç†ã‚’åœæ­¢ã—ã¾ã—ãŸ")
                break
                
            try:
                st.info(f"ğŸ” æ¤œç´¢ä¸­: {query}")
                search_results = self.search_with_serpapi(query, serpapi_key)
                
                if search_results and 'organic_results' in search_results:
                    successful_searches += 1
                    for result in search_results['organic_results'][:2]:  # ä¸Šä½2ä»¶ã®ã¿
                        # æ™‚é–“åˆ¶é™ãƒã‚§ãƒƒã‚¯
                        if time.time() - self.start_time > CONFIG['TIME_LIMIT_SECONDS']:
                            st.warning(f"â±ï¸ æ™‚é–“åˆ¶é™ã«é”ã—ãŸãŸã‚ã€æ®‹ã‚Šã®å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                            break
                            
                        url = result.get('link', '')
                        title = result.get('title', '')
                        snippet = result.get('snippet', '')
                        
                        # IRé–¢é€£URLã‹ãƒã‚§ãƒƒã‚¯
                        if self.is_ir_related_url(url, title):
                            # Webãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’å–å¾—ï¼ˆæ‹¡å¼µç‰ˆï¼‰
                            content = self.fetch_webpage_content(url)
                            if content:
                                # ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’é©ç”¨
                                filtered_content = self.smart_content_filter(content)
                                collected_data.append({
                                    'url': url,
                                    'content': filtered_content,  # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
                                    'title': title,
                                    'snippet': snippet,
                                    'search_query': query
                                })
                                st.success(f"âœ… IRæƒ…å ±ã‚’å–å¾—: {title}")
                elif search_results and 'error' in search_results:
                    st.warning(f"âš ï¸ æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {search_results.get('error', 'Unknown error')}")
                else:
                    st.info(f"ğŸ“Š æ¤œç´¢çµæœ: {query}")
                
                time.sleep(1)  # APIåˆ¶é™å›é¿
                
            except requests.exceptions.Timeout:
                st.warning(f"âš ï¸ æ¤œç´¢ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {query}")
                continue
            except requests.exceptions.RequestException as e:
                st.warning(f"âš ï¸ æ¤œç´¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}")
                continue
            except Exception as e:
                st.warning(f"âš ï¸ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}")
                continue
        
        if collected_data:
            st.success(f"ğŸ“Š {len(collected_data)}ä»¶ã®IRæƒ…å ±ã‚’æ¤œç´¢ã§åé›†ã—ã¾ã—ãŸï¼ˆ{successful_searches}/{len(search_queries)}ä»¶ã®æ¤œç´¢ãŒæˆåŠŸï¼‰")
        elif successful_searches > 0:
            st.info("ğŸ” æ¤œç´¢ã¯æˆåŠŸã—ã¾ã—ãŸãŒã€IRé–¢é€£ã®æœ‰ç”¨ãªæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ä¸€èˆ¬çš„ãªå…¬é–‹æƒ…å ±ã§åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
        else:
            st.warning("âš ï¸ æ¤œç´¢ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ä¸€èˆ¬çš„ãªå…¬é–‹æƒ…å ±ã§åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
        
        return collected_data
    
    def is_ir_related_url(self, url, title):
        """IRé–¢é€£URLã‹ã©ã†ã‹ã‚’åˆ¤å®šï¼ˆPDFå«ã‚€ï¼‰"""
        ir_keywords = ['ir', 'investor', 'æŠ•è³‡å®¶', 'æ±ºç®—', 'æ¥­ç¸¾', 'è²¡å‹™', 'æœ‰ä¾¡è¨¼åˆ¸', 'å¹´æ¬¡å ±å‘Š', 
                      'pdf', 'å ±å‘Šæ›¸', 'report', 'financial', 'annual', 'quarterly']
        url_lower = url.lower()
        title_lower = title.lower()
        
        return any(keyword in url_lower or keyword in title_lower for keyword in ir_keywords)
    
    def fetch_webpage_content(self, url, depth=0):
        """Webãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’å–å¾—ï¼ˆPDFå¯¾å¿œãƒ»å¤šéšå±¤ã‚¯ãƒ­ãƒ¼ãƒ«ï¼‰"""
        try:
            response = self.session.get(url, timeout=10)
            if response.status_code == 200:
                content_type = response.headers.get('content-type', '').lower()
                
                # PDFå‡¦ç†
                if 'pdf' in content_type:
                    return self.extract_pdf_content(response.content)
                
                # HTMLå‡¦ç†
                soup = BeautifulSoup(response.text, 'html.parser')
                text_content = soup.get_text()
                content = ' '.join(text_content.split())
                
                # å¤šéšå±¤ã‚¯ãƒ­ãƒ¼ãƒ«: æ·±åº¦ãŒåˆ¶é™å†…ã§ãƒªãƒ³ã‚¯ã‚’åé›†
                if depth < CONFIG['MAX_CRAWL_DEPTH']:
                    sub_content = self.crawl_subpages(soup, url, depth + 1)
                    content += sub_content
                
                # æ–‡å­—æ•°åˆ¶é™ã‚’é©ç”¨
                return content[:CONFIG['MAX_CONTENT_LENGTH']]
            else:

                return None
        except requests.exceptions.Timeout:

            return None
        except requests.exceptions.RequestException as e:

            return None
        except Exception as e:

            return None
    
    def extract_pdf_content(self, pdf_content):
        """PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        try:
            # pdfplumberã‚’å„ªå…ˆä½¿ç”¨ï¼ˆãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæƒ…å ±ã‚’ä¿æŒï¼‰
            with io.BytesIO(pdf_content) as pdf_stream:
                with pdfplumber.open(pdf_stream) as pdf:
                    text = ""
                    for page in pdf.pages[:CONFIG['PDF_PAGES_LIMIT']]:  # CONFIGè¨­å®šã«å¾“ã†
                        if page.extract_text():
                            text += page.extract_text() + "\n"
                    
                    if text.strip():
                        return ' '.join(text.split())[:CONFIG['MAX_CONTENT_LENGTH']]
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: PyPDF2ã‚’ä½¿ç”¨
            with io.BytesIO(pdf_content) as pdf_stream:
                pdf_reader = PyPDF2.PdfReader(pdf_stream)
                text = ""
                for page_num in range(min(len(pdf_reader.pages), 20)):
                    page = pdf_reader.pages[page_num]
                    text += page.extract_text() + "\n"
                
                return ' '.join(text.split())[:CONFIG['MAX_CONTENT_LENGTH']]
                
        except Exception as e:

            return None
    
    def crawl_subpages(self, soup, base_url, current_depth):
        """ã‚µãƒ–ãƒšãƒ¼ã‚¸ã‚’å†å¸°çš„ã«ã‚¯ãƒ­ãƒ¼ãƒ«"""
        if current_depth >= CONFIG['MAX_CRAWL_DEPTH']:
            return ""
        
        sub_content = ""
        ir_links = []
        
        # IRé–¢é€£ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            if href:
                full_url = urljoin(base_url, href)
                link_text = link.get_text().strip()
                
                # IRé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
                if self.is_ir_related_url(full_url, link_text) and full_url not in ir_links:
                    ir_links.append(full_url)
                    
                    if len(ir_links) >= 5:  # å„éšå±¤ã§æœ€å¤§5ãƒªãƒ³ã‚¯
                        break
        
        # ã‚µãƒ–ãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’å–å¾—
        for link_url in ir_links:
            time.sleep(0.5)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            subcontent = self.fetch_webpage_content(link_url, current_depth)
            if subcontent:
                sub_content += f"\n[ã‚µãƒ–ãƒšãƒ¼ã‚¸ {current_depth}éšå±¤]: {subcontent[:5000]}"  # å„ã‚µãƒ–ãƒšãƒ¼ã‚¸5000æ–‡å­—ã¾ã§
        
        return sub_content

class BusinessAnalyzer:
    """ä¼æ¥­ãƒ“ã‚¸ãƒã‚¹åˆ†æã‚·ã‚¹ãƒ†ãƒ ï¼ˆäº‹æ¥­åˆ†æç‰¹åŒ–ï¼‰"""
    
    def __init__(self):
        self.client = OpenAI(api_key=self._get_api_key())
        
    def _get_api_key(self):
        """APIã‚­ãƒ¼ã‚’å–å¾—"""
        # Streamlit Cloudã®å ´åˆ
        if hasattr(st, 'secrets') and 'OPENAI_API_KEY' in st.secrets:
            return st.secrets["OPENAI_API_KEY"]
        
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            st.error("âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            st.stop()
        
        return api_key
    
    def create_analysis_prompt(self, company_name, ir_data=None):
        """çµ±ä¸€ã•ã‚ŒãŸåˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆäº‹æ¥­åˆ†æã®ã¿ï¼‰"""
        
        ir_content = ""
        sources_list = []
        if ir_data:
            ir_content = "\n".join([
                f"ã€IRæƒ…å ±æºã€‘: {item['title']}\nå‡ºå…¸URL: {item['url']}\nå†…å®¹: {item['content'][:2400]}...\n"
                for item in ir_data[:3]
            ])
            sources_list = [item['url'] for item in ir_data[:3]]
        
        prompt = f"""
ä»¥ä¸‹ã®ä¼æ¥­ã«ã¤ã„ã¦äº‹æ¥­åˆ†æã‚’å®Ÿè¡Œã—ã€å¿…ãšæœ‰åŠ¹ãªJSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚

ä¼æ¥­å: {company_name}

åˆ©ç”¨å¯èƒ½ãªæƒ…å ±:
{ir_content if ir_content else f"ã€{company_name}ã€‘ã®ä¸€èˆ¬çš„ãªå…¬é–‹æƒ…å ±ãƒ»çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«åŸºã¥ãåŒ…æ‹¬çš„åˆ†æ"}

ã€é‡è¦ãªåˆ†æè¦æ±‚ã€‘:
- å„é …ç›®ã§2400æ–‡å­—ç¨‹åº¦ã®è©³ç´°åˆ†æã‚’å®Ÿæ–½ã—ã¦ãã ã•ã„
- ã‚ãªãŸã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å…·ä½“çš„ãªæ•°å€¤ãƒ‡ãƒ¼ã‚¿ï¼ˆå£²ä¸Šã€åˆ©ç›Šã€å¾“æ¥­å“¡æ•°ã€å¸‚å ´ã‚·ã‚§ã‚¢ç­‰ï¼‰ã‚’å¿…ãšå«ã‚ã¦ãã ã•ã„
- ç«¶åˆä»–ç¤¾ã¨ã®æ¯”è¼ƒã‚’å®šé‡çš„ã«è¡Œã£ã¦ãã ã•ã„
- éå»3å¹´é–“ã®ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã‚’å«ã‚ã¦ãã ã•ã„
- å°†æ¥äºˆæ¸¬ã¨æˆ¦ç•¥çš„ç¤ºå”†ã‚’å«ã‚ã¦ãã ã•ã„
- IRæƒ…å ±ãŒç„¡ã„å ´åˆã§ã‚‚ã€ã‚ãªãŸã®çŸ¥è­˜ã‹ã‚‰æœ€æ–°ã®ä¼æ¥­æƒ…å ±ã‚’æ´»ç”¨ã—ã¦ãã ã•ã„

ä»¥ä¸‹ã®æ­£ç¢ºãªJSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„:

{{
  "business_analysis": {{
    "industry_market": "æ¥­ç•Œãƒ»å¸‚å ´åˆ†æã®è©³ç´°ï¼ˆ2400æ–‡å­—ç¨‹åº¦ï¼‰- å¸‚å ´è¦æ¨¡ã€æˆé•·ç‡ã€ä¸»è¦ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã€ãƒˆãƒ¬ãƒ³ãƒ‰ã€å°†æ¥äºˆæ¸¬ã‚’å«ã‚€åŒ…æ‹¬çš„åˆ†æã€‚å…·ä½“çš„ãªæ•°å€¤ã¨çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚ã‚‹ã“ã¨ã€‚",
    "market_position": "æ¥­ç•Œå†…ãƒã‚¸ã‚·ãƒ§ãƒ³ã®åˆ†æï¼ˆ2400æ–‡å­—ç¨‹åº¦ï¼‰- å¸‚å ´ã‚·ã‚§ã‚¢ã€å£²ä¸Šãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€ç«¶åˆæ¯”è¼ƒã€å¼·ã¿ãƒ»å¼±ã¿ã®å®šé‡çš„åˆ†æã€‚å£²ä¸Šé«˜ã€åˆ©ç›Šç‡ã€å¾“æ¥­å“¡æ•°ç­‰ã®å…·ä½“çš„ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚ã‚‹ã“ã¨ã€‚",
    "differentiation": "ç‹¬è‡ªæ€§ãƒ»å·®åˆ¥åŒ–è¦å› ã®åˆ†æï¼ˆ2400æ–‡å­—ç¨‹åº¦ï¼‰- æŠ€è¡“åŠ›ã€ãƒ–ãƒ©ãƒ³ãƒ‰åŠ›ã€ãƒ“ã‚¸ãƒã‚¹ãƒ¢ãƒ‡ãƒ«ã€ç‰¹è¨±ã€äººæç­‰ã®ç«¶äº‰å„ªä½æ€§ã®è©³ç´°åˆ†æã€‚å…·ä½“çš„ãªäº‹ä¾‹ã¨æ•°å€¤ã‚’å«ã‚ã‚‹ã“ã¨ã€‚",
    "business_portfolio": "äº‹æ¥­ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®åˆ†æï¼ˆ2400æ–‡å­—ç¨‹åº¦ï¼‰- äº‹æ¥­ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥å£²ä¸Šã€åˆ©ç›Šç‡ã€æˆé•·æ€§ã€ãƒªã‚¹ã‚¯åˆ†æã€ä»Šå¾Œã®æˆ¦ç•¥æ–¹å‘æ€§ã€‚å…·ä½“çš„ãªäº‹æ¥­åˆ¥æ•°å€¤ã¨å°†æ¥äºˆæ¸¬ã‚’å«ã‚ã‚‹ã“ã¨ã€‚"
  }},
  "analysis_metadata": {{
    "company_name": "{company_name}",
    "analysis_date": "{datetime.now().strftime('%Y-%m-%d')}",
    "data_sources": {sources_list if sources_list else [f"{company_name}ã®ä¸€èˆ¬çš„ãªå…¬é–‹æƒ…å ±ãƒ»AIçŸ¥è­˜ãƒ™ãƒ¼ã‚¹"]},
    "ir_sources_count": {len(sources_list) if sources_list else 0},
    "reliability_score": {90 if sources_list else 70}
  }}
}}

é‡è¦: JSONå½¢å¼ä»¥å¤–ã®æ–‡å­—ã¯ä¸€åˆ‡å«ã‚ãšã€ä¸Šè¨˜ã®æ§‹é€ ã«å¾“ã£ã¦æœ‰åŠ¹ãªJSONã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
å„åˆ†æé …ç›®ã§ã¯å…·ä½“çš„ãªæ•°å€¤ã€æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ã€ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã‚’å¿…ãšå«ã‚ã¦ãã ã•ã„ã€‚
"""
        return prompt
    
    def analyze_company(self, company_name, company_url=None):
        """ä¼æ¥­ã®äº‹æ¥­åˆ†æã‚’å®Ÿè¡Œï¼ˆæ¤œç´¢ãƒ™ãƒ¼ã‚¹ï¼‰"""
        
        # æ¤œç´¢ãƒ™ãƒ¼ã‚¹ã§IRæƒ…å ±åé›†
        collector = SearchBasedIRCollector(company_name)
        ir_data = collector.search_ir_information()
        
        # åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        prompt = self.create_analysis_prompt(company_name, ir_data)
        
        try:
            st.info("ğŸ¤– AIåˆ†æã‚’å®Ÿè¡Œä¸­...")
            
            # JSONå½¢å¼ã‚’å¼·åˆ¶ã™ã‚‹ãŸã‚ã®æ”¹å–„ã•ã‚ŒãŸã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
            response = self.client.chat.completions.create(
                model="gpt-5",
                messages=[
                    {
                        "role": "system", 
                        "content": "ã‚ãªãŸã¯ä¼æ¥­åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚å¿…ãšæœ‰åŠ¹ãªJSONå½¢å¼ã§ã®ã¿å›ç­”ã—ã¦ãã ã•ã„ã€‚JSONã®æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã¯çµ¶å¯¾ã«é¿ã‘ã¦ãã ã•ã„ã€‚"
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                max_completion_tokens=12000,  # GPT-5å¯¾å¿œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                response_format={"type": "json_object"}  # JSONå½¢å¼ã‚’å¼·åˆ¶
            )
            
            result_text = response.choices[0].message.content
            st.success("âœ… AIå¿œç­”ã‚’å—ä¿¡ã—ã¾ã—ãŸ")
            
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆé–‹ç™ºæ™‚ã®ã¿è¡¨ç¤ºï¼‰
            with st.expander("ğŸ” AIå¿œç­”ã®è©³ç´°ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰", expanded=False):
                st.text(f"å¿œç­”é•·: {len(result_text)}æ–‡å­—")
                st.text(f"æœ€åˆã®200æ–‡å­—: {result_text[:200]}...")
                st.text(f"æœ€å¾Œã®200æ–‡å­—: ...{result_text[-200:]}")
            
            # ç›´æ¥JSONè§£æã‚’è©¦è¡Œ
            try:
                result = json.loads(result_text)
                
                # å¿…è¦ãªã‚­ãƒ¼ã®å­˜åœ¨ã‚’ç¢ºèª
                if 'business_analysis' in result:
                    st.success("âœ… JSONè§£ææˆåŠŸ")
                    return result
                else:
                    st.warning("âš ï¸ å¿…è¦ãªã‚­ãƒ¼ 'business_analysis' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬æ§‹é€ ã‚’ä½œæˆ
                    return self._create_fallback_result(company_name, result_text)
                    
            except json.JSONDecodeError as e:
                st.error(f"âŒ ç›´æ¥JSONè§£æã‚¨ãƒ©ãƒ¼: {str(e)}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
                return self._create_fallback_result(company_name, result_text)
                
        except Exception as e:
            st.error(f"âŒ AIåˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None
    
    def _create_fallback_result(self, company_name, raw_text):
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: AIã®å¿œç­”ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã§çµæœã‚’ç”Ÿæˆ"""
        st.warning("âš ï¸ JSONè§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã§çµæœã‚’ç”Ÿæˆã—ã¾ã™ã€‚")
        
        with st.expander("ğŸ“„ ç”Ÿã®AIå¿œç­”", expanded=False):
            st.text(raw_text)
        
        # åŸºæœ¬çš„ãªæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        fallback_result = {
            "business_analysis": {
                "industry_market": f"{company_name}ã®æ¥­ç•Œãƒ»å¸‚å ´åˆ†ææƒ…å ±ï¼ˆAIå¿œç­”ã®è§£æã«å¤±æ•—ã—ãŸãŸã‚ã€è©³ç´°ãªåˆ†æã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼‰",
                "market_position": f"{company_name}ã®å¸‚å ´ãƒã‚¸ã‚·ãƒ§ãƒ³æƒ…å ±ï¼ˆAIå¿œç­”ã®è§£æã«å¤±æ•—ã—ãŸãŸã‚ã€è©³ç´°ãªåˆ†æã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼‰",
                "differentiation": f"{company_name}ã®ç‹¬è‡ªæ€§ãƒ»å·®åˆ¥åŒ–æƒ…å ±ï¼ˆAIå¿œç­”ã®è§£æã«å¤±æ•—ã—ãŸãŸã‚ã€è©³ç´°ãªåˆ†æã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼‰",
                "business_portfolio": f"{company_name}ã®äº‹æ¥­ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæƒ…å ±ï¼ˆAIå¿œç­”ã®è§£æã«å¤±æ•—ã—ãŸãŸã‚ã€è©³ç´°ãªåˆ†æã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼‰"
            },
            "analysis_metadata": {
                "company_name": company_name,
                "analysis_date": datetime.now().strftime('%Y-%m-%d'),
                "data_sources": ["è§£æå¤±æ•—ã«ã‚ˆã‚Šä¸æ˜"],
                "ir_sources_count": 0,
                "reliability_score": 30,
                "error_note": "JSONè§£æã«å¤±æ•—ã—ãŸãŸã‚ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯çµæœã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™"
            }
        }
        
        # ç”Ÿã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æœ‰ç”¨ãªæƒ…å ±ã‚’æŠ½å‡ºã‚’è©¦è¡Œ
        if raw_text and len(raw_text) > 100:
            # ç°¡å˜ãªãƒ†ã‚­ã‚¹ãƒˆåˆ†æã§éƒ¨åˆ†çš„ã«æƒ…å ±ã‚’æŠ½å‡º
            lines = raw_text.split('\n')
            useful_lines = [line.strip() for line in lines if line.strip() and len(line.strip()) > 20]
            
            if useful_lines:
                combined_text = ' '.join(useful_lines[:5])  # æœ€åˆã®5è¡Œã‚’çµåˆ
                fallback_result["business_analysis"]["industry_market"] = f"{company_name}ã«é–¢ã™ã‚‹æƒ…å ±: {combined_text[:400]}..."
        
        return fallback_result
    
    def save_results(self, company_name, analysis_data):
        """åˆ†æçµæœã‚’ä¿å­˜"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"business_analysis_{company_name}_{timestamp}.json"
            
            save_data = {
                "analysis_results": analysis_data,
                "generated_at": datetime.now().isoformat(),
                "system_info": {
                    "version": "3.0_search_based",
                    "analysis_type": "business_search_focused"
                }
            }
            
            # JSONæ–‡å­—åˆ—ã¨ã—ã¦è¿”ã™ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã¯ç’°å¢ƒã«ã‚ˆã‚Šç•°ãªã‚‹ï¼‰
            return filename, save_data
            
        except Exception as e:
            st.warning(f"âš ï¸ çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None, analysis_data

def main():
    st.title("ğŸ¢ ä¼æ¥­ãƒ“ã‚¸ãƒã‚¹åˆ†æã‚·ã‚¹ãƒ†ãƒ ")
    st.markdown("### ä¼æ¥­ã®äº‹æ¥­æˆ¦ç•¥ãƒ»ç«¶åˆåˆ†æãƒ»å¸‚å ´ãƒã‚¸ã‚·ãƒ§ãƒ³ã‚’è‡ªå‹•åˆ†æ")
    
    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
    with st.expander("â„¹ï¸ ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±", expanded=False):
        st.markdown("""
        **åˆ†æå†…å®¹:**
        - ğŸ“ˆ **æ¥­ç•Œãƒ»å¸‚å ´åˆ†æ**: æ‰€å±æ¥­ç•Œã¨å¸‚å ´è¦æ¨¡ãƒ»æˆé•·æ€§
        - ğŸ† **æ¥­ç•Œå†…ãƒã‚¸ã‚·ãƒ§ãƒ³**: å£²ä¸Šè¦æ¨¡ãƒ»å¸‚å ´ã‚·ã‚§ã‚¢ãƒ»ç«¶åˆæ¯”è¼ƒ
        - â­ **ç‹¬è‡ªæ€§ãƒ»å·®åˆ¥åŒ–**: æŠ€è¡“åŠ›ãƒ»ãƒ–ãƒ©ãƒ³ãƒ‰åŠ›ãƒ»äº‹æ¥­ãƒ¢ãƒ‡ãƒ«
        - ğŸ—ï¸ **äº‹æ¥­ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ª**: ä¸»åŠ›äº‹æ¥­ãƒ»åç›Šæ§‹é€ ãƒ»äº‹æ¥­é ˜åŸŸ
        
        **ç‰¹å¾´:**
        - ğŸ¤– OpenAI GPT-4o-mini ã«ã‚ˆã‚‹é«˜åº¦ãªAIåˆ†æ
        - ï¿½ SerpAPIæ¤œç´¢ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼šè¨­å®šæ™‚ã®ã¿ï¼‰
        - ğŸ¯ äº‹æ¥­åˆ†æã«ç‰¹åŒ–ï¼ˆEVPåˆ†æã¯å»ƒæ­¢ï¼‰
        - ğŸ“ 2400æ–‡å­—ã®è©³ç´°åˆ†æï¼ˆ3å€æ‹¡å¼µï¼‰
        - ğŸ“„ JSONå½¢å¼ã§ã®çµæœå‡ºåŠ›
        - ï¿½ AIçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«ã‚ˆã‚‹åŒ…æ‹¬çš„ä¼æ¥­åˆ†æ
        - ğŸ“‹ PDFè³‡æ–™å¯¾å¿œãƒ»å¤šéšå±¤ã‚¯ãƒ­ãƒ¼ãƒ«ï¼ˆ3-4éšå±¤ï¼‰
        - ğŸ’¾ 100,000æ–‡å­—ã®ãƒ‡ãƒ¼ã‚¿åé›†å®¹é‡
        """)
    
    # APIã‚­ãƒ¼è¨ºæ–­
    with st.expander("ğŸ”§ APIã‚­ãƒ¼è¨ºæ–­", expanded=False):
        if st.button("ğŸ“‹ APIã‚­ãƒ¼è¨­å®šçŠ¶æ³ã‚’ç¢ºèª"):
            # OpenAI APIã‚­ãƒ¼ç¢ºèª
            try:
                analyzer = BusinessAnalyzer()
                st.success("âœ… OpenAI APIã‚­ãƒ¼: æ­£å¸¸è¨­å®šæ¸ˆã¿")
            except:
                st.error("âŒ OpenAI APIã‚­ãƒ¼: æœªè¨­å®šã¾ãŸã¯ç„¡åŠ¹")
            
            # SerpAPIã‚­ãƒ¼ç¢ºèª
            test_collector = SearchBasedIRCollector("ãƒ†ã‚¹ãƒˆ")
            serpapi_key = test_collector.get_serpapi_key()
            if serpapi_key:
                st.success("âœ… SerpAPI ã‚­ãƒ¼: æ­£å¸¸è¨­å®šæ¸ˆã¿")
                # ç°¡å˜ãªãƒ†ã‚¹ãƒˆæ¤œç´¢
                if st.button("ğŸ” SerpAPIãƒ†ã‚¹ãƒˆæ¤œç´¢å®Ÿè¡Œ"):
                    test_result = test_collector.search_with_serpapi("ãƒˆãƒ¨ã‚¿", serpapi_key)
                    if test_result and 'error' not in test_result:
                        st.success("âœ… SerpAPI: æ¤œç´¢ãƒ†ã‚¹ãƒˆæˆåŠŸ")
                    else:
                        st.error(f"âŒ SerpAPI: æ¤œç´¢ãƒ†ã‚¹ãƒˆå¤±æ•— - {test_result.get('error', 'Unknown error')}")
            else:
                st.warning("âš ï¸ SerpAPI ã‚­ãƒ¼: æœªè¨­å®šï¼ˆæ¤œç´¢æ©Ÿèƒ½ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¾ã™ï¼‰")
    
    # å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
    with st.form("analysis_form"):
        company_name = st.text_input(
            "ğŸ¢ ä¼æ¥­å *", 
            placeholder="ä¾‹: ãƒˆãƒ¨ã‚¿è‡ªå‹•è»Šã€ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯ã€ãƒªã‚¯ãƒ«ãƒ¼ãƒˆ",
            help="åˆ†æå¯¾è±¡ã®ä¼æ¥­åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆæ¤œç´¢ãƒ™ãƒ¼ã‚¹ã§IRæƒ…å ±ã‚’è‡ªå‹•åé›†ã—ã¾ã™ï¼‰"
        )
        
        st.markdown("---")
        submitted = st.form_submit_button("ğŸ” äº‹æ¥­åˆ†æé–‹å§‹", type="primary", use_container_width=True)
    
    # åˆ†æå®Ÿè¡Œ
    if submitted:
        if not company_name:
            st.error("ğŸš¨ ä¼æ¥­åã¯å¿…é ˆå…¥åŠ›ã§ã™ã€‚")
            return
        
        analyzer = BusinessAnalyzer()
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        status_text.text("ğŸ” æ¤œç´¢ãƒ™ãƒ¼ã‚¹ã§IRæƒ…å ±ã‚’åé›†ä¸­...")
        progress_bar.progress(25)
        
        with st.spinner("ğŸ¤– AIåˆ†æä¸­... (30-60ç§’ç¨‹åº¦ãŠå¾…ã¡ãã ã•ã„)"):
            progress_bar.progress(50)
            analysis_result = analyzer.analyze_company(company_name)
            progress_bar.progress(80)
        
        if analysis_result:
            # çµæœä¿å­˜
            filename, save_data = analyzer.save_results(company_name, analysis_result)
            progress_bar.progress(100)
            status_text.text("âœ… åˆ†æå®Œäº†ï¼")
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
            st.session_state.analysis_results = {
                "data": analysis_result,
                "company_name": company_name,
                "save_data": save_data,
                "filename": filename
            }
        else:
            progress_bar.progress(0)
            status_text.text("âŒ åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ")
            st.error("âŒ åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸã€‚APIã‚­ãƒ¼ã¾ãŸã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    
    # çµæœè¡¨ç¤º
    if 'analysis_results' in st.session_state:
        results = st.session_state.analysis_results
        analysis_data = results["data"]
        company_name = results["company_name"]
        save_data = results["save_data"]
        filename = results["filename"]
        
        st.success("ğŸ‰ äº‹æ¥­åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        
        # åŸºæœ¬æƒ…å ±
        st.markdown("---")
        st.subheader("ğŸ“Š åˆ†æçµæœã‚µãƒãƒªãƒ¼")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ğŸ¢ ä¼æ¥­å", company_name)
        with col2:
            st.metric("ğŸ¯ åˆ†æã‚¿ã‚¤ãƒ—", "äº‹æ¥­åˆ†æç‰¹åŒ–")
        with col3:
            metadata = analysis_data.get('analysis_metadata', {})
            st.metric("ğŸ“ˆ ä¿¡é ¼æ€§ã‚¹ã‚³ã‚¢", f"{metadata.get('reliability_score', 'N/A')}/100")
        
        st.markdown("---")
        
        # ã‚¿ãƒ–å½¢å¼ã§çµæœè¡¨ç¤º
        tab1, tab2 = st.tabs(["ğŸ† äº‹æ¥­åˆ†æçµæœ", "ğŸ“„ JSONå‡ºåŠ›"])
        
        with tab1:
            st.subheader("ğŸ† ä¼æ¥­ãƒ“ã‚¸ãƒã‚¹åˆ†æ")
            
            business_data = analysis_data.get('business_analysis', {})
            if business_data:
                # æ•´å½¢æ©Ÿèƒ½ã‚’ä½¿ã£ã¦ç¾ã—ãè¡¨ç¤º
                test_collector = SearchBasedIRCollector("display")
                test_collector.display_formatted_analysis(business_data)
            else:
                st.warning("äº‹æ¥­åˆ†æãƒ‡ãƒ¼ã‚¿ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
        
        with tab2:
            st.subheader("ğŸ“„ JSONå½¢å¼ã®åˆ†æçµæœ")
            
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
            json_output = json.dumps(save_data, ensure_ascii=False, indent=2)
            st.download_button(
                label="ğŸ’¾ JSONçµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=json_output,
                file_name=f"business_analysis_{company_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json"
            )
            
            # JSONè¡¨ç¤º
            st.code(json_output, language="json")
        
        # æ–°ã—ã„åˆ†æãƒœã‚¿ãƒ³
        if st.button("ğŸ”„ æ–°ã—ã„åˆ†æã‚’é–‹å§‹"):
            if 'analysis_results' in st.session_state:
                del st.session_state.analysis_results
            st.rerun()

    # ãƒ•ãƒƒã‚¿ãƒ¼
    st.markdown("---")
    st.markdown(
        """
        <div style="text-align: center; color: #666;">
            ğŸ” ä¼æ¥­ãƒ“ã‚¸ãƒã‚¹åˆ†æã‚·ã‚¹ãƒ†ãƒ  v3.0 (æ¤œç´¢ç‰¹åŒ–ç‰ˆ) | Powered by OpenAI GPT-4o-mini + SerpAPI
        </div>
        """, 
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()