#!/usr/bin/env python3
"""
ä¼æ¥­EVPãƒ»ä¼æ¥­åˆ†æã‚·ã‚¹ãƒ†ãƒ  - Streamlit Webç‰ˆï¼ˆæœ¬ç•ªç’°å¢ƒå¯¾å¿œï¼‰
"""

import streamlit as st
import os
import json
import datetime
import re
import requests
from pathlib import Path
from openai import OpenAI
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from datetime import datetime, timedelta

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ğŸ¢ AIä¼æ¥­åˆ†æã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="collapsed"
)

class SmartIRCrawler:
    """ã‚¹ãƒãƒ¼ãƒˆIRæƒ…å ±åé›†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾ç­–ä»˜ãï¼‰"""
    
    def __init__(self, company_domain, ir_url=None, max_depth=4, date_limit_years=3):
        self.company_domain = company_domain
        self.ir_url = ir_url or f"https://{company_domain}/ir/"
        self.max_depth = max_depth
        self.date_limit = datetime.now() - timedelta(days=date_limit_years * 365)
        self.discovered_content = []
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
    
    def is_valid_domain(self, url):
        """ä¼æ¥­ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ã¿è¨±å¯"""
        try:
            parsed = urlparse(url)
            return self.company_domain in parsed.netloc
        except:
            return False
    
    def extract_date_from_content(self, content, url):
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º"""
        date_patterns = [
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
            r'(\d{4})-(\d{2})-(\d{2})',
            r'(\d{4})/(\d{1,2})/(\d{1,2})',
            r'å…¬è¡¨æ—¥[ï¼š:\s]*(\d{4}[/-]\d{1,2}[/-]\d{1,2})',
            r'ç™ºè¡¨æ—¥[ï¼š:\s]*(\d{4}[/-]\d{1,2}[/-]\d{1,2})'
        ]
        
        for pattern in date_patterns:
            matches = re.findall(pattern, content)
            if matches:
                try:
                    # æœ€åˆã®ãƒãƒƒãƒã‚’æ—¥ä»˜ã¨ã—ã¦è§£æ
                    match = matches[0]
                    if isinstance(match, tuple):
                        if len(match) == 3:
                            year, month, day = match
                            return datetime(int(year), int(month), int(day))
                except:
                    continue
        
        # æ—¥ä»˜ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç¾åœ¨æ—¥æ™‚ã‚’è¿”ã™
        return datetime.now()
    
    def score_content_importance(self, content, url):
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®é‡è¦åº¦ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°"""
        priority_keywords = {
            "æ±ºç®—çŸ­ä¿¡": 10,
            "æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸": 9,
            "æ±ºç®—èª¬æ˜ä¼š": 8,
            "æ¥­ç¸¾ãƒã‚¤ãƒ©ã‚¤ãƒˆ": 7,
            "ä¸­æœŸçµŒå–¶è¨ˆç”»": 6,
            "æ ªä¸»ç·ä¼š": 5,
            "é©æ™‚é–‹ç¤º": 4,
            "ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªãƒªãƒ¼ã‚¹": 3,
            "IR": 2
        }
        
        score = 0
        content_lower = content.lower()
        url_lower = url.lower()
        
        for keyword, points in priority_keywords.items():
            if keyword in content or keyword in url:
                score += points
        
        # PDFæ–‡æ›¸ã¯é‡è¦åº¦ãŒé«˜ã„
        if '.pdf' in url_lower:
            score += 3
        
        # æ±ºç®—é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        earnings_keywords = ["æ±ºç®—", "æ¥­ç¸¾", "è²¡å‹™", "å£²ä¸Š", "åˆ©ç›Š"]
        for keyword in earnings_keywords:
            if keyword in content:
                score += 2
        
        return score
    
    def discover_ir_links(self, start_url, depth=0):
        """IRãƒšãƒ¼ã‚¸ã‹ã‚‰é‡è¦ãªãƒªãƒ³ã‚¯ã‚’ç™ºè¦‹ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        if depth > self.max_depth:
            return []
        
        try:
            # URLã®æ¤œè¨¼ã‚’ç·©å’Œ
            if not start_url.startswith(('http://', 'https://')):
                start_url = 'https://' + start_url
            
            st.info(f"ğŸ” æ¢ç´¢ä¸­: {start_url}")
            
            response = self.session.get(start_url, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰æ—¥ä»˜æŠ½å‡º
            page_date = self.extract_date_from_content(response.text, start_url)
            
            # é‡è¦åº¦ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
            importance_score = self.score_content_importance(response.text, start_url)
            
            discovered = [{
                'url': start_url,
                'content': response.text[:3000],  # 3000æ–‡å­—ã«çŸ­ç¸®
                'date': page_date,
                'importance': importance_score,
                'title': soup.title.string if soup.title else start_url.split('/')[-1]
            }]
            
            # åŸºæœ¬çš„ãªIRæƒ…å ±ãŒã‚ã‚Œã°åé›†æˆåŠŸã¨ã¿ãªã™
            if importance_score > 0:
                st.success(f"âœ… IRæƒ…å ±ã‚’ç™ºè¦‹: {soup.title.string if soup.title else start_url}")
            
            # ãƒªãƒ³ã‚¯æ¢ç´¢ã¯ç°¡æ½”ã«
            if depth < 2:  # æ¢ç´¢æ·±åº¦ã‚’åˆ¶é™
                ir_keywords = ['æ±ºç®—', 'æ¥­ç¸¾', 'ir', 'investor']
                for link in soup.find_all('a', href=True)[:20]:  # æœ€åˆã®20å€‹ã®ãƒªãƒ³ã‚¯ã®ã¿
                    href = link.get('href')
                    if not href:
                        continue
                    
                    full_url = urljoin(start_url, href)
                    link_text = link.get_text().lower()
                    
                    if any(keyword in link_text or keyword in href.lower() for keyword in ir_keywords):
                        discovered.extend(self.discover_ir_links(full_url, depth + 1))
                        if len(discovered) >= 5:  # 5ä»¶è¦‹ã¤ã‹ã£ãŸã‚‰çµ‚äº†
                            break
            
            return discovered
            
        except requests.exceptions.RequestException as e:
            st.warning(f"âš ï¸ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {start_url} - {str(e)}")
            return []
        except Exception as e:
            st.warning(f"âš ï¸ è§£æã‚¨ãƒ©ãƒ¼: {start_url} - {str(e)}")
            return []
    
    def crawl_with_intelligence(self):
        """ã‚¹ãƒãƒ¼ãƒˆãªIRæƒ…å ±åé›†ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        try:
            st.info(f"ğŸ” IRæƒ…å ±æ¢ç´¢ã‚’é–‹å§‹: {self.ir_url}")
            
            # è¤‡æ•°ã®IR URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œ
            ir_patterns = [
                self.ir_url,
                f"https://{self.company_domain}/ir/",
                f"https://{self.company_domain}/investor/",
                f"https://{self.company_domain}/company/ir/",
                f"https://ir.{self.company_domain}/",
            ]
            
            all_content = []
            
            for url_pattern in ir_patterns:
                if not url_pattern or len(all_content) >= 3:  # 3ä»¶è¦‹ã¤ã‹ã£ãŸã‚‰çµ‚äº†
                    continue
                    
                try:
                    content = self.discover_ir_links(url_pattern, 0)
                    if content:
                        all_content.extend(content)
                        st.success(f"âœ… {len(content)}ä»¶ã®æƒ…å ±ã‚’ {url_pattern} ã‹ã‚‰åé›†")
                        break  # æˆåŠŸã—ãŸã‚‰ä»–ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯è©¦è¡Œã—ãªã„
                except:
                    continue
            
            if not all_content:
                st.warning("âš ï¸ IRæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ä¸€èˆ¬çš„ãªä¼æ¥­åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
                return []
            
            # é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆã—ã¦é‡è¤‡é™¤å»
            unique_content = {}
            for item in all_content:
                if item['url'] not in unique_content:
                    unique_content[item['url']] = item
            
            sorted_content = sorted(unique_content.values(), key=lambda x: x['importance'], reverse=True)
            
            # ä¸Šä½5ä»¶ã‚’è¿”ã™
            result = sorted_content[:5]
            st.success(f"ğŸ‰ åˆè¨ˆ {len(result)} ä»¶ã®IRæƒ…å ±ã‚’åé›†ã—ã¾ã—ãŸ")
            
            return result
            
        except Exception as e:
            st.error(f"âŒ IRæƒ…å ±åé›†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []

class StreamlitCompanyResearcher:
    def __init__(self):
        # APIã‚­ãƒ¼è¨­å®šï¼ˆæœ¬ç•ªç’°å¢ƒå¯¾å¿œï¼‰
        api_key = self.get_openai_api_key()
        
        try:
            self.client = OpenAI(api_key=api_key)
        except Exception as e:
            st.error(f"âŒ OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            st.stop()
            
        # çµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆæœ¬ç•ªç’°å¢ƒã§ã¯ä¸€æ™‚çš„ï¼‰
        self.results_dir = Path('results')
        self.results_dir.mkdir(exist_ok=True)
        
        # HTTP ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼ˆWebæ¤œç´¢ç”¨ï¼‰
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
    
    def extract_domain_from_url(self, url):
        """URLã‹ã‚‰ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’æŠ½å‡º"""
        if not url:
            return None
        try:
            if not url.startswith(('http://', 'https://')):
                url = 'https://' + url
            parsed = urlparse(url)
            domain = parsed.netloc
            # www. ã‚’é™¤å»
            if domain.startswith('www.'):
                domain = domain[4:]
            return domain
        except:
            return None
    
    def validate_response_content(self, response, source_data):
        """ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾ç­–ï¼šå›ç­”å†…å®¹ã®æ¤œè¨¼"""
        # æ¨æ¸¬è¡¨ç¾ã®æ¤œå‡º
        speculation_patterns = [
            r'ã¨æ€ã‚ã‚Œ', r'å¯èƒ½æ€§ãŒ', r'ãŠãã‚‰ã', r'ä¸€èˆ¬çš„ã«', 
            r'é€šå¸¸ã¯', r'äºˆæƒ³', r'æ¨æ¸¬', r'æ†¶æ¸¬', r'ã‹ã‚‚ã—ã‚Œ'
        ]
        
        for pattern in speculation_patterns:
            if re.search(pattern, response):
                return False, f"æ¨æ¸¬çš„è¡¨ç¾ãŒå«ã¾ã‚Œã¦ã„ã¾ã™: {pattern}"
        
        # å‡ºå…¸è¨˜è¼‰ã®ç¢ºèª
        if 'å‡ºå…¸ï¼š' not in response and 'ã‚½ãƒ¼ã‚¹ï¼š' not in response:
            return False, "å‡ºå…¸ãŒæ˜è¨˜ã•ã‚Œã¦ã„ã¾ã›ã‚“"
        
        # åŸºæœ¬çš„ãªäº‹å®Ÿç¢ºèªï¼ˆä¼æ¥­åã®ä¸€è‡´ãªã©ï¼‰
        company_name_in_source = any(source['title'] for source in source_data if source.get('title'))
        if not company_name_in_source and len(source_data) > 0:
            st.warning("âš ï¸ ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã¨ä¼æ¥­åã®æ•´åˆæ€§ã‚’ç¢ºèªä¸­...")
        
        return True, "æ¤œè¨¼OK"
    
    def create_constrained_prompt(self, company_info, ir_data):
        """åˆ¶ç´„ä»˜ããƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ"""
        ir_content = "\n".join([
            f"ã€{item['title']}ã€‘(é‡è¦åº¦: {item['importance']}, æ—¥ä»˜: {item['date'].strftime('%Y-%m-%d')})\n"
            f"URL: {item['url']}\n"
            f"å†…å®¹: {item['content'][:800]}...\n"
            for item in ir_data[:5]  # ä¸Šä½5ä»¶
        ])
        
        system_prompt = f"""
ã‚ãªãŸã¯ä¼æ¥­åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã‚’å³æ ¼ã«å®ˆã£ã¦ãã ã•ã„ï¼š

ã€é‡è¦åˆ¶ç´„ã€‘
1. æä¾›ã•ã‚ŒãŸIRæƒ…å ±ã¨Webãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’å‚ç…§ã™ã‚‹ã“ã¨
2. ãƒ‡ãƒ¼ã‚¿ã«ãªã„æƒ…å ±ã¯ã€Œåˆ†æãƒ‡ãƒ¼ã‚¿ã«ã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€ã¨æ˜è¨˜
3. æ¨æ¸¬ã‚„ä¸€èˆ¬è«–ã§ã¯ãªãã€å…·ä½“çš„ãªæ ¹æ‹ ã‚’ç¤ºã™ã“ã¨
4. å¿…ãšã€Œå‡ºå…¸ï¼š[URL] [å–å¾—æ—¥æ™‚]ã€ã‚’æ˜è¨˜ã™ã‚‹ã“ã¨
5. ã€ŒãŠãã‚‰ãã€ã€Œä¸€èˆ¬çš„ã«ã€ã€Œæ¨æ¸¬ã§ã¯ã€ç­‰ã®è¡¨ç¾ã¯ä½¿ç”¨ç¦æ­¢
6. 3å¹´ä»¥å†…ï¼ˆ2022å¹´1æœˆä»¥é™ï¼‰ã®æƒ…å ±ã®ã¿ä½¿ç”¨ã™ã‚‹ã“ã¨

ã€åˆ†æå¯¾è±¡ä¼æ¥­ã€‘: {company_info['company_name']}
ã€é‡ç‚¹åˆ†é‡ã€‘: {company_info['focus_area']}

ã€åˆ©ç”¨å¯èƒ½ãªIRæƒ…å ±ã€‘:
{ir_content}

ã€åˆ†ææŒ‡ç¤ºã€‘:
ä¸Šè¨˜ã®IRæƒ…å ±ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ã€EVPåˆ†æã¨ãƒ“ã‚¸ãƒã‚¹åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚
ãƒ‡ãƒ¼ã‚¿ã«ãªã„é …ç›®ã«ã¤ã„ã¦ã¯ã€Œãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚åˆ†æã§ãã¾ã›ã‚“ã€ã¨è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚
"""
        
        return system_prompt
    
    def verify_with_double_check(self, question, answer, sources):
        """äºŒæ®µéšæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ """
        verification_prompt = f"""
ä»¥ä¸‹ã®å›ç­”ã«ã¤ã„ã¦äº‹å®Ÿç¢ºèªã‚’è¡Œã£ã¦ãã ã•ã„ï¼š

è³ªå•: {question}
å›ç­”: {answer}

ãƒã‚§ãƒƒã‚¯é …ç›®:
1. æä¾›ã•ã‚ŒãŸã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã‹ï¼Ÿ
2. 3å¹´ä»¥å†…ã®æƒ…å ±ã®ã¿ã‹ï¼Ÿ
3. æ¨æ¸¬ã‚„å¤–éƒ¨çŸ¥è­˜ãŒæ··å…¥ã—ã¦ã„ãªã„ã‹ï¼Ÿ
4. å‡ºå…¸ãŒæ­£ã—ãæ˜è¨˜ã•ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ

å•é¡ŒãŒã‚ã‚Œã°ã€ŒNGï¼šç†ç”±ã€ã€å•é¡Œãªã‘ã‚Œã°ã€ŒOKã€ã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚
"""
        
        try:
            verification_response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": verification_prompt}],
                temperature=0.1,
                max_tokens=100
            )
            
            verification_result = verification_response.choices[0].message.content
            return "OK" in verification_result, verification_result
            
        except Exception as e:
            return False, f"æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def search_existing_sources(self, question, analysis_data):
        """æ·±åº¦èª¿æŸ»: ä¼æ¥­ã‚µã‚¤ãƒˆã®éšå±¤ã‚’æ·±ãæ¢ç´¢ã—ã¦é–¢é€£æƒ…å ±ã‚’åé›†"""
        company_domain = analysis_data.get('company_domain')
        if not company_domain:
            return []
        
        try:
            # è³ªå•ã«åŸºã¥ã„ã¦æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
            search_keywords = self.extract_search_keywords(question)
            
            st.info("ğŸ” ä¼æ¥­ã‚µã‚¤ãƒˆã‚’æ·±åº¦èª¿æŸ»ä¸­...")
            
            # æ®µéšçš„ãªæ·±åº¦èª¿æŸ»
            additional_sources = []
            
            # Step 1: åŸºæœ¬ã‚»ã‚¯ã‚·ãƒ§ãƒ³ + ã‚µãƒ–ãƒšãƒ¼ã‚¸ç™ºè¦‹
            base_sections = {
                'ir': ['investor', 'ir', 'finance'],
                'business': ['business', 'service', 'product', 'solution'],
                'company': ['company', 'about', 'corporate'],
                'news': ['news', 'press', 'release'],
                'strategy': ['strategy', 'vision', 'plan', 'management']
            }
            
            for section_type, url_patterns in base_sections.items():
                section_sources = self.deep_explore_section(
                    company_domain, section_type, url_patterns, search_keywords, question
                )
                additional_sources.extend(section_sources)
                
                # æœ€å¤§12ã‚½ãƒ¼ã‚¹ã¾ã§ï¼ˆå„ã‚»ã‚¯ã‚·ãƒ§ãƒ³2-3å€‹ï¼‰
                if len(additional_sources) >= 12:
                    break
            
            # Step 2: é‡è¦æ–‡æ›¸ã®è‡ªå‹•ç™ºè¦‹ãƒ»å–å¾—
            document_sources = self.discover_important_documents(
                company_domain, search_keywords, question
            )
            additional_sources.extend(document_sources)
            
            return additional_sources[:15]  # æœ€å¤§15ã‚½ãƒ¼ã‚¹
            
        except Exception as e:
            st.warning(f"æ·±åº¦èª¿æŸ»ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def deep_explore_section(self, domain, section_type, url_patterns, keywords, question):
        """ç‰¹å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æ·±åº¦æ¢ç´¢"""
        sources = []
        
        for pattern in url_patterns:
            # è¤‡æ•°ã®URLå€™è£œã‚’è©¦è¡Œ
            candidate_urls = [
                f"https://{domain}/{pattern}/",
                f"https://{domain}/{pattern}.html",
                f"https://{domain}/jp/{pattern}/",
                f"https://{domain}/ja/{pattern}/",
            ]
            
            for base_url in candidate_urls:
                try:
                    response = self.session.get(base_url, timeout=15)
                    if response.status_code != 200:
                        continue
                        
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # ã‚µãƒ–ãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯ã‚’ç™ºè¦‹
                    subpage_links = self.discover_subpages(soup, base_url, section_type)
                    
                    # ãƒ™ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è§£æ
                    base_content = self.extract_relevant_content(soup, keywords, question)
                    if base_content:
                        sources.append({
                            'url': base_url,
                            'content': base_content,
                            'source_type': f'{section_type.title()}æƒ…å ±',
                            'depth': 'base'
                        })
                    
                    # ã‚µãƒ–ãƒšãƒ¼ã‚¸ã®æ¢ç´¢ï¼ˆæœ€å¤§3ã¤ã¾ã§ï¼‰
                    for sublink in subpage_links[:3]:
                        sub_content = self.explore_subpage(sublink, keywords, question)
                        if sub_content:
                            sources.append(sub_content)
                    
                    break  # æˆåŠŸã—ãŸã‚‰ä»–ã®å€™è£œURLã¯è©¦è¡Œã—ãªã„
                    
                except:
                    continue
                    
            if len(sources) >= 3:  # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚ãŸã‚Šæœ€å¤§3ã‚½ãƒ¼ã‚¹
                break
                
        return sources
    
    def discover_subpages(self, soup, base_url, section_type):
        """ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã®ã‚µãƒ–ãƒšãƒ¼ã‚¸ã‚’ç™ºè¦‹"""
        subpages = []
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥ã®é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        important_keywords = {
            'ir': ['æ±ºç®—', 'æ¥­ç¸¾', 'èª¬æ˜ä¼š', 'ä¸­æœŸ', 'è¨ˆç”»', 'æœ‰ä¾¡è¨¼åˆ¸', 'è²¡å‹™', 'financial'],
            'business': ['äº‹æ¥­ç´¹ä»‹', 'ã‚µãƒ¼ãƒ“ã‚¹', 'è£½å“', 'ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³', 'å¼·ã¿', 'ç‰¹å¾´'],
            'company': ['ä»£è¡¨', 'ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸', 'æ²¿é©', 'çµ„ç¹”', 'ãƒŸãƒƒã‚·ãƒ§ãƒ³', 'ãƒ“ã‚¸ãƒ§ãƒ³'],
            'news': ['ãƒ—ãƒ¬ã‚¹', 'ãƒªãƒªãƒ¼ã‚¹', 'ç™ºè¡¨', 'æ–°ç€', 'æœ€æ–°'],
            'strategy': ['æˆ¦ç•¥', 'æ–¹é‡', 'ãƒ“ã‚¸ãƒ§ãƒ³', 'è¨ˆç”»', 'å–ã‚Šçµ„ã¿', 'DX']
        }
        
        keywords = important_keywords.get(section_type, [])
        
        # ãƒªãƒ³ã‚¯ã‚’æ¢ç´¢
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            text = link.get_text().strip()
            
            if not href or len(text) < 3:
                continue
                
            # ç›¸å¯¾ãƒ‘ã‚¹ã‚’çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
            if href.startswith('/'):
                full_url = f"https://{base_url.split('/')[2]}{href}"
            elif href.startswith('http'):
                full_url = href
            else:
                continue
                
            # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ãƒªãƒ³ã‚¯ã‚’å„ªå…ˆ
            relevance_score = 0
            text_lower = text.lower()
            
            for keyword in keywords:
                if keyword in text_lower:
                    relevance_score += 2
                if keyword in href.lower():
                    relevance_score += 1
            
            # PDFãƒ•ã‚¡ã‚¤ãƒ«ã¯ç‰¹ã«é‡è¦
            if href.endswith('.pdf'):
                relevance_score += 3
                
            if relevance_score > 0:
                subpages.append({
                    'url': full_url,
                    'text': text,
                    'score': relevance_score
                })
        
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        subpages.sort(key=lambda x: x['score'], reverse=True)
        return [page['url'] for page in subpages[:5]]
    
    def explore_subpage(self, url, keywords, question):
        """ã‚µãƒ–ãƒšãƒ¼ã‚¸ã®è©³ç´°æ¢ç´¢"""
        try:
            response = self.session.get(url, timeout=15)
            if response.status_code != 200:
                return None
                
            # PDFãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
            if url.endswith('.pdf'):
                return self.extract_pdf_content(url, keywords, question)
                
            # HTMLãƒšãƒ¼ã‚¸ã®å ´åˆ
            soup = BeautifulSoup(response.text, 'html.parser')
            content = self.extract_relevant_content(soup, keywords, question)
            
            if content:
                return {
                    'url': url,
                    'content': content,
                    'source_type': self.classify_source_type(url),
                    'depth': 'deep'
                }
                
        except:
            pass
            
        return None
    
    def extract_pdf_content(self, pdf_url, keywords, question):
        """PDFæ–‡æ›¸ã‹ã‚‰ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        try:
            response = self.session.get(pdf_url, timeout=20)
            if response.status_code == 200:
                # PDFè§£æã¯è¤‡é›‘ãªã®ã§ã€ã¾ãšã¯PDFã®å­˜åœ¨ã‚’è¨˜éŒ²
                return {
                    'url': pdf_url,
                    'content': f"PDFæ–‡æ›¸ãŒç™ºè¦‹ã•ã‚Œã¾ã—ãŸ: {pdf_url.split('/')[-1]}",
                    'source_type': 'PDFè³‡æ–™',
                    'depth': 'document'
                }
        except:
            pass
        return None
    
    def discover_important_documents(self, domain, keywords, question):
        """é‡è¦æ–‡æ›¸ã®è‡ªå‹•ç™ºè¦‹"""
        documents = []
        
        # ã‚ˆãä½¿ã‚ã‚Œã‚‹é‡è¦æ–‡æ›¸ã®ãƒ‘ã‚¹
        document_paths = [
            '/ir/library/',
            '/ir/finance/',
            '/ir/brief/',
            '/ir/plan/',
            '/company/plan/',
            '/sustainability/report/',
            '/investor/',
            '/finance/results/',
        ]
        
        for path in document_paths:
            try:
                url = f"https://{domain}{path}"
                response = self.session.get(url, timeout=10)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # PDFã‚„é‡è¦æ–‡æ›¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’ç™ºè¦‹
                    for link in soup.find_all('a', href=True):
                        href = link.get('href')
                        text = link.get_text().strip()
                        
                        # é‡è¦æ–‡æ›¸ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
                        if any(keyword in text for keyword in 
                               ['æ±ºç®—', 'æœ‰ä¾¡è¨¼åˆ¸', 'ä¸­æœŸè¨ˆç”»', 'äº‹æ¥­å ±å‘Š', 'èª¬æ˜è³‡æ–™', 'financial']):
                            
                            if href.startswith('/'):
                                full_url = f"https://{domain}{href}"
                            else:
                                full_url = href
                                
                            documents.append({
                                'url': full_url,
                                'content': f"é‡è¦æ–‡æ›¸: {text}",
                                'source_type': 'é‡è¦æ–‡æ›¸',
                                'depth': 'document'
                            })
                            
                        if len(documents) >= 3:
                            break
                            
            except:
                continue
                
        return documents
    
    def extract_search_keywords(self, question):
        """è³ªå•ã‹ã‚‰æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        # ä¼æ¥­åˆ†æã«é–¢é€£ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°
        keyword_mapping = {
            "å£²ä¸Š": ["å£²ä¸Š", "revenue", "æ¥­ç¸¾", "æ±ºç®—"],
            "åˆ©ç›Š": ["åˆ©ç›Š", "profit", "å–¶æ¥­åˆ©ç›Š", "å½“æœŸç´”åˆ©ç›Š"],
            "äº‹æ¥­": ["äº‹æ¥­", "business", "ã‚µãƒ¼ãƒ“ã‚¹", "äº‹æ¥­å†…å®¹"],
            "æ¡ç”¨": ["æ¡ç”¨", "recruit", "æ–°å’", "ä¸­é€”", "æ±‚äºº"],
            "åƒãæ–¹": ["åƒãæ–¹", "work", "ãƒªãƒ¢ãƒ¼ãƒˆ", "åˆ¶åº¦", "ç¦åˆ©åšç”Ÿ"],
            "å°†æ¥": ["å°†æ¥", "future", "æˆ¦ç•¥", "è¨ˆç”»", "ãƒ“ã‚¸ãƒ§ãƒ³"],
            "ç«¶åˆ": ["ç«¶åˆ", "ç«¶äº‰", "ãƒ©ã‚¤ãƒãƒ«", "ã‚·ã‚§ã‚¢", "å¸‚å ´"],
            "æŠ€è¡“": ["æŠ€è¡“", "technology", "IT", "DX", "ã‚·ã‚¹ãƒ†ãƒ "]
        }
        
        question_lower = question.lower()
        found_keywords = []
        
        for category, keywords in keyword_mapping.items():
            for keyword in keywords:
                if keyword in question_lower:
                    found_keywords.extend(keywords)
                    break
        
        return list(set(found_keywords)) if found_keywords else ["ä¼æ¥­æƒ…å ±", "ä¼šç¤¾æ¦‚è¦"]
    
    def extract_relevant_content(self, soup, keywords, question):
        """HTMLã‹ã‚‰è³ªå•ã«é–¢é€£ã™ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ·±åº¦æŠ½å‡º"""
        relevant_texts = []
        
        # ã‚ˆã‚Šè©³ç´°ãªè¦ç´ ã‚’å¯¾è±¡ã«æ‹¡å¼µ
        content_tags = ['h1', 'h2', 'h3', 'h4', 'p', 'div', 'li', 'span', 'td', 'th']
        
        for tag in soup.find_all(content_tags):
            text = tag.get_text().strip()
            
            # ãƒ†ã‚­ã‚¹ãƒˆé•·ã®æ¡ä»¶ã‚’ç·©å’Œï¼ˆçŸ­ã„é‡è¦æƒ…å ±ã‚‚å–å¾—ï¼‰
            if len(text) < 10 or len(text) > 800:
                continue
                
            text_lower = text.lower()
            question_lower = question.lower()
            
            relevance_score = 0
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ï¼ˆé‡ã¿ä»˜ã‘å¼·åŒ–ï¼‰
            for keyword in keywords:
                keyword_lower = keyword.lower()
                if keyword_lower in text_lower:
                    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å®Œå…¨ä¸€è‡´
                    if keyword_lower in text_lower.split():
                        relevance_score += 3
                    else:
                        relevance_score += 2
            
            # è³ªå•ã®å˜èªãƒãƒƒãƒãƒ³ã‚°
            question_words = [w for w in question_lower.split() if len(w) > 2]
            for word in question_words:
                if word in text_lower:
                    relevance_score += 1
            
            # ä¼æ¥­åˆ†æã«é‡è¦ãªç”¨èªã¸ã®è¿½åŠ ã‚¹ã‚³ã‚¢
            important_terms = [
                'å£²ä¸Š', 'åˆ©ç›Š', 'æ¥­ç¸¾', 'æ±ºç®—', 'æˆ¦ç•¥', 'è¨ˆç”»', 'äº‹æ¥­', 'ãƒ“ã‚¸ãƒ§ãƒ³',
                'å¼·ã¿', 'ç‰¹å¾´', 'ç«¶åˆ', 'å¸‚å ´', 'æŠ€è¡“', 'DX', 'AI', 'ã‚µã‚¹ãƒ†ãƒŠãƒ“ãƒªãƒ†ã‚£',
                'æ¡ç”¨', 'äººæ', 'åƒãæ–¹', 'åˆ¶åº¦', 'ç¦åˆ©åšç”Ÿ', 'ãƒŸãƒƒã‚·ãƒ§ãƒ³'
            ]
            
            for term in important_terms:
                if term in text:
                    relevance_score += 1.5
            
            # æ•°å€¤ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯é‡è¦åº¦UP
            if any(char.isdigit() for char in text) and ('å„„' in text or 'ä¸‡' in text or '%' in text):
                relevance_score += 2
            
            if relevance_score > 0:
                relevant_texts.append({
                    'text': text,
                    'score': relevance_score
                })
        
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ã‚’è¿”ã™ï¼ˆæ•°ã‚’å¢—åŠ ï¼‰
        relevant_texts.sort(key=lambda x: x['score'], reverse=True)
        
        # ã‚ˆã‚Šå¤šãã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å«ã‚ã‚‹ï¼ˆä¸Šä½6ä»¶ï¼‰
        top_texts = [item['text'] for item in relevant_texts[:6]]
        
        return '\n\n'.join(top_texts) if top_texts else ""
    
    def classify_source_type(self, url):
        """URLã‹ã‚‰ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã‚’åˆ†é¡"""
        if '/ir/' in url:
            return 'IRæƒ…å ±'
        elif '/news/' in url:
            return 'ãƒ‹ãƒ¥ãƒ¼ã‚¹'
        elif '/recruit/' in url:
            return 'æ¡ç”¨æƒ…å ±'
        elif '/company/' in url:
            return 'ä¼šç¤¾æ¦‚è¦'
        elif '/sustainability/' in url:
            return 'ã‚µã‚¹ãƒ†ãƒŠãƒ“ãƒªãƒ†ã‚£'
        else:
            return 'ä¼æ¥­æƒ…å ±'
    
    def generate_chat_response(self, question, analysis_data, company_info, chat_history):
        """æ‹¡å¼µãƒãƒ£ãƒƒãƒˆè³ªå•ã¸ã®å›ç­”ç”Ÿæˆï¼ˆæ—¢å­˜ã‚½ãƒ¼ã‚¹æ´»ç”¨ï¼‰"""
        
        # Step 1: åŸºæœ¬çš„ãªåˆ†æçµæœã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦æ•´ç†
        base_context = f"""
åˆ†æå¯¾è±¡ä¼æ¥­: {company_info['company_name']}
åˆ†æé‡ç‚¹åˆ†é‡: {company_info['focus_area']}

ã€EVPåˆ†æçµæœã€‘:
{json.dumps(analysis_data.get('evp', {}), ensure_ascii=False, indent=2)}

ã€ãƒ“ã‚¸ãƒã‚¹åˆ†æçµæœã€‘:
{json.dumps(analysis_data.get('business_analysis', {}), ensure_ascii=False, indent=2)}
"""
        
        # Step 2: æ—¢å­˜ã‚½ãƒ¼ã‚¹ã‹ã‚‰è¿½åŠ æƒ…å ±ã‚’æ¤œç´¢
        st.info("ğŸ” é–¢é€£æƒ…å ±ã‚’ä¼æ¥­ã‚µã‚¤ãƒˆã‹ã‚‰æ¤œç´¢ä¸­...")
        additional_sources = self.search_existing_sources(question, {
            'company_domain': company_info.get('company_domain')
        })
        
        # è¿½åŠ æƒ…å ±ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ
        additional_context = ""
        if additional_sources:
            st.success(f"âœ… {len(additional_sources)}ä»¶ã®è¿½åŠ æƒ…å ±ã‚’ç™ºè¦‹")
            additional_context = "\nã€è¿½åŠ åé›†æƒ…å ±ã€‘:\n"
            for i, source in enumerate(additional_sources, 1):
                additional_context += f"\n{i}. {source['source_type']} ({source['url']}):\n{source['content']}\n"
        else:
            st.info("â„¹ï¸ è¿½åŠ æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚åˆ†æçµæœã®ã¿ã§å›ç­”ã—ã¾ã™ã€‚")
        
        # Step 3: ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®æ•´ç†
        history_context = ""
        if chat_history:
            history_context = "ã€éå»ã®è³ªç–‘å¿œç­”ã€‘:\n"
            for q, a in chat_history[-2:]:  # ç›´è¿‘2ä»¶ã®ã¿
                history_context += f"Q: {q}\nA: {a}\n\n"
        
        # Step 4: æ‹¡å¼µãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        enhanced_prompt = f"""
ã‚ãªãŸã¯ä¼æ¥­åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã‚’å®ˆã£ã¦å›ç­”ã—ã¦ãã ã•ã„ï¼š

ã€å›ç­”ãƒ«ãƒ¼ãƒ«ã€‘
1. æä¾›ã•ã‚ŒãŸåˆ†æçµæœã¨è¿½åŠ åé›†æƒ…å ±ã‚’å„ªå…ˆçš„ã«å‚ç…§
2. æƒ…å ±æºã‚’æ˜è¨˜ï¼šã€Œåˆ†æçµæœã«ã‚ˆã‚‹ã¨ã€ã€Œä¼æ¥­ã‚µã‚¤ãƒˆã®â—‹â—‹æƒ…å ±ã«ã‚ˆã‚‹ã¨ã€
3. ãƒ‡ãƒ¼ã‚¿ã«ãªã„æƒ…å ±ã¯ã€Œæä¾›ã•ã‚ŒãŸæƒ…å ±ã«ã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€ã¨æ˜è¨˜
4. å›ç­”ã¯300-400æ–‡å­—ç¨‹åº¦ã§å…·ä½“çš„ã«
5. æƒ…å ±ã®å‡ºå…¸URLè¡¨ç¤ºï¼ˆè¿½åŠ æƒ…å ±ãŒã‚ã‚‹å ´åˆï¼‰

{base_context}

{additional_context}

{history_context}

ç¾åœ¨ã®è³ªå•: {question}

ä¸Šè¨˜ã®æƒ…å ±ã‚’ä½¿ç”¨ã—ã¦ã€å…·ä½“çš„ã§æœ‰ç”¨ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
æƒ…å ±æºãŒåˆ†æçµæœã‹è¿½åŠ åé›†æƒ…å ±ã‹ã‚’æ˜è¨˜ã—ã¦ãã ã•ã„ã€‚
"""
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": enhanced_prompt}],
                temperature=0.2,
                max_tokens=500
            )
            
            answer = response.choices[0].message.content.strip()
            
            # å‡ºå…¸æƒ…å ±ã‚’è¿½åŠ 
            if additional_sources:
                answer += "\n\nğŸ“š **å‚ç…§ã—ãŸè¿½åŠ æƒ…å ±:**"
                for source in additional_sources:
                    answer += f"\nâ€¢ {source['source_type']}: {source['url']}"
            
            return answer
            
        except Exception as e:
            return f"âŒ å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\n\nåˆ†æãƒ‡ãƒ¼ã‚¿ã‚’å‚ç…§ã—ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
    
    def get_openai_api_key(self):
        """APIã‚­ãƒ¼å–å¾—ï¼ˆæœ¬ç•ªç’°å¢ƒå¯¾å¿œï¼‰"""
        # Streamlit Cloud ã®Secretsæ©Ÿèƒ½ã‚’å„ªå…ˆ
        if hasattr(st, 'secrets') and "OPENAI_API_KEY" in st.secrets:
            return st.secrets["OPENAI_API_KEY"]
        # ç’°å¢ƒå¤‰æ•°ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        elif os.getenv("OPENAI_API_KEY"):
            return os.getenv("OPENAI_API_KEY")
        else:
            st.error("âš ï¸ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            st.markdown("""
            **è¨­å®šæ–¹æ³•:**
            - Streamlit Cloud: Secretsæ©Ÿèƒ½ã§OPENAI_API_KEYã‚’è¨­å®š
            - ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œ: ç’°å¢ƒå¤‰æ•°ã§OPENAI_API_KEYã‚’è¨­å®š
            """)
            st.stop()
    
    def create_research_prompt(self, company_info):
        """å·¥å¤«ã•ã‚ŒãŸãƒªã‚µãƒ¼ãƒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ"""
        prompt = f"""
ã‚ãªãŸã¯ä¼æ¥­ãƒªã‚µãƒ¼ãƒã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ä¼æ¥­ã«ã¤ã„ã¦ã€EVPï¼ˆEmployee Value Propositionï¼‰ã¨ä¼æ¥­åˆ†æã®å„é …ç›®ã‚’è©³ç´°ã«èª¿æŸ»ã—ã€å…·ä½“çš„ãªæƒ…å ±ã‚’åé›†ã—ã¦ãã ã•ã„ã€‚

## èª¿æŸ»å¯¾è±¡ä¼æ¥­
- ä¼æ¥­å: {company_info['company_name']}
- ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸: {company_info.get('website_url', 'ä¸æ˜')}
- é‡ç‚¹åˆ†é‡: {company_info['focus_area']}

### EVPï¼ˆEmployee Value Propositionï¼‰é …ç›®

#### 1. Rewardsï¼ˆå ±é…¬ãƒ»å¾…é‡ï¼‰
- åŸºæœ¬çµ¦ä¸æ°´æº–ã€è³ä¸åˆ¶åº¦ã€ç¦åˆ©åšç”Ÿã€å„ç¨®æ‰‹å½“ã‚„ã‚¤ãƒ³ã‚»ãƒ³ãƒ†ã‚£ãƒ–

#### 2. Opportunityï¼ˆæ©Ÿä¼šãƒ»æˆé•·ï¼‰
- ç ”ä¿®åˆ¶åº¦ã€ã‚­ãƒ£ãƒªã‚¢ãƒ‘ã‚¹ã€æ˜‡é€²åˆ¶åº¦ã€æµ·å¤–é§åœ¨æ©Ÿä¼šã€è³‡æ ¼å–å¾—æ”¯æ´

#### 3. Organizationï¼ˆçµ„ç¹”ãƒ»ä¼æ¥­æ–‡åŒ–ï¼‰
- ä¼æ¥­ç†å¿µã€ãƒŸãƒƒã‚·ãƒ§ãƒ³ã€ä¼æ¥­æ–‡åŒ–ã€ãƒ–ãƒ©ãƒ³ãƒ‰åŠ›ã€çµ„ç¹”é¢¨åœŸ

#### 4. Peopleï¼ˆäººæãƒ»ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆï¼‰
- ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚¿ã‚¤ãƒ«ã€ãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€å¿ƒç†çš„å®‰å…¨æ€§ã€å¤šæ§˜æ€§ãƒ»ã‚¤ãƒ³ã‚¯ãƒ«ãƒ¼ã‚¸ãƒ§ãƒ³

#### 5. Workï¼ˆåƒãæ–¹ãƒ»æ¥­å‹™ï¼‰
- å‹¤å‹™æ™‚é–“ã€ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯ã€ãƒ¯ãƒ¼ã‚¯ãƒ©ã‚¤ãƒ•ãƒãƒ©ãƒ³ã‚¹ã€æ¥­å‹™ã®å°‚é–€æ€§

### ä¼æ¥­åˆ†æé …ç›®ï¼ˆå„é …ç›®ã¯300-400æ–‡å­—ç¨‹åº¦ã§å…·ä½“çš„ã«è¨˜è¼‰ï¼‰

#### 1. æ¥­ç•Œãƒ»å¸‚å ´åˆ†æ
- ä¼æ¥­ãŒå±ã™ã‚‹ä¸»è¦æ¥­ç•Œã®ç‰¹å¾´ã¨å¸‚å ´è¦æ¨¡ï¼ˆå…·ä½“çš„ãªæ•°å€¤å¿…é ˆï¼‰
- æ¥­ç•Œå…¨ä½“ã®å¹´æˆé•·ç‡ã¨éå»3-5å¹´ã®ãƒˆãƒ¬ãƒ³ãƒ‰
- æ¥­ç•Œã®ä¸»è¦ãªæŠ€è¡“é©æ–°ã¨å¤‰åŒ–ï¼ˆå…·ä½“ä¾‹ã‚’å«ã‚€ï¼‰
- å‘ã“ã†5-10å¹´ã®æ¥­ç•Œå°†æ¥æ€§ã¨æˆé•·è¦‹é€šã—
- æ¥­ç•ŒãŒç›´é¢ã—ã¦ã„ã‚‹ä¸»è¦ãªèª²é¡Œã¨æ–°ãŸãªæ©Ÿä¼š
- **å¿…é ˆ**: æ¥­ç•Œå…¨ä½“ã®ç«¶äº‰çŠ¶æ³ã¨å‚å…¥ä¼æ¥­æ•°ã®æ¦‚æ³

#### 2. æ¥­ç•Œå†…ãƒã‚¸ã‚·ãƒ§ãƒ³ï¼ˆç«¶åˆæ¯”è¼ƒå¿…é ˆï¼‰
- å£²ä¸Šé«˜ã®æ¥­ç•Œå†…é †ä½ã¨å…·ä½“çš„ãªå¸‚å ´ã‚·ã‚§ã‚¢ï¼ˆï¼…è¡¨è¨˜ï¼‰
- ä¸»è¦ç«¶åˆä¼æ¥­ã‚’3-5ç¤¾æ˜è¨˜ã—ã€ãã‚Œã‚‰ã¨ã®å£²ä¸Šãƒ»è¦æ¨¡æ¯”è¼ƒ
- å–¶æ¥­åˆ©ç›Šç‡ãƒ»ROEç­‰ã®åç›Šæ€§æŒ‡æ¨™ã¨æ¥­ç•Œå¹³å‡ã¨ã®æ¯”è¼ƒ
- éå»3-5å¹´ã®å£²ä¸Šæˆé•·ç‡ãƒ»åˆ©ç›Šæˆé•·ç‡ã¨ç«¶åˆä»–ç¤¾æ¯”è¼ƒ
- æ ªä¾¡ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»æ™‚ä¾¡ç·é¡ã®æ¥­ç•Œå†…ä½ç½®ã¥ã‘
- **å¿…é ˆ**: ã€Œâ—‹â—‹ç¤¾ã¨æ¯”è¼ƒã—ã¦ã€ã¨ã„ã†å½¢ã§å…·ä½“çš„ç«¶åˆä¼æ¥­åã‚’æŒ™ã’ã¦åˆ†æ

#### 3. ç‹¬è‡ªæ€§ãƒ»å·®åˆ¥åŒ–è¦å› ï¼ˆå…·ä½“çš„äº‹ä¾‹é‡è¦–ï¼‰
- ç«¶åˆä»–ç¤¾ãŒçœŸä¼¼ã§ããªã„ç‹¬è‡ªæŠ€è¡“ãƒ»ã‚µãƒ¼ãƒ“ã‚¹ãƒ»ãƒ“ã‚¸ãƒã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆå…·ä½“åç§°ï¼‰
- ä¿æœ‰ã™ã‚‹ç‰¹è¨±æ•°ãƒ»çŸ¥çš„è²¡ç”£ãƒ»ç‹¬è‡ªãƒã‚¦ãƒã‚¦ã®æ¦‚è¦
- ãƒ–ãƒ©ãƒ³ãƒ‰èªçŸ¥åº¦ãƒ»é¡§å®¢ãƒ­ã‚¤ãƒ¤ãƒªãƒ†ã‚£ã®æ•°å€¤çš„æŒ‡æ¨™ã‚„èª¿æŸ»çµæœ
- R&DæŠ•è³‡é¡ãƒ»æŠ•è³‡æ¯”ç‡ã¨ç«¶åˆä»–ç¤¾ã¨ã®æ¯”è¼ƒ
- å‚å…¥éšœå£ã¨ãªã‚‹ç‹¬è‡ªè³‡ç”£ï¼ˆè¨­å‚™ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€ãƒ‡ãƒ¼ã‚¿ç­‰ï¼‰
- **å¿…é ˆ**: ä»£è¡¨çš„ãªè£½å“ãƒ»ã‚µãƒ¼ãƒ“ã‚¹åã‚’æ˜è¨˜ã—ã€ãªãœãã‚ŒãŒå·®åˆ¥åŒ–è¦å› ãªã®ã‹ã‚’èª¬æ˜

#### 4. äº‹æ¥­ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªåˆ†æï¼ˆå…·ä½“çš„æ•°å€¤ãƒ»äº‹æ¥­åé‡è¦–ï¼‰
- ä¸»è¦äº‹æ¥­ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã¨å£²ä¸Šãƒ»åˆ©ç›Šæ§‹æˆæ¯”ï¼ˆï¼…è¡¨è¨˜å¿…é ˆã€å…·ä½“çš„ãªäº‹æ¥­åæ˜è¨˜ï¼‰
- å„äº‹æ¥­ã®æˆé•·æ€§ï¼šã€Œä¼¸ã³ã¦ã„ã‚‹åˆ†é‡ã€ã®ç‰¹å®šã¨éå»3-5å¹´ã®æˆé•·ç‡ãƒ‡ãƒ¼ã‚¿
- åç›Šæ€§åˆ†æï¼šã€Œé‡‘ã®ãªã‚‹æœ¨ã€äº‹æ¥­ã®ç‰¹å®šã¨å–¶æ¥­åˆ©ç›Šç‡ãƒ»åç›Šè²¢çŒ®åº¦
- æˆ¦ç•¥çš„é‡ç‚¹é ˜åŸŸï¼šã©ã®åˆ†é‡ã¸ã®æŠ•è³‡æ‹¡å¤§ãƒ»M&Aãƒ»æ–°è¦å‚å…¥ã‚’é€²ã‚ã¦ã„ã‚‹ã‹
- äº‹æ¥­é–“ã‚·ãƒŠã‚¸ãƒ¼åŠ¹æœã¨ç›¸äº’ä½œç”¨ã®å…·ä½“ä¾‹ï¼ˆè£½å“ãƒ»æŠ€è¡“ãƒ»é¡§å®¢åŸºç›¤ã®å…±æœ‰ç­‰ï¼‰
- **å¿…é ˆ**: å„äº‹æ¥­ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®å…·ä½“åç§°ã€ä»£è¡¨çš„è£½å“ãƒ»ã‚µãƒ¼ãƒ“ã‚¹åã‚’æ˜è¨˜

## å›ç­”å½¢å¼
JSONå½¢å¼ã§ä»¥ä¸‹ã®é€šã‚Šå›ç­”ã—ã¦ãã ã•ã„ï¼š

```json
{{
  "evp": {{
    "rewards": "å…·ä½“çš„ãªå ±é…¬ãƒ»å¾…é‡æƒ…å ±",
    "opportunity": "å…·ä½“çš„ãªæˆé•·æ©Ÿä¼šæƒ…å ±",
    "organization": "å…·ä½“çš„ãªçµ„ç¹”ãƒ»ä¼æ¥­æ–‡åŒ–æƒ…å ±",
    "people": "å…·ä½“çš„ãªäººæãƒ»ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆæƒ…å ±",
    "work": "å…·ä½“çš„ãªåƒãæ–¹ãƒ»æ¥­å‹™æƒ…å ±"
  }},
  "business_analysis": {{
    "industry_market": "å…·ä½“çš„ãªæ¥­ç•Œãƒ»å¸‚å ´åˆ†æ",
    "market_position": "å…·ä½“çš„ãªæ¥­ç•Œå†…ãƒã‚¸ã‚·ãƒ§ãƒ³",
    "differentiation": "å…·ä½“çš„ãªç‹¬è‡ªæ€§ãƒ»å·®åˆ¥åŒ–è¦å› ",
    "business_portfolio": "å…·ä½“çš„ãªäº‹æ¥­ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªåˆ†æ"
  }}
}}
```

å„é …ç›®ã¯**300-400æ–‡å­—ã§å…·ä½“çš„ã«**è¨˜è¼‰ã—ã€ä»¥ä¸‹ã‚’å¿…ãšå«ã‚ã¦ãã ã•ã„ï¼š
- **æ•°å€¤ãƒ‡ãƒ¼ã‚¿**: å£²ä¸Šé«˜ã€å¸‚å ´ã‚·ã‚§ã‚¢ã€æˆé•·ç‡ç­‰ã®å…·ä½“çš„æ•°å­—
- **ç«¶åˆä¼æ¥­å**: ä¸»è¦ç«¶åˆä»–ç¤¾ã‚’æ˜è¨˜ï¼ˆã€Œâ—‹â—‹ç¤¾ã¨æ¯”è¼ƒã—ã¦ã€ï¼‰
- **å›ºæœ‰åè©**: è£½å“åã€ã‚µãƒ¼ãƒ“ã‚¹åã€æŠ€è¡“åç­‰ã®å…·ä½“åç§°
- **æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿**: ã€Œéå»â—‹å¹´ã§ã€ã€Œè¿‘å¹´ã§ã¯ã€ç­‰ã®å¤‰åŒ–ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰
- **æƒ…å ±æºã®æ˜ç¢ºåŒ–**: ãƒ‡ãƒ¼ã‚¿ãŒä¸æ˜ãªå ´åˆã¯ã€Œå…¬é–‹æƒ…å ±ã§ã¯ç¢ºèªã§ããšã€ã¨æ˜è¨˜

**ç¦æ­¢è¡¨ç¾**: ã€Œä¸€èˆ¬çš„ã«ã€ã€Œå¤šãã®ä¼æ¥­ã¨åŒæ§˜ã€ã€Œæ¥­ç•Œæ¨™æº–çš„ã€ãªã©ã®æ›–æ˜§ãªè¡¨ç¾ã¯ä½¿ç”¨ç¦æ­¢
"""
        return prompt
    
    def research_company(self, company_info):
        """LLMã«ä¼æ¥­èª¿æŸ»ã‚’ä¾é ¼ï¼ˆIRæ©Ÿèƒ½ä¸€æ™‚ç„¡åŠ¹åŒ–ã€å¾“æ¥æ–¹å¼ã§å®‰å®šåŒ–ï¼‰"""
        
        # IRæƒ…å ±åé›†ã‚’ä¸€æ™‚ç„¡åŠ¹åŒ–
        if False:  # IRæ©Ÿèƒ½ã‚’ç„¡åŠ¹åŒ–
            st.info("ğŸ” IRæƒ…å ±ã‚’è‡ªå‹•åé›†ä¸­...")
            try:
                crawler = SmartIRCrawler(
                    company_info['company_domain'],
                    company_info.get('ir_top_url'),
                    max_depth=2,
                    date_limit_years=3
                )
                ir_data = crawler.crawl_with_intelligence()
                
                if ir_data:
                    st.success(f"âœ… {len(ir_data)}ä»¶ã®IRæƒ…å ±ã‚’åé›†ã—ã¾ã—ãŸ")
                else:
                    st.info("â„¹ï¸ IRæƒ…å ±ã®åé›†ã«å¤±æ•—ã—ã¾ã—ãŸã€‚å¾“æ¥ã®åˆ†ææ–¹æ³•ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            except Exception as e:
                st.warning(f"âš ï¸ IRåé›†ã‚¨ãƒ©ãƒ¼: {str(e)} - å¾“æ¥ã®åˆ†ææ–¹æ³•ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                ir_data = []
        
        # å¾“æ¥ã®å®‰å®šã—ãŸåˆ†ææ–¹å¼ã‚’ä½¿ç”¨
        st.info("ğŸ” å®‰å®šã—ãŸæ±ç”¨åˆ†æã‚’å®Ÿè¡Œä¸­...")
        prompt = self.create_research_prompt(company_info)
        temperature = 0.3
        ir_data = []  # IRæƒ…å ±ã‚’ã‚¯ãƒªã‚¢
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ä¼æ¥­ãƒªã‚µãƒ¼ãƒã®å°‚é–€å®¶ã¨ã—ã¦ã€æ­£ç¢ºã§å…·ä½“çš„ãªæƒ…å ±ã‚’JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=6000,
                temperature=temperature
            )
            
            content = response.choices[0].message.content.strip()
            
            # JSONãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            if "```json" in content:
                json_start = content.find("```json") + 7
                json_end = content.find("```", json_start)
                json_content = content[json_start:json_end].strip()
            else:
                json_content = content
            
            research_data = json.loads(json_content)
            
            # IRé–¢é€£ã®å‡¦ç†ã¯ã‚¹ã‚­ãƒƒãƒ—
            # å¾“æ¥é€šã‚Šã®åˆ†æçµæœã®ã¿è¿”å´
            
            return research_data
            
        except Exception as e:
            st.error(f"AIèª¿æŸ»ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return None
    
    def save_results(self, company_info, research_data):
        """çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"research_{company_info['company_name']}_{timestamp}.json"
        filepath = self.results_dir / filename
        
        save_data = {
            "company_info": company_info,
            "research_results": research_data,
            "generated_at": datetime.now().isoformat()
        }
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            return filepath, save_data
        except:
            # æœ¬ç•ªç’°å¢ƒã§ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã«å¤±æ•—ã—ãŸå ´åˆã¯çµæœã®ã¿è¿”ã™
            return None, save_data

def main():
    st.title("ğŸ¢ AIä¼æ¥­åˆ†æã‚·ã‚¹ãƒ†ãƒ ")
    st.markdown("### ä¼æ¥­ã®EVPãƒ»ãƒ“ã‚¸ãƒã‚¹åˆ†æã‚’è‡ªå‹•åŒ–ã™ã‚‹AIã‚·ã‚¹ãƒ†ãƒ ")
    
    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
    with st.expander("â„¹ï¸ ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±", expanded=False):
        st.markdown("""
        **æ©Ÿèƒ½:**
        - **EVPåˆ†æ**: Rewards, Opportunity, Organization, People, Work
        - **ãƒ“ã‚¸ãƒã‚¹åˆ†æ**: æ¥­ç•Œåˆ†æ, ç«¶åˆæ¯”è¼ƒ, ç‹¬è‡ªæ€§, äº‹æ¥­ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ª
        - **AIè‡ªå‹•èª¿æŸ»**: OpenAI GPT-4o-miniä½¿ç”¨
        - **çµæœä¿å­˜**: JSONå½¢å¼ã§ã®è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ
        
        **å…¥åŠ›é …ç›®:**
        - ä¼æ¥­åï¼ˆå¿…é ˆï¼‰
        - ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸URLï¼ˆä»»æ„ï¼‰
        - åˆ†æé‡ç‚¹åˆ†é‡ï¼ˆå¿…é ˆï¼‰
        """)
    
    # å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
    with st.form("company_form"):
        col1, col2 = st.columns(2)
        
        with col1:
            company_name = st.text_input(
                "ğŸ¢ ä¼æ¥­å *", 
                placeholder="ä¾‹: ãƒˆãƒ¨ã‚¿è‡ªå‹•è»Šã€ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯ã€ãƒªã‚¯ãƒ«ãƒ¼ãƒˆ",
                help="åˆ†æå¯¾è±¡ã®ä¼æ¥­åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"
            )
            website_url = st.text_input(
                "ğŸŒ ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸URL", 
                placeholder="ä¾‹: https://www.company.co.jp/",
                help="ä¼æ¥­ã®å…¬å¼ã‚µã‚¤ãƒˆURLï¼ˆIRæ¢ç´¢ã«ã‚‚ä½¿ç”¨ã•ã‚Œã¾ã™ï¼‰"
            )
        
        with col2:
            focus_area = st.text_input(
                "ğŸ¯ åˆ†æé‡ç‚¹åˆ†é‡ *", 
                placeholder="ä¾‹: æ–°å’æ¡ç”¨ã€ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢æ¡ç”¨ã€ä¸­é€”æ¡ç”¨",
                help="ã©ã®åˆ†é‡ã«é‡ç‚¹ã‚’ç½®ã„ã¦åˆ†æã™ã‚‹ã‹ã‚’æŒ‡å®š"
            )
            
            # åˆ†æãƒ¬ãƒ™ãƒ«é¸æŠ
            analysis_level = st.selectbox(
                "ğŸ“Š åˆ†æãƒ¬ãƒ™ãƒ«",
                ["æ¨™æº–åˆ†æ", "è©³ç´°åˆ†æ"],
                help="è©³ç´°åˆ†æã§ã¯æ›´ã«æ·±ã„èª¿æŸ»ã‚’å®Ÿæ–½ã—ã¾ã™"
            )
        
        # è©³ç´°è¨­å®šï¼ˆä¸Šç´šè€…å‘ã‘ï¼‰
        with st.expander("âš™ï¸ è©³ç´°è¨­å®š", expanded=False):
            col3, col4 = st.columns(2)
            with col3:
                date_range = st.selectbox("æƒ…å ±ç¯„å›²", ["3å¹´ä»¥å†…", "2å¹´ä»¥å†…", "1å¹´ä»¥å†…"], index=0)
                enable_chat = st.checkbox("åˆ†æå¾Œãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½", value=True, help="åˆ†æçµæœã«é–¢ã™ã‚‹è¿½åŠ è³ªå•ãŒå¯èƒ½ã«ãªã‚Šã¾ã™")
            with col4:
                # IRé–¢é€£è¨­å®šã¯éè¡¨ç¤ºï¼ˆå°†æ¥ã®æ‹¡å¼µç”¨ï¼‰
                max_crawl_depth = 2  # å›ºå®šå€¤
                enable_hallucination_check = False  # IRæ©Ÿèƒ½ç„¡åŠ¹æ™‚ã¯OFF
        
        st.markdown("---")
        submitted = st.form_submit_button("ğŸ” AIåˆ†æé–‹å§‹", type="primary", use_container_width=True)
    
    # ãƒ•ã‚©ãƒ¼ãƒ é€ä¿¡æ™‚ã®å‡¦ç†
    if submitted:
        if not company_name or not focus_area:
            st.error("ğŸš¨ ä¼æ¥­åã¨åˆ†æé‡ç‚¹åˆ†é‡ã¯å¿…é ˆå…¥åŠ›ã§ã™ã€‚")
            return
        
        # èª¿æŸ»ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å…ˆã«åˆæœŸåŒ–
        researcher = StreamlitCompanyResearcher()
        
        # ä¼šç¤¾æƒ…å ±ã®æº–å‚™
        company_domain = researcher.extract_domain_from_url(website_url)
        company_info = {
            "company_name": company_name,
            "website_url": website_url,
            "company_domain": company_domain,
            "focus_area": focus_area,
            "analysis_level": analysis_level,
            "max_crawl_depth": max_crawl_depth,
            "date_range": date_range,
            "enable_hallucination_check": enable_hallucination_check,
            "enable_chat": enable_chat,
            "timestamp": datetime.now().isoformat()
        }
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ä»˜ãã§å®Ÿè¡Œ
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        status_text.text("ğŸ” AIåˆ†æã‚’é–‹å§‹ã—ã¦ã„ã¾ã™...")
        progress_bar.progress(20)
        
        with st.spinner("ğŸ¤– AIåˆ†æä¸­... (30-60ç§’ç¨‹åº¦ãŠå¾…ã¡ãã ã•ã„)"):
            progress_bar.progress(50)
            research_data = researcher.research_company(company_info)
            progress_bar.progress(80)
        
        if research_data:
            # çµæœä¿å­˜
            filepath, save_data = researcher.save_results(company_info, research_data)
            progress_bar.progress(100)
            status_text.text("âœ… åˆ†æå®Œäº†ï¼")
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«åˆ†æçµæœã‚’ä¿å­˜
            st.session_state.analysis_results = {
                "research_data": research_data,
                "company_info": company_info,
                "save_data": save_data,
                "filepath": filepath,
                "researcher": researcher
            }
        else:
            progress_bar.progress(0)
            status_text.text("âŒ åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ")
            st.error("âŒ AIåˆ†æã«å¤±æ•—ã—ã¾ã—ãŸã€‚APIã‚­ãƒ¼ã®è¨­å®šã¾ãŸã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    
    # åˆ†æçµæœã®è¡¨ç¤ºï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‹ã‚‰ï¼‰
    if 'analysis_results' in st.session_state:
        results = st.session_state.analysis_results
        research_data = results["research_data"]
        company_info = results["company_info"]
        save_data = results["save_data"]
        filepath = results["filepath"]
        researcher = results["researcher"]
        
        # åˆ†æçµæœã®æ§‹é€ ç¢ºèªï¼ˆç°¡ç´ åŒ–ï¼‰
        st.info("ğŸ“ åˆ†æçµæœ:")
        st.json({
            "ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¼": list(research_data.keys()),
            "EVPé …ç›®æ•°": len(research_data.get('evp', {})),
            "ãƒ“ã‚¸ãƒã‚¹åˆ†æé …ç›®æ•°": len(research_data.get('business_analysis', {}))
        })
        
        # çµæœè¡¨ç¤º
        st.success("ğŸ‰ AIåˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        
        # åŸºæœ¬æƒ…å ±è¡¨ç¤º
        st.markdown("---")
        st.subheader("ğŸ“Š åˆ†æçµæœã‚µãƒãƒªãƒ¼")
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ğŸ¢ ä¼æ¥­å", company_info["company_name"])
        with col2:
            st.metric("ğŸ¯ é‡ç‚¹åˆ†é‡", company_info["focus_area"])
        with col3:
            st.metric("ğŸ“Š åˆ†æãƒ¬ãƒ™ãƒ«", company_info["analysis_level"])
        with col4:
            st.metric("ğŸ• åˆ†ææ—¥æ™‚", company_info['timestamp'][:19])
        
        st.markdown("---")
            
        
        # ã‚¿ãƒ–å½¢å¼ã§çµæœè¡¨ç¤º
        tab1, tab2, tab3 = st.tabs(["ğŸ“ˆ EVPåˆ†æ", "ğŸ† ãƒ“ã‚¸ãƒã‚¹åˆ†æ", "ğŸ“„ JSONå‡ºåŠ›"])
        
        with tab1:
            st.subheader("ğŸ“ˆ EVPï¼ˆEmployee Value Propositionï¼‰åˆ†æ")
            
            evp_labels = {
                "rewards": "ğŸ’° Rewardsï¼ˆå ±é…¬ãƒ»å¾…é‡ï¼‰",
                "opportunity": "ğŸš€ Opportunityï¼ˆæ©Ÿä¼šãƒ»æˆé•·ï¼‰",
                "organization": "ğŸ¢ Organizationï¼ˆçµ„ç¹”ãƒ»ä¼æ¥­æ–‡åŒ–ï¼‰",
                "people": "ğŸ‘¥ Peopleï¼ˆäººæãƒ»ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆï¼‰",
                "work": "ğŸ’¼ Workï¼ˆåƒãæ–¹ãƒ»æ¥­å‹™ï¼‰"
            }
            
            # EVPåˆ†æçµæœã®å®‰å…¨ãªè¡¨ç¤º
            evp_data = research_data.get('evp', {})
            if evp_data:
                for key, label in evp_labels.items():
                    with st.expander(label, expanded=True):
                        content = evp_data.get(key, "åˆ†æãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
                        st.write(content)
            else:
                st.warning("EVPåˆ†æãƒ‡ãƒ¼ã‚¿ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                st.json(research_data)  # ãƒ‡ãƒãƒƒã‚°ç”¨
        
        with tab2:
            st.subheader("ğŸ† ãƒ“ã‚¸ãƒã‚¹åˆ†æ")
            
            business_labels = {
                "industry_market": "ğŸ“ˆ æ¥­ç•Œãƒ»å¸‚å ´åˆ†æ",
                "market_position": "ğŸ† æ¥­ç•Œå†…ãƒã‚¸ã‚·ãƒ§ãƒ³",
                "differentiation": "â­ ç‹¬è‡ªæ€§ãƒ»å·®åˆ¥åŒ–è¦å› ",
                "business_portfolio": "ğŸ—ï¸ äº‹æ¥­ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªåˆ†æ"
            }
            
            # ãƒ“ã‚¸ãƒã‚¹åˆ†æçµæœã®å®‰å…¨ãªè¡¨ç¤º
            business_data = research_data.get('business_analysis', {})
            if business_data:
                for key, label in business_labels.items():
                    with st.expander(label, expanded=True):
                        content = business_data.get(key, "åˆ†æãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
                        st.write(content)
            else:
                st.warning("ãƒ“ã‚¸ãƒã‚¹åˆ†æãƒ‡ãƒ¼ã‚¿ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                st.json(research_data)  # ãƒ‡ãƒãƒƒã‚°ç”¨
        
        with tab3:
            st.subheader("ğŸ“„ JSONå½¢å¼ã®åˆ†æçµæœ")
            st.markdown("åˆ†æçµæœã‚’JSONå½¢å¼ã§è¡¨ç¤ºã—ã¾ã™ã€‚ã‚³ãƒ”ãƒ¼ã—ã¦ä»–ã®ã‚·ã‚¹ãƒ†ãƒ ã§ã‚‚æ´»ç”¨ã§ãã¾ã™ã€‚")
            
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
            json_output = json.dumps(save_data, ensure_ascii=False, indent=2)
            st.download_button(
                label="ğŸ’¾ JSONçµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=json_output,
                file_name=f"research_{company_info['company_name']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json"
            )
            
            # JSONè¡¨ç¤º
            st.code(json_output, language="json")
            
            if filepath:
                st.info(f"ğŸ’¾ çµæœã¯ã‚µãƒ¼ãƒãƒ¼ã«ã‚‚ä¿å­˜ã•ã‚Œã¾ã—ãŸ: {filepath}")
        
        # ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ï¼ˆåˆ†æçµæœå¾Œã®ã¿è¡¨ç¤ºï¼‰
        if company_info.get('enable_chat', True):
            st.markdown("---")
            st.subheader("ğŸ’¬ åˆ†æçµæœã«é–¢ã™ã‚‹è¿½åŠ è³ªå•")
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
            if 'chat_history' not in st.session_state:
                st.session_state.chat_history = []
            if 'analysis_context' not in st.session_state:
                st.session_state.analysis_context = None
            
            # åˆ†æçµæœã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ä¿å­˜
            if st.session_state.analysis_context != research_data:
                st.session_state.analysis_context = research_data
                st.session_state.chat_history = []  # æ–°ã—ã„åˆ†ææ™‚ã¯ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆ
            
            # åˆ†æçµæœãƒ™ãƒ¼ã‚¹ã®è­¦å‘Š
            st.warning("âš ï¸ ã“ã®è³ªå•æ©Ÿèƒ½ã¯åˆ†æçµæœã«åŸºã¥ã„ã¦å›ç­”ã—ã¾ã™ã€‚åˆ†æãƒ‡ãƒ¼ã‚¿ä»¥å¤–ã®æƒ…å ±ã¯æä¾›ã§ãã¾ã›ã‚“ã€‚")
            
            # ãƒãƒ£ãƒƒãƒˆå±¥æ­´è¡¨ç¤º
            for i, (question, answer) in enumerate(st.session_state.chat_history):
                with st.chat_message("user"):
                    st.write(question)
                with st.chat_message("assistant"):
                    st.write(answer)
            
            # è³ªå•å…¥åŠ›
            user_question = st.chat_input("åˆ†æçµæœã«ã¤ã„ã¦è³ªå•ã—ã¦ãã ã•ã„...")
            
            if user_question:
                # è³ªå•ã‚’å±¥æ­´ã«è¿½åŠ 
                with st.chat_message("user"):
                    st.write(user_question)
                
                # AIå›ç­”ç”Ÿæˆ
                with st.chat_message("assistant"):
                    with st.spinner("å›ç­”ã‚’ç”Ÿæˆä¸­..."):
                        answer = researcher.generate_chat_response(
                            user_question, 
                            research_data, 
                            company_info,
                            st.session_state.chat_history
                        )
                        st.write(answer)
                
                # å±¥æ­´ã«è¿½åŠ ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’æ›´æ–°ï¼‰
                st.session_state.chat_history.append((user_question, answer))
            
            # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ãƒªã‚»ãƒƒãƒˆãƒœã‚¿ãƒ³
            if st.button("ğŸ—‘ï¸ ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆ"):
                st.session_state.chat_history = []
                st.success("ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸã€‚")
                
        # æ–°ã—ã„åˆ†æã‚’é–‹å§‹ã™ã‚‹ãƒœã‚¿ãƒ³
        if st.button("ğŸ”„ æ–°ã—ã„åˆ†æã‚’é–‹å§‹"):
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢
            for key in ['analysis_results', 'chat_history', 'analysis_context']:
                if key in st.session_state:
                    del st.session_state[key]
            st.rerun()

    # ãƒ•ãƒƒã‚¿ãƒ¼
    st.markdown("---")
    st.markdown(
        """
        <div style="text-align: center; color: #666;">
            ğŸ” AIä¼æ¥­åˆ†æã‚·ã‚¹ãƒ†ãƒ  | Powered by OpenAI GPT-4o-mini
        </div>
        """, 
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()