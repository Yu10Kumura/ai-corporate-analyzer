#!/usr/bin/env python3
"""
ä¼æ¥­EVPãƒ»ä¼æ¥­åˆ†æã‚·ã‚¹ãƒ†ãƒ  - Streamlit Webç‰ˆï¼ˆæœ¬ç•ªç’°å¢ƒå¯¾å¿œï¼‰
"""

import streamlit as st
import os
import json
import datetime
import re
import requests
from pathlib import Path
from openai import OpenAI
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from datetime import datetime, timedelta

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ğŸ¢ AIä¼æ¥­åˆ†æã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="collapsed"
)

class SmartIRCrawler:
    """ã‚¹ãƒãƒ¼ãƒˆIRæƒ…å ±åé›†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾ç­–ä»˜ãï¼‰"""
    
    def __init__(self, company_domain, ir_url=None, max_depth=4, date_limit_years=3):
        self.company_domain = company_domain
        self.ir_url = ir_url or f"https://{company_domain}/ir/"
        self.max_depth = max_depth
        self.date_limit = datetime.now() - timedelta(days=date_limit_years * 365)
        self.discovered_content = []
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
    
    def is_valid_domain(self, url):
        """ä¼æ¥­ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ã¿è¨±å¯"""
        try:
            parsed = urlparse(url)
            return self.company_domain in parsed.netloc
        except:
            return False
    
    def extract_date_from_content(self, content, url):
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º"""
        date_patterns = [
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
            r'(\d{4})-(\d{2})-(\d{2})',
            r'(\d{4})/(\d{1,2})/(\d{1,2})',
            r'å…¬è¡¨æ—¥[ï¼š:\s]*(\d{4}[/-]\d{1,2}[/-]\d{1,2})',
            r'ç™ºè¡¨æ—¥[ï¼š:\s]*(\d{4}[/-]\d{1,2}[/-]\d{1,2})'
        ]
        
        for pattern in date_patterns:
            matches = re.findall(pattern, content)
            if matches:
                try:
                    # æœ€åˆã®ãƒãƒƒãƒã‚’æ—¥ä»˜ã¨ã—ã¦è§£æ
                    match = matches[0]
                    if isinstance(match, tuple):
                        if len(match) == 3:
                            year, month, day = match
                            return datetime(int(year), int(month), int(day))
                except:
                    continue
        
        # æ—¥ä»˜ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç¾åœ¨æ—¥æ™‚ã‚’è¿”ã™
        return datetime.now()
    
    def score_content_importance(self, content, url):
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®é‡è¦åº¦ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°"""
        priority_keywords = {
            "æ±ºç®—çŸ­ä¿¡": 10,
            "æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸": 9,
            "æ±ºç®—èª¬æ˜ä¼š": 8,
            "æ¥­ç¸¾ãƒã‚¤ãƒ©ã‚¤ãƒˆ": 7,
            "ä¸­æœŸçµŒå–¶è¨ˆç”»": 6,
            "æ ªä¸»ç·ä¼š": 5,
            "é©æ™‚é–‹ç¤º": 4,
            "ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªãƒªãƒ¼ã‚¹": 3,
            "IR": 2
        }
        
        score = 0
        content_lower = content.lower()
        url_lower = url.lower()
        
        for keyword, points in priority_keywords.items():
            if keyword in content or keyword in url:
                score += points
        
        # PDFæ–‡æ›¸ã¯é‡è¦åº¦ãŒé«˜ã„
        if '.pdf' in url_lower:
            score += 3
        
        # æ±ºç®—é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        earnings_keywords = ["æ±ºç®—", "æ¥­ç¸¾", "è²¡å‹™", "å£²ä¸Š", "åˆ©ç›Š"]
        for keyword in earnings_keywords:
            if keyword in content:
                score += 2
        
        return score
    
    def discover_ir_links(self, start_url, depth=0):
        """IRãƒšãƒ¼ã‚¸ã‹ã‚‰é‡è¦ãªãƒªãƒ³ã‚¯ã‚’ç™ºè¦‹"""
        if depth > self.max_depth or not self.is_valid_domain(start_url):
            return []
        
        try:
            response = self.session.get(start_url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰æ—¥ä»˜æŠ½å‡º
            page_date = self.extract_date_from_content(response.text, start_url)
            
            # 3å¹´ä»¥å†…ã®æƒ…å ±ã®ã¿
            if page_date < self.date_limit:
                return []
            
            # é‡è¦åº¦ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
            importance_score = self.score_content_importance(response.text, start_url)
            
            discovered = [{
                'url': start_url,
                'content': response.text[:5000],  # æœ€åˆã®5000æ–‡å­—
                'date': page_date,
                'importance': importance_score,
                'title': soup.title.string if soup.title else 'No Title'
            }]
            
            # IRé–¢é€£ã®ãƒªãƒ³ã‚¯ã‚’æ¢ç´¢
            ir_keywords = ['ir', 'æ±ºç®—', 'æ¥­ç¸¾', 'è²¡å‹™', 'æŠ•è³‡å®¶', 'investor']
            
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                if not href:
                    continue
                
                full_url = urljoin(start_url, href)
                
                # è‡ªç¤¾ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ã¿
                if not self.is_valid_domain(full_url):
                    continue
                
                # IRé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ãƒªãƒ³ã‚¯
                link_text = link.get_text().lower()
                if any(keyword in link_text or keyword in href.lower() for keyword in ir_keywords):
                    if depth < self.max_depth - 1:
                        discovered.extend(self.discover_ir_links(full_url, depth + 1))
            
            return discovered
            
        except Exception as e:
            st.warning(f"âš ï¸ URLæ¢ç´¢ã‚¨ãƒ©ãƒ¼: {start_url} - {str(e)}")
            return []
    
    def crawl_with_intelligence(self):
        """ã‚¹ãƒãƒ¼ãƒˆãªIRæƒ…å ±åé›†"""
        try:
            # IRæ¢ç´¢é–‹å§‹
            all_content = self.discover_ir_links(self.ir_url)
            
            # é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆ
            sorted_content = sorted(all_content, key=lambda x: x['importance'], reverse=True)
            
            # ä¸Šä½10ä»¶ã‚’è¿”ã™
            return sorted_content[:10]
            
        except Exception as e:
            st.error(f"âŒ IRæƒ…å ±åé›†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []

class StreamlitCompanyResearcher:
    def __init__(self):
        # APIã‚­ãƒ¼è¨­å®šï¼ˆæœ¬ç•ªç’°å¢ƒå¯¾å¿œï¼‰
        api_key = self.get_openai_api_key()
        
        try:
            self.client = OpenAI(api_key=api_key)
        except Exception as e:
            st.error(f"âŒ OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            st.stop()
            
        # çµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆæœ¬ç•ªç’°å¢ƒã§ã¯ä¸€æ™‚çš„ï¼‰
        self.results_dir = Path('results')
        self.results_dir.mkdir(exist_ok=True)
    
    def validate_response_content(self, response, source_data):
        """ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾ç­–ï¼šå›ç­”å†…å®¹ã®æ¤œè¨¼"""
        # æ¨æ¸¬è¡¨ç¾ã®æ¤œå‡º
        speculation_patterns = [
            r'ã¨æ€ã‚ã‚Œ', r'å¯èƒ½æ€§ãŒ', r'ãŠãã‚‰ã', r'ä¸€èˆ¬çš„ã«', 
            r'é€šå¸¸ã¯', r'äºˆæƒ³', r'æ¨æ¸¬', r'æ†¶æ¸¬', r'ã‹ã‚‚ã—ã‚Œ'
        ]
        
        for pattern in speculation_patterns:
            if re.search(pattern, response):
                return False, f"æ¨æ¸¬çš„è¡¨ç¾ãŒå«ã¾ã‚Œã¦ã„ã¾ã™: {pattern}"
        
        # å‡ºå…¸è¨˜è¼‰ã®ç¢ºèª
        if 'å‡ºå…¸ï¼š' not in response and 'ã‚½ãƒ¼ã‚¹ï¼š' not in response:
            return False, "å‡ºå…¸ãŒæ˜è¨˜ã•ã‚Œã¦ã„ã¾ã›ã‚“"
        
        # åŸºæœ¬çš„ãªäº‹å®Ÿç¢ºèªï¼ˆä¼æ¥­åã®ä¸€è‡´ãªã©ï¼‰
        company_name_in_source = any(source['title'] for source in source_data if source.get('title'))
        if not company_name_in_source and len(source_data) > 0:
            st.warning("âš ï¸ ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã¨ä¼æ¥­åã®æ•´åˆæ€§ã‚’ç¢ºèªä¸­...")
        
        return True, "æ¤œè¨¼OK"
    
    def create_constrained_prompt(self, company_info, ir_data):
        """åˆ¶ç´„ä»˜ããƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ"""
        ir_content = "\n".join([
            f"ã€{item['title']}ã€‘(é‡è¦åº¦: {item['importance']}, æ—¥ä»˜: {item['date'].strftime('%Y-%m-%d')})\n"
            f"URL: {item['url']}\n"
            f"å†…å®¹: {item['content'][:800]}...\n"
            for item in ir_data[:5]  # ä¸Šä½5ä»¶
        ])
        
        system_prompt = f"""
ã‚ãªãŸã¯ä¼æ¥­åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã‚’å³æ ¼ã«å®ˆã£ã¦ãã ã•ã„ï¼š

ã€é‡è¦åˆ¶ç´„ã€‘
1. æä¾›ã•ã‚ŒãŸIRæƒ…å ±ã¨Webãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’å‚ç…§ã™ã‚‹ã“ã¨
2. ãƒ‡ãƒ¼ã‚¿ã«ãªã„æƒ…å ±ã¯ã€Œåˆ†æãƒ‡ãƒ¼ã‚¿ã«ã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€ã¨æ˜è¨˜
3. æ¨æ¸¬ã‚„ä¸€èˆ¬è«–ã§ã¯ãªãã€å…·ä½“çš„ãªæ ¹æ‹ ã‚’ç¤ºã™ã“ã¨
4. å¿…ãšã€Œå‡ºå…¸ï¼š[URL] [å–å¾—æ—¥æ™‚]ã€ã‚’æ˜è¨˜ã™ã‚‹ã“ã¨
5. ã€ŒãŠãã‚‰ãã€ã€Œä¸€èˆ¬çš„ã«ã€ã€Œæ¨æ¸¬ã§ã¯ã€ç­‰ã®è¡¨ç¾ã¯ä½¿ç”¨ç¦æ­¢
6. 3å¹´ä»¥å†…ï¼ˆ2022å¹´1æœˆä»¥é™ï¼‰ã®æƒ…å ±ã®ã¿ä½¿ç”¨ã™ã‚‹ã“ã¨

ã€åˆ†æå¯¾è±¡ä¼æ¥­ã€‘: {company_info['company_name']}
ã€é‡ç‚¹åˆ†é‡ã€‘: {company_info['focus_area']}

ã€åˆ©ç”¨å¯èƒ½ãªIRæƒ…å ±ã€‘:
{ir_content}

ã€åˆ†ææŒ‡ç¤ºã€‘:
ä¸Šè¨˜ã®IRæƒ…å ±ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ã€EVPåˆ†æã¨ãƒ“ã‚¸ãƒã‚¹åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚
ãƒ‡ãƒ¼ã‚¿ã«ãªã„é …ç›®ã«ã¤ã„ã¦ã¯ã€Œãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚åˆ†æã§ãã¾ã›ã‚“ã€ã¨è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚
"""
        
        return system_prompt
    
    def verify_with_double_check(self, question, answer, sources):
        """äºŒæ®µéšæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ """
        verification_prompt = f"""
ä»¥ä¸‹ã®å›ç­”ã«ã¤ã„ã¦äº‹å®Ÿç¢ºèªã‚’è¡Œã£ã¦ãã ã•ã„ï¼š

è³ªå•: {question}
å›ç­”: {answer}

ãƒã‚§ãƒƒã‚¯é …ç›®:
1. æä¾›ã•ã‚ŒãŸã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã‹ï¼Ÿ
2. 3å¹´ä»¥å†…ã®æƒ…å ±ã®ã¿ã‹ï¼Ÿ
3. æ¨æ¸¬ã‚„å¤–éƒ¨çŸ¥è­˜ãŒæ··å…¥ã—ã¦ã„ãªã„ã‹ï¼Ÿ
4. å‡ºå…¸ãŒæ­£ã—ãæ˜è¨˜ã•ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ

å•é¡ŒãŒã‚ã‚Œã°ã€ŒNGï¼šç†ç”±ã€ã€å•é¡Œãªã‘ã‚Œã°ã€ŒOKã€ã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚
"""
        
        try:
            verification_response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": verification_prompt}],
                temperature=0.1,
                max_tokens=100
            )
            
            verification_result = verification_response.choices[0].message.content
            return "OK" in verification_result, verification_result
            
        except Exception as e:
            return False, f"æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def generate_chat_response(self, question, analysis_data, company_info, chat_history):
        """ãƒãƒ£ãƒƒãƒˆè³ªå•ã¸ã®å›ç­”ç”Ÿæˆï¼ˆãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾ç­–ä»˜ãï¼‰"""
        
        # åˆ†æçµæœã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦æ•´ç†
        context = f"""
åˆ†æå¯¾è±¡ä¼æ¥­: {company_info['company_name']}
åˆ†æé‡ç‚¹åˆ†é‡: {company_info['focus_area']}

ã€EVPåˆ†æçµæœã€‘:
{json.dumps(analysis_data.get('evp', {}), ensure_ascii=False, indent=2)}

ã€ãƒ“ã‚¸ãƒã‚¹åˆ†æçµæœã€‘:
{json.dumps(analysis_data.get('business_analysis', {}), ensure_ascii=False, indent=2)}

ã€IRæƒ…å ±ã‚½ãƒ¼ã‚¹ã€‘:
{json.dumps(analysis_data.get('ir_sources', []), ensure_ascii=False, indent=2)}
"""
        
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®æ•´ç†
        history_context = ""
        if chat_history:
            history_context = "ã€éå»ã®è³ªç–‘å¿œç­”ã€‘:\n"
            for q, a in chat_history[-3:]:  # ç›´è¿‘3ä»¶ã®ã¿
                history_context += f"Q: {q}\nA: {a}\n\n"
        
        # åˆ¶ç´„ä»˜ããƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        chat_prompt = f"""
ã‚ãªãŸã¯ä¼æ¥­åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã‚’å³æ ¼ã«å®ˆã£ã¦å›ç­”ã—ã¦ãã ã•ã„ï¼š

ã€é‡è¦åˆ¶ç´„ã€‘
1. æä¾›ã•ã‚ŒãŸåˆ†æçµæœã¨IRæƒ…å ±ã®ã¿ã‚’å‚ç…§ã™ã‚‹ã“ã¨
2. ãƒ‡ãƒ¼ã‚¿ã«ãªã„æƒ…å ±ã¯ã€Œåˆ†æãƒ‡ãƒ¼ã‚¿ã«ã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€ã¨æ˜è¨˜
3. æ¨æ¸¬ã‚„ä¸€èˆ¬è«–ã§ã¯ãªãã€å…·ä½“çš„ãªæ ¹æ‹ ã‚’ç¤ºã™ã“ã¨
4. å¿…ãšå‡ºå…¸ï¼ˆåˆ†æçµæœã®è©²å½“ç®‡æ‰€ï¼‰ã‚’æ˜è¨˜ã™ã‚‹ã“ã¨
5. ã€ŒãŠãã‚‰ãã€ã€Œä¸€èˆ¬çš„ã«ã€ã€Œæ¨æ¸¬ã§ã¯ã€ç­‰ã®è¡¨ç¾ã¯ä½¿ç”¨ç¦æ­¢
6. å›ç­”ã¯200-300æ–‡å­—ä»¥å†…ã«åã‚ã‚‹ã“ã¨

{context}

{history_context}

ç¾åœ¨ã®è³ªå•: {question}

ä¸Šè¨˜ã®åˆ†æçµæœã®ã¿ã‚’ä½¿ç”¨ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚ãƒ‡ãƒ¼ã‚¿ã«ãªã„æƒ…å ±ã«ã¤ã„ã¦ã¯ã€Œåˆ†æãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€ã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚
"""
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": chat_prompt}],
                temperature=0.1,
                max_tokens=300
            )
            
            answer = response.choices[0].message.content.strip()
            
            # ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾ç­–ãƒã‚§ãƒƒã‚¯
            if company_info.get('enable_hallucination_check', True):
                is_valid, validation_message = self.validate_response_content(answer, analysis_data.get('ir_sources', []))
                if not is_valid:
                    return f"âš ï¸ å›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {validation_message}\n\nç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€åˆ†æãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãæ­£ç¢ºãªå›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚è³ªå•ã‚’å¤‰æ›´ã—ã¦ãŠè©¦ã—ãã ã•ã„ã€‚"
            
            return answer
            
        except Exception as e:
            return f"âŒ å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\n\nåˆ†æãƒ‡ãƒ¼ã‚¿ã‚’å‚ç…§ã—ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
    
    def get_openai_api_key(self):
        """APIã‚­ãƒ¼å–å¾—ï¼ˆæœ¬ç•ªç’°å¢ƒå¯¾å¿œï¼‰"""
        # Streamlit Cloud ã®Secretsæ©Ÿèƒ½ã‚’å„ªå…ˆ
        if hasattr(st, 'secrets') and "OPENAI_API_KEY" in st.secrets:
            return st.secrets["OPENAI_API_KEY"]
        # ç’°å¢ƒå¤‰æ•°ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        elif os.getenv("OPENAI_API_KEY"):
            return os.getenv("OPENAI_API_KEY")
        else:
            st.error("âš ï¸ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            st.markdown("""
            **è¨­å®šæ–¹æ³•:**
            - Streamlit Cloud: Secretsæ©Ÿèƒ½ã§OPENAI_API_KEYã‚’è¨­å®š
            - ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œ: ç’°å¢ƒå¤‰æ•°ã§OPENAI_API_KEYã‚’è¨­å®š
            """)
            st.stop()
    
    def create_research_prompt(self, company_info):
        """å·¥å¤«ã•ã‚ŒãŸãƒªã‚µãƒ¼ãƒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ"""
        prompt = f"""
ã‚ãªãŸã¯ä¼æ¥­ãƒªã‚µãƒ¼ãƒã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ä¼æ¥­ã«ã¤ã„ã¦ã€EVPï¼ˆEmployee Value Propositionï¼‰ã¨ä¼æ¥­åˆ†æã®å„é …ç›®ã‚’è©³ç´°ã«èª¿æŸ»ã—ã€å…·ä½“çš„ãªæƒ…å ±ã‚’åé›†ã—ã¦ãã ã•ã„ã€‚

## èª¿æŸ»å¯¾è±¡ä¼æ¥­
- ä¼æ¥­å: {company_info['company_name']}
- ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸: {company_info.get('website_url', 'ä¸æ˜')}
- é‡ç‚¹åˆ†é‡: {company_info['focus_area']}

### EVPï¼ˆEmployee Value Propositionï¼‰é …ç›®

#### 1. Rewardsï¼ˆå ±é…¬ãƒ»å¾…é‡ï¼‰
- åŸºæœ¬çµ¦ä¸æ°´æº–ã€è³ä¸åˆ¶åº¦ã€ç¦åˆ©åšç”Ÿã€å„ç¨®æ‰‹å½“ã‚„ã‚¤ãƒ³ã‚»ãƒ³ãƒ†ã‚£ãƒ–

#### 2. Opportunityï¼ˆæ©Ÿä¼šãƒ»æˆé•·ï¼‰
- ç ”ä¿®åˆ¶åº¦ã€ã‚­ãƒ£ãƒªã‚¢ãƒ‘ã‚¹ã€æ˜‡é€²åˆ¶åº¦ã€æµ·å¤–é§åœ¨æ©Ÿä¼šã€è³‡æ ¼å–å¾—æ”¯æ´

#### 3. Organizationï¼ˆçµ„ç¹”ãƒ»ä¼æ¥­æ–‡åŒ–ï¼‰
- ä¼æ¥­ç†å¿µã€ãƒŸãƒƒã‚·ãƒ§ãƒ³ã€ä¼æ¥­æ–‡åŒ–ã€ãƒ–ãƒ©ãƒ³ãƒ‰åŠ›ã€çµ„ç¹”é¢¨åœŸ

#### 4. Peopleï¼ˆäººæãƒ»ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆï¼‰
- ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚¿ã‚¤ãƒ«ã€ãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€å¿ƒç†çš„å®‰å…¨æ€§ã€å¤šæ§˜æ€§ãƒ»ã‚¤ãƒ³ã‚¯ãƒ«ãƒ¼ã‚¸ãƒ§ãƒ³

#### 5. Workï¼ˆåƒãæ–¹ãƒ»æ¥­å‹™ï¼‰
- å‹¤å‹™æ™‚é–“ã€ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯ã€ãƒ¯ãƒ¼ã‚¯ãƒ©ã‚¤ãƒ•ãƒãƒ©ãƒ³ã‚¹ã€æ¥­å‹™ã®å°‚é–€æ€§

### ä¼æ¥­åˆ†æé …ç›®ï¼ˆå„é …ç›®ã¯300-400æ–‡å­—ç¨‹åº¦ã§å…·ä½“çš„ã«è¨˜è¼‰ï¼‰

#### 1. æ¥­ç•Œãƒ»å¸‚å ´åˆ†æ
- ä¼æ¥­ãŒå±ã™ã‚‹ä¸»è¦æ¥­ç•Œã®ç‰¹å¾´ã¨å¸‚å ´è¦æ¨¡ï¼ˆå…·ä½“çš„ãªæ•°å€¤å¿…é ˆï¼‰
- æ¥­ç•Œå…¨ä½“ã®å¹´æˆé•·ç‡ã¨éå»3-5å¹´ã®ãƒˆãƒ¬ãƒ³ãƒ‰
- æ¥­ç•Œã®ä¸»è¦ãªæŠ€è¡“é©æ–°ã¨å¤‰åŒ–ï¼ˆå…·ä½“ä¾‹ã‚’å«ã‚€ï¼‰
- å‘ã“ã†5-10å¹´ã®æ¥­ç•Œå°†æ¥æ€§ã¨æˆé•·è¦‹é€šã—
- æ¥­ç•ŒãŒç›´é¢ã—ã¦ã„ã‚‹ä¸»è¦ãªèª²é¡Œã¨æ–°ãŸãªæ©Ÿä¼š
- **å¿…é ˆ**: æ¥­ç•Œå…¨ä½“ã®ç«¶äº‰çŠ¶æ³ã¨å‚å…¥ä¼æ¥­æ•°ã®æ¦‚æ³

#### 2. æ¥­ç•Œå†…ãƒã‚¸ã‚·ãƒ§ãƒ³ï¼ˆç«¶åˆæ¯”è¼ƒå¿…é ˆï¼‰
- å£²ä¸Šé«˜ã®æ¥­ç•Œå†…é †ä½ã¨å…·ä½“çš„ãªå¸‚å ´ã‚·ã‚§ã‚¢ï¼ˆï¼…è¡¨è¨˜ï¼‰
- ä¸»è¦ç«¶åˆä¼æ¥­ã‚’3-5ç¤¾æ˜è¨˜ã—ã€ãã‚Œã‚‰ã¨ã®å£²ä¸Šãƒ»è¦æ¨¡æ¯”è¼ƒ
- å–¶æ¥­åˆ©ç›Šç‡ãƒ»ROEç­‰ã®åç›Šæ€§æŒ‡æ¨™ã¨æ¥­ç•Œå¹³å‡ã¨ã®æ¯”è¼ƒ
- éå»3-5å¹´ã®å£²ä¸Šæˆé•·ç‡ãƒ»åˆ©ç›Šæˆé•·ç‡ã¨ç«¶åˆä»–ç¤¾æ¯”è¼ƒ
- æ ªä¾¡ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»æ™‚ä¾¡ç·é¡ã®æ¥­ç•Œå†…ä½ç½®ã¥ã‘
- **å¿…é ˆ**: ã€Œâ—‹â—‹ç¤¾ã¨æ¯”è¼ƒã—ã¦ã€ã¨ã„ã†å½¢ã§å…·ä½“çš„ç«¶åˆä¼æ¥­åã‚’æŒ™ã’ã¦åˆ†æ

#### 3. ç‹¬è‡ªæ€§ãƒ»å·®åˆ¥åŒ–è¦å› ï¼ˆå…·ä½“çš„äº‹ä¾‹é‡è¦–ï¼‰
- ç«¶åˆä»–ç¤¾ãŒçœŸä¼¼ã§ããªã„ç‹¬è‡ªæŠ€è¡“ãƒ»ã‚µãƒ¼ãƒ“ã‚¹ãƒ»ãƒ“ã‚¸ãƒã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆå…·ä½“åç§°ï¼‰
- ä¿æœ‰ã™ã‚‹ç‰¹è¨±æ•°ãƒ»çŸ¥çš„è²¡ç”£ãƒ»ç‹¬è‡ªãƒã‚¦ãƒã‚¦ã®æ¦‚è¦
- ãƒ–ãƒ©ãƒ³ãƒ‰èªçŸ¥åº¦ãƒ»é¡§å®¢ãƒ­ã‚¤ãƒ¤ãƒªãƒ†ã‚£ã®æ•°å€¤çš„æŒ‡æ¨™ã‚„èª¿æŸ»çµæœ
- R&DæŠ•è³‡é¡ãƒ»æŠ•è³‡æ¯”ç‡ã¨ç«¶åˆä»–ç¤¾ã¨ã®æ¯”è¼ƒ
- å‚å…¥éšœå£ã¨ãªã‚‹ç‹¬è‡ªè³‡ç”£ï¼ˆè¨­å‚™ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€ãƒ‡ãƒ¼ã‚¿ç­‰ï¼‰
- **å¿…é ˆ**: ä»£è¡¨çš„ãªè£½å“ãƒ»ã‚µãƒ¼ãƒ“ã‚¹åã‚’æ˜è¨˜ã—ã€ãªãœãã‚ŒãŒå·®åˆ¥åŒ–è¦å› ãªã®ã‹ã‚’èª¬æ˜

#### 4. äº‹æ¥­ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªåˆ†æï¼ˆå…·ä½“çš„æ•°å€¤ãƒ»äº‹æ¥­åé‡è¦–ï¼‰
- ä¸»è¦äº‹æ¥­ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã¨å£²ä¸Šãƒ»åˆ©ç›Šæ§‹æˆæ¯”ï¼ˆï¼…è¡¨è¨˜å¿…é ˆã€å…·ä½“çš„ãªäº‹æ¥­åæ˜è¨˜ï¼‰
- å„äº‹æ¥­ã®æˆé•·æ€§ï¼šã€Œä¼¸ã³ã¦ã„ã‚‹åˆ†é‡ã€ã®ç‰¹å®šã¨éå»3-5å¹´ã®æˆé•·ç‡ãƒ‡ãƒ¼ã‚¿
- åç›Šæ€§åˆ†æï¼šã€Œé‡‘ã®ãªã‚‹æœ¨ã€äº‹æ¥­ã®ç‰¹å®šã¨å–¶æ¥­åˆ©ç›Šç‡ãƒ»åç›Šè²¢çŒ®åº¦
- æˆ¦ç•¥çš„é‡ç‚¹é ˜åŸŸï¼šã©ã®åˆ†é‡ã¸ã®æŠ•è³‡æ‹¡å¤§ãƒ»M&Aãƒ»æ–°è¦å‚å…¥ã‚’é€²ã‚ã¦ã„ã‚‹ã‹
- äº‹æ¥­é–“ã‚·ãƒŠã‚¸ãƒ¼åŠ¹æœã¨ç›¸äº’ä½œç”¨ã®å…·ä½“ä¾‹ï¼ˆè£½å“ãƒ»æŠ€è¡“ãƒ»é¡§å®¢åŸºç›¤ã®å…±æœ‰ç­‰ï¼‰
- **å¿…é ˆ**: å„äº‹æ¥­ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®å…·ä½“åç§°ã€ä»£è¡¨çš„è£½å“ãƒ»ã‚µãƒ¼ãƒ“ã‚¹åã‚’æ˜è¨˜

## å›ç­”å½¢å¼
JSONå½¢å¼ã§ä»¥ä¸‹ã®é€šã‚Šå›ç­”ã—ã¦ãã ã•ã„ï¼š

```json
{{
  "evp": {{
    "rewards": "å…·ä½“çš„ãªå ±é…¬ãƒ»å¾…é‡æƒ…å ±",
    "opportunity": "å…·ä½“çš„ãªæˆé•·æ©Ÿä¼šæƒ…å ±",
    "organization": "å…·ä½“çš„ãªçµ„ç¹”ãƒ»ä¼æ¥­æ–‡åŒ–æƒ…å ±",
    "people": "å…·ä½“çš„ãªäººæãƒ»ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆæƒ…å ±",
    "work": "å…·ä½“çš„ãªåƒãæ–¹ãƒ»æ¥­å‹™æƒ…å ±"
  }},
  "business_analysis": {{
    "industry_market": "å…·ä½“çš„ãªæ¥­ç•Œãƒ»å¸‚å ´åˆ†æ",
    "market_position": "å…·ä½“çš„ãªæ¥­ç•Œå†…ãƒã‚¸ã‚·ãƒ§ãƒ³",
    "differentiation": "å…·ä½“çš„ãªç‹¬è‡ªæ€§ãƒ»å·®åˆ¥åŒ–è¦å› ",
    "business_portfolio": "å…·ä½“çš„ãªäº‹æ¥­ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªåˆ†æ"
  }}
}}
```

å„é …ç›®ã¯**300-400æ–‡å­—ã§å…·ä½“çš„ã«**è¨˜è¼‰ã—ã€ä»¥ä¸‹ã‚’å¿…ãšå«ã‚ã¦ãã ã•ã„ï¼š
- **æ•°å€¤ãƒ‡ãƒ¼ã‚¿**: å£²ä¸Šé«˜ã€å¸‚å ´ã‚·ã‚§ã‚¢ã€æˆé•·ç‡ç­‰ã®å…·ä½“çš„æ•°å­—
- **ç«¶åˆä¼æ¥­å**: ä¸»è¦ç«¶åˆä»–ç¤¾ã‚’æ˜è¨˜ï¼ˆã€Œâ—‹â—‹ç¤¾ã¨æ¯”è¼ƒã—ã¦ã€ï¼‰
- **å›ºæœ‰åè©**: è£½å“åã€ã‚µãƒ¼ãƒ“ã‚¹åã€æŠ€è¡“åç­‰ã®å…·ä½“åç§°
- **æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿**: ã€Œéå»â—‹å¹´ã§ã€ã€Œè¿‘å¹´ã§ã¯ã€ç­‰ã®å¤‰åŒ–ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰
- **æƒ…å ±æºã®æ˜ç¢ºåŒ–**: ãƒ‡ãƒ¼ã‚¿ãŒä¸æ˜ãªå ´åˆã¯ã€Œå…¬é–‹æƒ…å ±ã§ã¯ç¢ºèªã§ããšã€ã¨æ˜è¨˜

**ç¦æ­¢è¡¨ç¾**: ã€Œä¸€èˆ¬çš„ã«ã€ã€Œå¤šãã®ä¼æ¥­ã¨åŒæ§˜ã€ã€Œæ¥­ç•Œæ¨™æº–çš„ã€ãªã©ã®æ›–æ˜§ãªè¡¨ç¾ã¯ä½¿ç”¨ç¦æ­¢
"""
        return prompt
    
    def research_company(self, company_info):
        """LLMã«ä¼æ¥­èª¿æŸ»ã‚’ä¾é ¼ï¼ˆIRæƒ…å ±åé›†ï¼‹ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾ç­–ä»˜ãï¼‰"""
        
        # IRæƒ…å ±åé›†
        ir_data = []
        if company_info.get('company_domain'):
            st.info("ğŸ” IRæƒ…å ±ã‚’è‡ªå‹•åé›†ä¸­...")
            crawler = SmartIRCrawler(
                company_info['company_domain'],
                company_info.get('ir_top_url'),
                max_depth=company_info.get('max_crawl_depth', 4),
                date_limit_years=3
            )
            ir_data = crawler.crawl_with_intelligence()
            
            if ir_data:
                st.success(f"âœ… {len(ir_data)}ä»¶ã®IRæƒ…å ±ã‚’åé›†ã—ã¾ã—ãŸ")
            else:
                st.warning("âš ï¸ IRæƒ…å ±ã®åé›†ã«å¤±æ•—ã—ã¾ã—ãŸã€‚æ—¢å­˜ã®æ–¹æ³•ã§åˆ†æã‚’ç¶™ç¶šã—ã¾ã™ã€‚")
        
        # åˆ¶ç´„ä»˜ããƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        if ir_data and company_info.get('enable_hallucination_check', True):
            prompt = self.create_constrained_prompt(company_info, ir_data)
        else:
            prompt = self.create_research_prompt(company_info)
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ä¼æ¥­ãƒªã‚µãƒ¼ãƒã®å°‚é–€å®¶ã¨ã—ã¦ã€æ­£ç¢ºã§å…·ä½“çš„ãªæƒ…å ±ã‚’JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=6000,
                temperature=0.1 if company_info.get('enable_hallucination_check', True) else 0.3
            )
            
            content = response.choices[0].message.content.strip()
            
            # JSONãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            if "```json" in content:
                json_start = content.find("```json") + 7
                json_end = content.find("```", json_start)
                json_content = content[json_start:json_end].strip()
            else:
                json_content = content
            
            research_data = json.loads(json_content)
            
            # ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾ç­–ãƒã‚§ãƒƒã‚¯
            if ir_data and company_info.get('enable_hallucination_check', True):
                is_valid, validation_message = self.validate_response_content(json_content, ir_data)
                if not is_valid:
                    st.warning(f"âš ï¸ å›ç­”æ¤œè¨¼: {validation_message}")
            
            # IRæƒ…å ±ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦è¿½åŠ 
            research_data['ir_sources'] = [
                {
                    'url': item['url'],
                    'title': item['title'],
                    'date': item['date'].isoformat(),
                    'importance': item['importance']
                }
                for item in ir_data[:5]
            ]
            
            return research_data
            
        except Exception as e:
            st.error(f"AIèª¿æŸ»ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return None
    
    def save_results(self, company_info, research_data):
        """çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"research_{company_info['company_name']}_{timestamp}.json"
        filepath = self.results_dir / filename
        
        save_data = {
            "company_info": company_info,
            "research_results": research_data,
            "generated_at": datetime.now().isoformat()
        }
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            return filepath, save_data
        except:
            # æœ¬ç•ªç’°å¢ƒã§ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã«å¤±æ•—ã—ãŸå ´åˆã¯çµæœã®ã¿è¿”ã™
            return None, save_data

def main():
    st.title("ğŸ¢ AIä¼æ¥­åˆ†æã‚·ã‚¹ãƒ†ãƒ ")
    st.markdown("### ä¼æ¥­ã®EVPãƒ»ãƒ“ã‚¸ãƒã‚¹åˆ†æã‚’è‡ªå‹•åŒ–ã™ã‚‹AIã‚·ã‚¹ãƒ†ãƒ ")
    
    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
    with st.expander("â„¹ï¸ ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±", expanded=False):
        st.markdown("""
        **æ©Ÿèƒ½:**
        - **EVPåˆ†æ**: Rewards, Opportunity, Organization, People, Work
        - **ãƒ“ã‚¸ãƒã‚¹åˆ†æ**: æ¥­ç•Œåˆ†æ, ç«¶åˆæ¯”è¼ƒ, ç‹¬è‡ªæ€§, äº‹æ¥­ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ª
        - **AIè‡ªå‹•èª¿æŸ»**: OpenAI GPT-4o-miniä½¿ç”¨
        - **çµæœä¿å­˜**: JSONå½¢å¼ã§ã®è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ
        
        **å…¥åŠ›é …ç›®:**
        - ä¼æ¥­åï¼ˆå¿…é ˆï¼‰
        - ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸URLï¼ˆä»»æ„ï¼‰
        - åˆ†æé‡ç‚¹åˆ†é‡ï¼ˆå¿…é ˆï¼‰
        """)
    
    # å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
    with st.form("company_form"):
        col1, col2 = st.columns(2)
        
        with col1:
            company_name = st.text_input(
                "ğŸ¢ ä¼æ¥­å *", 
                placeholder="ä¾‹: ãƒˆãƒ¨ã‚¿è‡ªå‹•è»Šã€ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯ã€ãƒªã‚¯ãƒ«ãƒ¼ãƒˆ",
                help="åˆ†æå¯¾è±¡ã®ä¼æ¥­åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"
            )
            website_url = st.text_input(
                "ğŸŒ ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸URLï¼ˆä»»æ„ï¼‰", 
                placeholder="ä¾‹: https://www.company.co.jp/",
                help="ä¼æ¥­ã®å…¬å¼ã‚µã‚¤ãƒˆURLï¼ˆã‚ˆã‚Šè©³ç´°ãªåˆ†æãŒå¯èƒ½ï¼‰"
            )
            # æ–°è¦è¿½åŠ : ä¼æ¥­ãƒ‰ãƒ¡ã‚¤ãƒ³
            company_domain = st.text_input(
                "ğŸ”— ä¼æ¥­ãƒ‰ãƒ¡ã‚¤ãƒ³",
                placeholder="ä¾‹: toyota.co.jp",
                help="IRãƒšãƒ¼ã‚¸ã®è‡ªå‹•æ¢ç´¢ã«ä½¿ç”¨ã•ã‚Œã¾ã™ï¼ˆä»»æ„ï¼‰"
            )
        
        with col2:
            focus_area = st.text_input(
                "ğŸ¯ åˆ†æé‡ç‚¹åˆ†é‡ *", 
                placeholder="ä¾‹: æ–°å’æ¡ç”¨ã€ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢æ¡ç”¨ã€ä¸­é€”æ¡ç”¨",
                help="ã©ã®åˆ†é‡ã«é‡ç‚¹ã‚’ç½®ã„ã¦åˆ†æã™ã‚‹ã‹ã‚’æŒ‡å®š"
            )
            
            # åˆ†æãƒ¬ãƒ™ãƒ«é¸æŠ
            analysis_level = st.selectbox(
                "ğŸ“Š åˆ†æãƒ¬ãƒ™ãƒ«",
                ["æ¨™æº–åˆ†æ", "è©³ç´°åˆ†æ"],
                help="è©³ç´°åˆ†æã§ã¯æ›´ã«æ·±ã„èª¿æŸ»ã‚’å®Ÿæ–½ã—ã¾ã™"
            )
            
            # æ–°è¦è¿½åŠ : IRæƒ…å ±URL
            ir_top_url = st.text_input(
                "ğŸ“Š IRæƒ…å ±ãƒˆãƒƒãƒ—URL",
                placeholder="ä¾‹: https://toyota.co.jp/ir/",
                help="æŒ‡å®šã—ãªã„å ´åˆã¯è‡ªå‹•ã§æ¨æ¸¬ã•ã‚Œã¾ã™ï¼ˆä»»æ„ï¼‰"
            )
        
        # è©³ç´°è¨­å®šï¼ˆä¸Šç´šè€…å‘ã‘ï¼‰
        with st.expander("âš™ï¸ è©³ç´°è¨­å®š", expanded=False):
            col3, col4 = st.columns(2)
            with col3:
                max_crawl_depth = st.slider("æ¢ç´¢æ·±åº¦", 1, 5, 4, help="IRãƒšãƒ¼ã‚¸ã®æ¢ç´¢æ·±åº¦")
                date_range = st.selectbox("æƒ…å ±ç¯„å›²", ["3å¹´ä»¥å†…", "2å¹´ä»¥å†…", "1å¹´ä»¥å†…"], index=0)
            with col4:
                enable_hallucination_check = st.checkbox("ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾ç­–å¼·åŒ–", value=True, help="å›ç­”ã®äº‹å®Ÿç¢ºèªã‚’å¼·åŒ–ã—ã¾ã™")
                enable_chat = st.checkbox("åˆ†æå¾Œãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½", value=True, help="åˆ†æçµæœã«é–¢ã™ã‚‹è¿½åŠ è³ªå•ãŒå¯èƒ½ã«ãªã‚Šã¾ã™")
        
        st.markdown("---")
        submitted = st.form_submit_button("ğŸ” AIåˆ†æé–‹å§‹", type="primary", use_container_width=True)
    
    # ãƒ•ã‚©ãƒ¼ãƒ é€ä¿¡æ™‚ã®å‡¦ç†
    if submitted:
        if not company_name or not focus_area:
            st.error("ğŸš¨ ä¼æ¥­åã¨åˆ†æé‡ç‚¹åˆ†é‡ã¯å¿…é ˆå…¥åŠ›ã§ã™ã€‚")
            return
        
        # ä¼šç¤¾æƒ…å ±ã®æº–å‚™
        company_info = {
            "company_name": company_name,
            "website_url": website_url,
            "company_domain": company_domain,
            "ir_top_url": ir_top_url,
            "focus_area": focus_area,
            "analysis_level": analysis_level,
            "max_crawl_depth": max_crawl_depth,
            "date_range": date_range,
            "enable_hallucination_check": enable_hallucination_check,
            "enable_chat": enable_chat,
            "timestamp": datetime.now().isoformat()
        }
        
        # èª¿æŸ»å®Ÿè¡Œ
        researcher = StreamlitCompanyResearcher()
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ä»˜ãã§å®Ÿè¡Œ
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        status_text.text("ğŸ” AIåˆ†æã‚’é–‹å§‹ã—ã¦ã„ã¾ã™...")
        progress_bar.progress(20)
        
        with st.spinner("ğŸ¤– AIåˆ†æä¸­... (30-60ç§’ç¨‹åº¦ãŠå¾…ã¡ãã ã•ã„)"):
            progress_bar.progress(50)
            research_data = researcher.research_company(company_info)
            progress_bar.progress(80)
        
        if research_data:
            # çµæœä¿å­˜
            filepath, save_data = researcher.save_results(company_info, research_data)
            progress_bar.progress(100)
            status_text.text("âœ… åˆ†æå®Œäº†ï¼")
            
            # çµæœè¡¨ç¤º
            st.success("ğŸ‰ AIåˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼")
            
            # åŸºæœ¬æƒ…å ±è¡¨ç¤º
            st.markdown("---")
            st.subheader("ğŸ“Š åˆ†æçµæœã‚µãƒãƒªãƒ¼")
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ğŸ¢ ä¼æ¥­å", company_name)
            with col2:
                st.metric("ğŸ¯ é‡ç‚¹åˆ†é‡", focus_area)
            with col3:
                st.metric("ğŸ“Š åˆ†æãƒ¬ãƒ™ãƒ«", analysis_level)
            with col4:
                st.metric("ğŸ• åˆ†ææ—¥æ™‚", company_info['timestamp'][:19])
            
            st.markdown("---")
            
            # ã‚¿ãƒ–å½¢å¼ã§çµæœè¡¨ç¤º
            tab1, tab2, tab3 = st.tabs(["ğŸ“ˆ EVPåˆ†æ", "ğŸ† ãƒ“ã‚¸ãƒã‚¹åˆ†æ", "ğŸ“„ JSONå‡ºåŠ›"])
            
            with tab1:
                st.subheader("ğŸ“ˆ EVPï¼ˆEmployee Value Propositionï¼‰åˆ†æ")
                
                evp_labels = {
                    "rewards": "ğŸ’° Rewardsï¼ˆå ±é…¬ãƒ»å¾…é‡ï¼‰",
                    "opportunity": "ğŸš€ Opportunityï¼ˆæ©Ÿä¼šãƒ»æˆé•·ï¼‰",
                    "organization": "ğŸ¢ Organizationï¼ˆçµ„ç¹”ãƒ»ä¼æ¥­æ–‡åŒ–ï¼‰",
                    "people": "ğŸ‘¥ Peopleï¼ˆäººæãƒ»ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆï¼‰",
                    "work": "ğŸ’¼ Workï¼ˆåƒãæ–¹ãƒ»æ¥­å‹™ï¼‰"
                }
                
                for key, label in evp_labels.items():
                    with st.expander(label, expanded=True):
                        st.write(research_data['evp'][key])
            
            with tab2:
                st.subheader("ğŸ† ãƒ“ã‚¸ãƒã‚¹åˆ†æ")
                
                business_labels = {
                    "industry_market": "ğŸ“ˆ æ¥­ç•Œãƒ»å¸‚å ´åˆ†æ",
                    "market_position": "ğŸ† æ¥­ç•Œå†…ãƒã‚¸ã‚·ãƒ§ãƒ³",
                    "differentiation": "â­ ç‹¬è‡ªæ€§ãƒ»å·®åˆ¥åŒ–è¦å› ",
                    "business_portfolio": "ğŸ—ï¸ äº‹æ¥­ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªåˆ†æ"
                }
                
                for key, label in business_labels.items():
                    with st.expander(label, expanded=True):
                        st.write(research_data['business_analysis'][key])
            
            with tab3:
                st.subheader("ğŸ“„ JSONå½¢å¼ã®åˆ†æçµæœ")
                st.markdown("åˆ†æçµæœã‚’JSONå½¢å¼ã§è¡¨ç¤ºã—ã¾ã™ã€‚ã‚³ãƒ”ãƒ¼ã—ã¦ä»–ã®ã‚·ã‚¹ãƒ†ãƒ ã§ã‚‚æ´»ç”¨ã§ãã¾ã™ã€‚")
                
                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
                json_output = json.dumps(save_data, ensure_ascii=False, indent=2)
                st.download_button(
                    label="ğŸ’¾ JSONçµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=json_output,
                    file_name=f"research_{company_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                    mime="application/json"
                )
                
                # JSONè¡¨ç¤º
                st.code(json_output, language="json")
                
                if filepath:
                    st.info(f"ğŸ’¾ çµæœã¯ã‚µãƒ¼ãƒãƒ¼ã«ã‚‚ä¿å­˜ã•ã‚Œã¾ã—ãŸ: {filepath}")
            
            # ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ï¼ˆåˆ†æçµæœå¾Œã®ã¿è¡¨ç¤ºï¼‰
            if company_info.get('enable_chat', True):
                st.markdown("---")
                st.subheader("ğŸ’¬ åˆ†æçµæœã«é–¢ã™ã‚‹è¿½åŠ è³ªå•")
                
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
                if 'chat_history' not in st.session_state:
                    st.session_state.chat_history = []
                if 'analysis_context' not in st.session_state:
                    st.session_state.analysis_context = None
                
                # åˆ†æçµæœã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ä¿å­˜
                if st.session_state.analysis_context != research_data:
                    st.session_state.analysis_context = research_data
                    st.session_state.chat_history = []  # æ–°ã—ã„åˆ†ææ™‚ã¯ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆ
                
                # ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾ç­–ã®è­¦å‘Š
                st.warning("âš ï¸ ã“ã®è³ªå•æ©Ÿèƒ½ã¯åˆ†æçµæœã¨IRæƒ…å ±ã«åŸºã¥ã„ã¦å›ç­”ã—ã¾ã™ã€‚æ¨æ¸¬çš„ãªå›ç­”ã¯è¡Œã„ã¾ã›ã‚“ã€‚")
                
                # ãƒãƒ£ãƒƒãƒˆå±¥æ­´è¡¨ç¤º
                for i, (question, answer) in enumerate(st.session_state.chat_history):
                    with st.chat_message("user"):
                        st.write(question)
                    with st.chat_message("assistant"):
                        st.write(answer)
                
                # è³ªå•å…¥åŠ›
                user_question = st.chat_input("åˆ†æçµæœã«ã¤ã„ã¦è³ªå•ã—ã¦ãã ã•ã„...")
                
                if user_question:
                    # è³ªå•ã‚’å±¥æ­´ã«è¿½åŠ 
                    with st.chat_message("user"):
                        st.write(user_question)
                    
                    # AIå›ç­”ç”Ÿæˆ
                    with st.chat_message("assistant"):
                        with st.spinner("å›ç­”ã‚’ç”Ÿæˆä¸­..."):
                            answer = researcher.generate_chat_response(
                                user_question, 
                                research_data, 
                                company_info,
                                st.session_state.chat_history
                            )
                            st.write(answer)
                    
                    # å±¥æ­´ã«è¿½åŠ 
                    st.session_state.chat_history.append((user_question, answer))
                
                # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ãƒªã‚»ãƒƒãƒˆãƒœã‚¿ãƒ³
                if st.button("ğŸ—‘ï¸ ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆ"):
                    st.session_state.chat_history = []
                    st.rerun()
        
        else:
            progress_bar.progress(0)
            status_text.text("âŒ åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ")
            st.error("âŒ AIåˆ†æã«å¤±æ•—ã—ã¾ã—ãŸã€‚APIã‚­ãƒ¼ã®è¨­å®šã¾ãŸã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    # ãƒ•ãƒƒã‚¿ãƒ¼
    st.markdown("---")
    st.markdown(
        """
        <div style="text-align: center; color: #666;">
            ğŸ” AIä¼æ¥­åˆ†æã‚·ã‚¹ãƒ†ãƒ  | Powered by OpenAI GPT-4o-mini
        </div>
        """, 
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()