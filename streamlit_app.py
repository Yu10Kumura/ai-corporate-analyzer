#!/usr/bin/env python3
"""
ä¼æ¥­EVPãƒ»ä¼æ¥­åˆ†æã‚·ã‚¹ãƒ†ãƒ  - Streamlit Webç‰ˆï¼ˆæœ¬ç•ªç’°å¢ƒå¯¾å¿œï¼‰
"""

import streamlit as st
import os
import json
import datetime
import time
import re
import urllib.parse
import requests
from pathlib import Path
from openai import OpenAI
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from datetime import datetime, timedelta

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ğŸ¢ AIä¼æ¥­åˆ†æã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="collapsed"
)

class SmartIRCrawler:
    """ã‚¹ãƒãƒ¼ãƒˆIRæƒ…å ±åé›†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾ç­–ä»˜ãï¼‰"""
    
    def __init__(self, company_domain, ir_url=None, max_depth=4, date_limit_years=3):
        self.company_domain = company_domain
        self.ir_url = ir_url or f"https://{company_domain}/ir/"
        self.max_depth = max_depth
        self.date_limit = datetime.now() - timedelta(days=date_limit_years * 365)
        self.discovered_content = []
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
    
    def is_valid_domain(self, url):
        """ä¼æ¥­ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ã¿è¨±å¯"""
        try:
            parsed = urlparse(url)
            return self.company_domain in parsed.netloc
        except:
            return False
    
    def extract_date_from_content(self, content, url):
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º"""
        date_patterns = [
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
            r'(\d{4})-(\d{2})-(\d{2})',
            r'(\d{4})/(\d{1,2})/(\d{1,2})',
            r'å…¬è¡¨æ—¥[ï¼š:\s]*(\d{4}[/-]\d{1,2}[/-]\d{1,2})',
            r'ç™ºè¡¨æ—¥[ï¼š:\s]*(\d{4}[/-]\d{1,2}[/-]\d{1,2})'
        ]
        
        for pattern in date_patterns:
            matches = re.findall(pattern, content)
            if matches:
                try:
                    # æœ€åˆã®ãƒãƒƒãƒã‚’æ—¥ä»˜ã¨ã—ã¦è§£æ
                    match = matches[0]
                    if isinstance(match, tuple):
                        if len(match) == 3:
                            year, month, day = match
                            return datetime(int(year), int(month), int(day))
                except:
                    continue
        
        # æ—¥ä»˜ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç¾åœ¨æ—¥æ™‚ã‚’è¿”ã™
        return datetime.now()
    
    def score_content_importance(self, content, url):
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®é‡è¦åº¦ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°"""
        priority_keywords = {
            "æ±ºç®—çŸ­ä¿¡": 10,
            "æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸": 9,
            "æ±ºç®—èª¬æ˜ä¼š": 8,
            "æ¥­ç¸¾ãƒã‚¤ãƒ©ã‚¤ãƒˆ": 7,
            "ä¸­æœŸçµŒå–¶è¨ˆç”»": 6,
            "æ ªä¸»ç·ä¼š": 5,
            "é©æ™‚é–‹ç¤º": 4,
            "ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªãƒªãƒ¼ã‚¹": 3,
            "IR": 2
        }
        
        score = 0
        content_lower = content.lower()
        url_lower = url.lower()
        
        for keyword, points in priority_keywords.items():
            if keyword in content or keyword in url:
                score += points
        
        # PDFæ–‡æ›¸ã¯é‡è¦åº¦ãŒé«˜ã„
        if '.pdf' in url_lower:
            score += 3
        
        # æ±ºç®—é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        earnings_keywords = ["æ±ºç®—", "æ¥­ç¸¾", "è²¡å‹™", "å£²ä¸Š", "åˆ©ç›Š"]
        for keyword in earnings_keywords:
            if keyword in content:
                score += 2
        
        return score
    
    def discover_ir_links(self, start_url, depth=0):
        """IRãƒšãƒ¼ã‚¸ã‹ã‚‰é‡è¦ãªãƒªãƒ³ã‚¯ã‚’ç™ºè¦‹ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        if depth > self.max_depth:
            return []
        
        try:
            # URLã®æ¤œè¨¼ã‚’ç·©å’Œ
            if not start_url.startswith(('http://', 'https://')):
                start_url = 'https://' + start_url
            
            st.info(f"ğŸ” æ¢ç´¢ä¸­: {start_url}")
            
            response = self.session.get(start_url, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰æ—¥ä»˜æŠ½å‡º
            page_date = self.extract_date_from_content(response.text, start_url)
            
            # é‡è¦åº¦ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
            importance_score = self.score_content_importance(response.text, start_url)
            
            discovered = [{
                'url': start_url,
                'content': response.text[:3000],  # 3000æ–‡å­—ã«çŸ­ç¸®
                'date': page_date,
                'importance': importance_score,
                'title': soup.title.string if soup.title else start_url.split('/')[-1]
            }]
            
            # åŸºæœ¬çš„ãªIRæƒ…å ±ãŒã‚ã‚Œã°åé›†æˆåŠŸã¨ã¿ãªã™
            if importance_score > 0:
                st.success(f"âœ… IRæƒ…å ±ã‚’ç™ºè¦‹: {soup.title.string if soup.title else start_url}")
            
            # ãƒªãƒ³ã‚¯æ¢ç´¢ã¯ç°¡æ½”ã«
            if depth < 2:  # æ¢ç´¢æ·±åº¦ã‚’åˆ¶é™
                ir_keywords = ['æ±ºç®—', 'æ¥­ç¸¾', 'ir', 'investor']
                for link in soup.find_all('a', href=True)[:20]:  # æœ€åˆã®20å€‹ã®ãƒªãƒ³ã‚¯ã®ã¿
                    href = link.get('href')
                    if not href:
                        continue
                    
                    full_url = urljoin(start_url, href)
                    link_text = link.get_text().lower()
                    
                    if any(keyword in link_text or keyword in href.lower() for keyword in ir_keywords):
                        discovered.extend(self.discover_ir_links(full_url, depth + 1))
                        if len(discovered) >= 5:  # 5ä»¶è¦‹ã¤ã‹ã£ãŸã‚‰çµ‚äº†
                            break
            
            return discovered
            
        except requests.exceptions.RequestException as e:
            st.warning(f"âš ï¸ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {start_url} - {str(e)}")
            return []
        except Exception as e:
            st.warning(f"âš ï¸ è§£æã‚¨ãƒ©ãƒ¼: {start_url} - {str(e)}")
            return []
    
    def crawl_with_intelligence(self):
        """ã‚¹ãƒãƒ¼ãƒˆãªIRæƒ…å ±åé›†ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        try:
            st.info(f"ğŸ” IRæƒ…å ±æ¢ç´¢ã‚’é–‹å§‹: {self.ir_url}")
            
            # è¤‡æ•°ã®IR URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œ
            ir_patterns = [
                self.ir_url,
                f"https://{self.company_domain}/ir/",
                f"https://{self.company_domain}/investor/",
                f"https://{self.company_domain}/company/ir/",
                f"https://ir.{self.company_domain}/",
            ]
            
            all_content = []
            
            for url_pattern in ir_patterns:
                if not url_pattern or len(all_content) >= 3:  # 3ä»¶è¦‹ã¤ã‹ã£ãŸã‚‰çµ‚äº†
                    continue
                    
                try:
                    content = self.discover_ir_links(url_pattern, 0)
                    if content:
                        all_content.extend(content)
                        st.success(f"âœ… {len(content)}ä»¶ã®æƒ…å ±ã‚’ {url_pattern} ã‹ã‚‰åé›†")
                        break  # æˆåŠŸã—ãŸã‚‰ä»–ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯è©¦è¡Œã—ãªã„
                except:
                    continue
            
            if not all_content:
                st.warning("âš ï¸ IRæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ä¸€èˆ¬çš„ãªä¼æ¥­åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
                return []
            
            # é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆã—ã¦é‡è¤‡é™¤å»
            unique_content = {}
            for item in all_content:
                if item['url'] not in unique_content:
                    unique_content[item['url']] = item
            
            sorted_content = sorted(unique_content.values(), key=lambda x: x['importance'], reverse=True)
            
            # ä¸Šä½5ä»¶ã‚’è¿”ã™
            result = sorted_content[:5]
            st.success(f"ğŸ‰ åˆè¨ˆ {len(result)} ä»¶ã®IRæƒ…å ±ã‚’åé›†ã—ã¾ã—ãŸ")
            
            return result
            
        except Exception as e:
            st.error(f"âŒ IRæƒ…å ±åé›†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []

class StreamlitCompanyResearcher:
    def __init__(self):
        # APIã‚­ãƒ¼è¨­å®šï¼ˆæœ¬ç•ªç’°å¢ƒå¯¾å¿œï¼‰
        api_key = self.get_openai_api_key()
        
        try:
            self.client = OpenAI(api_key=api_key)
        except Exception as e:
            st.error(f"âŒ OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            st.stop()
            
        # çµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆæœ¬ç•ªç’°å¢ƒã§ã¯ä¸€æ™‚çš„ï¼‰
        self.results_dir = Path('results')
        self.results_dir.mkdir(exist_ok=True)
        
        # HTTP ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼ˆWebæ¤œç´¢ç”¨ï¼‰
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
    
    def extract_domain_from_url(self, url):
        """URLã‹ã‚‰ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’æŠ½å‡º"""
        if not url:
            return None
        try:
            if not url.startswith(('http://', 'https://')):
                url = 'https://' + url
            parsed = urlparse(url)
            domain = parsed.netloc
            # www. ã‚’é™¤å»
            if domain.startswith('www.'):
                domain = domain[4:]
            return domain
        except:
            return None
    
    def validate_response_content(self, response, source_data):
        """ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾ç­–ï¼šå›ç­”å†…å®¹ã®æ¤œè¨¼"""
        # æ¨æ¸¬è¡¨ç¾ã®æ¤œå‡º
        speculation_patterns = [
            r'ã¨æ€ã‚ã‚Œ', r'å¯èƒ½æ€§ãŒ', r'ãŠãã‚‰ã', r'ä¸€èˆ¬çš„ã«', 
            r'é€šå¸¸ã¯', r'äºˆæƒ³', r'æ¨æ¸¬', r'æ†¶æ¸¬', r'ã‹ã‚‚ã—ã‚Œ'
        ]
        
        for pattern in speculation_patterns:
            if re.search(pattern, response):
                return False, f"æ¨æ¸¬çš„è¡¨ç¾ãŒå«ã¾ã‚Œã¦ã„ã¾ã™: {pattern}"
        
        # å‡ºå…¸è¨˜è¼‰ã®ç¢ºèª
        if 'å‡ºå…¸ï¼š' not in response and 'ã‚½ãƒ¼ã‚¹ï¼š' not in response:
            return False, "å‡ºå…¸ãŒæ˜è¨˜ã•ã‚Œã¦ã„ã¾ã›ã‚“"
        
        # åŸºæœ¬çš„ãªäº‹å®Ÿç¢ºèªï¼ˆä¼æ¥­åã®ä¸€è‡´ãªã©ï¼‰
        company_name_in_source = any(source['title'] for source in source_data if source.get('title'))
        if not company_name_in_source and len(source_data) > 0:
            st.warning("âš ï¸ ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã¨ä¼æ¥­åã®æ•´åˆæ€§ã‚’ç¢ºèªä¸­...")
        
        return True, "æ¤œè¨¼OK"
    
    def create_constrained_prompt(self, company_info, ir_data):
        """åˆ¶ç´„ä»˜ããƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ"""
        ir_content = "\n".join([
            f"ã€{item['title']}ã€‘(é‡è¦åº¦: {item['importance']}, æ—¥ä»˜: {item['date'].strftime('%Y-%m-%d')})\n"
            f"URL: {item['url']}\n"
            f"å†…å®¹: {item['content'][:800]}...\n"
            for item in ir_data[:5]  # ä¸Šä½5ä»¶
        ])
        
        system_prompt = f"""
ã‚ãªãŸã¯ä¼æ¥­åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã‚’å³æ ¼ã«å®ˆã£ã¦ãã ã•ã„ï¼š

ã€é‡è¦åˆ¶ç´„ã€‘
1. æä¾›ã•ã‚ŒãŸIRæƒ…å ±ã¨Webãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’å‚ç…§ã™ã‚‹ã“ã¨
2. ãƒ‡ãƒ¼ã‚¿ã«ãªã„æƒ…å ±ã¯ã€Œåˆ†æãƒ‡ãƒ¼ã‚¿ã«ã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€ã¨æ˜è¨˜
3. æ¨æ¸¬ã‚„ä¸€èˆ¬è«–ã§ã¯ãªãã€å…·ä½“çš„ãªæ ¹æ‹ ã‚’ç¤ºã™ã“ã¨
4. å¿…ãšã€Œå‡ºå…¸ï¼š[URL] [å–å¾—æ—¥æ™‚]ã€ã‚’æ˜è¨˜ã™ã‚‹ã“ã¨
5. ã€ŒãŠãã‚‰ãã€ã€Œä¸€èˆ¬çš„ã«ã€ã€Œæ¨æ¸¬ã§ã¯ã€ç­‰ã®è¡¨ç¾ã¯ä½¿ç”¨ç¦æ­¢
6. 3å¹´ä»¥å†…ï¼ˆ2022å¹´1æœˆä»¥é™ï¼‰ã®æƒ…å ±ã®ã¿ä½¿ç”¨ã™ã‚‹ã“ã¨

ã€åˆ†æå¯¾è±¡ä¼æ¥­ã€‘: {company_info['company_name']}
ã€é‡ç‚¹åˆ†é‡ã€‘: {company_info['focus_area']}

ã€åˆ©ç”¨å¯èƒ½ãªIRæƒ…å ±ã€‘:
{ir_content}

ã€åˆ†ææŒ‡ç¤ºã€‘:
ä¸Šè¨˜ã®IRæƒ…å ±ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ã€EVPåˆ†æã¨ãƒ“ã‚¸ãƒã‚¹åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚
ãƒ‡ãƒ¼ã‚¿ã«ãªã„é …ç›®ã«ã¤ã„ã¦ã¯ã€Œãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚åˆ†æã§ãã¾ã›ã‚“ã€ã¨è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚
"""
        
        return system_prompt
    
    def verify_with_double_check(self, question, answer, sources):
        """äºŒæ®µéšæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ """
        verification_prompt = f"""
ä»¥ä¸‹ã®å›ç­”ã«ã¤ã„ã¦äº‹å®Ÿç¢ºèªã‚’è¡Œã£ã¦ãã ã•ã„ï¼š

è³ªå•: {question}
å›ç­”: {answer}

ãƒã‚§ãƒƒã‚¯é …ç›®:
1. æä¾›ã•ã‚ŒãŸã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã‹ï¼Ÿ
2. 3å¹´ä»¥å†…ã®æƒ…å ±ã®ã¿ã‹ï¼Ÿ
3. æ¨æ¸¬ã‚„å¤–éƒ¨çŸ¥è­˜ãŒæ··å…¥ã—ã¦ã„ãªã„ã‹ï¼Ÿ
4. å‡ºå…¸ãŒæ­£ã—ãæ˜è¨˜ã•ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ

å•é¡ŒãŒã‚ã‚Œã°ã€ŒNGï¼šç†ç”±ã€ã€å•é¡Œãªã‘ã‚Œã°ã€ŒOKã€ã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚
"""
        
        try:
            verification_response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": verification_prompt}],
                temperature=0.1,
                max_tokens=100
            )
            
            verification_result = verification_response.choices[0].message.content
            return "OK" in verification_result, verification_result
            
        except Exception as e:
            return False, f"æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def search_existing_sources(self, question, analysis_data):
        """æ·±åº¦èª¿æŸ»: ä¼æ¥­ã‚µã‚¤ãƒˆã®éšå±¤ã‚’æ·±ãæ¢ç´¢ã—ã¦é–¢é€£æƒ…å ±ã‚’åé›†"""
        company_domain = analysis_data.get('company_domain')
        if not company_domain:
            st.error("âŒ ä¼æ¥­ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒæä¾›ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return []
        
        try:
            # è³ªå•ã«åŸºã¥ã„ã¦æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
            search_keywords = self.extract_search_keywords(question)
            st.write(f"ğŸ” æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(search_keywords)}")
            
            st.info("ğŸ” ä¼æ¥­ã‚µã‚¤ãƒˆã‚’æ·±åº¦èª¿æŸ»ä¸­...")
            
            # æ®µéšçš„ãªæ·±åº¦èª¿æŸ»
            additional_sources = []
            
            # Step 1: åŸºæœ¬ã‚»ã‚¯ã‚·ãƒ§ãƒ³ + ã‚µãƒ–ãƒšãƒ¼ã‚¸ç™ºè¦‹
            base_sections = {
                'ir': ['investor', 'ir', 'finance'],
                'business': ['business', 'service', 'product', 'solution'],
                'company': ['company', 'about', 'corporate'],
                'news': ['news', 'press', 'release'],
                'strategy': ['strategy', 'vision', 'plan', 'management']
            }
            
            for section_type, url_patterns in base_sections.items():
                st.write(f"ğŸ“‚ {section_type.title()}ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’èª¿æŸ»ä¸­...")
                section_sources = self.deep_explore_section(
                    company_domain, section_type, url_patterns, search_keywords, question
                )
                if section_sources:
                    st.write(f"âœ… {len(section_sources)}ä»¶ç™ºè¦‹")
                    additional_sources.extend(section_sources)
                else:
                    st.write("âŒ æƒ…å ±ãªã—")
                
                # æœ€å¤§12ã‚½ãƒ¼ã‚¹ã¾ã§ï¼ˆå„ã‚»ã‚¯ã‚·ãƒ§ãƒ³2-3å€‹ï¼‰
                if len(additional_sources) >= 12:
                    break
            
            # Step 2: é‡è¦æ–‡æ›¸ã®è‡ªå‹•ç™ºè¦‹ãƒ»å–å¾—
            st.write("ğŸ“„ é‡è¦æ–‡æ›¸ã‚’æ¤œç´¢ä¸­...")
            document_sources = self.discover_important_documents(
                company_domain, search_keywords, question
            )
            if document_sources:
                st.write(f"ğŸ“‹ {len(document_sources)}ä»¶ã®é‡è¦æ–‡æ›¸ã‚’ç™ºè¦‹")
                additional_sources.extend(document_sources)
            else:
                st.write("âŒ é‡è¦æ–‡æ›¸ãªã—")
            
            return additional_sources[:15]  # æœ€å¤§15ã‚½ãƒ¼ã‚¹
            
        except Exception as e:
            st.warning(f"æ·±åº¦èª¿æŸ»ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def deep_explore_section(self, domain, section_type, url_patterns, keywords, question):
        """ç‰¹å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æ·±åº¦æ¢ç´¢"""
        sources = []
        
        for pattern in url_patterns:
            # è¤‡æ•°ã®URLå€™è£œã‚’è©¦è¡Œ
            candidate_urls = [
                f"https://{domain}/{pattern}/",
                f"https://{domain}/{pattern}.html",
                f"https://{domain}/jp/{pattern}/",
                f"https://{domain}/ja/{pattern}/",
            ]
            
            for base_url in candidate_urls:
                try:
                    response = self.session.get(base_url, timeout=15)
                    if response.status_code != 200:
                        continue
                        
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # ã‚µãƒ–ãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯ã‚’ç™ºè¦‹
                    subpage_links = self.discover_subpages(soup, base_url, section_type)
                    
                    # ãƒ™ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è§£æ
                    base_content = self.extract_relevant_content(soup, keywords, question)
                    if base_content:
                        sources.append({
                            'url': base_url,
                            'content': base_content,
                            'source_type': f'{section_type.title()}æƒ…å ±',
                            'depth': 'base'
                        })
                    
                    # ã‚µãƒ–ãƒšãƒ¼ã‚¸ã®æ¢ç´¢ï¼ˆæœ€å¤§3ã¤ã¾ã§ï¼‰
                    for sublink in subpage_links[:3]:
                        sub_content = self.explore_subpage(sublink, keywords, question)
                        if sub_content:
                            sources.append(sub_content)
                    
                    break  # æˆåŠŸã—ãŸã‚‰ä»–ã®å€™è£œURLã¯è©¦è¡Œã—ãªã„
                    
                except:
                    continue
                    
            if len(sources) >= 3:  # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚ãŸã‚Šæœ€å¤§3ã‚½ãƒ¼ã‚¹
                break
                
        return sources
    
    def discover_subpages(self, soup, base_url, section_type):
        """ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã®ã‚µãƒ–ãƒšãƒ¼ã‚¸ã‚’ç™ºè¦‹"""
        subpages = []
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥ã®é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        important_keywords = {
            'ir': ['æ±ºç®—', 'æ¥­ç¸¾', 'èª¬æ˜ä¼š', 'ä¸­æœŸ', 'è¨ˆç”»', 'æœ‰ä¾¡è¨¼åˆ¸', 'è²¡å‹™', 'financial'],
            'business': ['äº‹æ¥­ç´¹ä»‹', 'ã‚µãƒ¼ãƒ“ã‚¹', 'è£½å“', 'ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³', 'å¼·ã¿', 'ç‰¹å¾´'],
            'company': ['ä»£è¡¨', 'ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸', 'æ²¿é©', 'çµ„ç¹”', 'ãƒŸãƒƒã‚·ãƒ§ãƒ³', 'ãƒ“ã‚¸ãƒ§ãƒ³'],
            'news': ['ãƒ—ãƒ¬ã‚¹', 'ãƒªãƒªãƒ¼ã‚¹', 'ç™ºè¡¨', 'æ–°ç€', 'æœ€æ–°'],
            'strategy': ['æˆ¦ç•¥', 'æ–¹é‡', 'ãƒ“ã‚¸ãƒ§ãƒ³', 'è¨ˆç”»', 'å–ã‚Šçµ„ã¿', 'DX']
        }
        
        keywords = important_keywords.get(section_type, [])
        
        # ãƒªãƒ³ã‚¯ã‚’æ¢ç´¢
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            text = link.get_text().strip()
            
            if not href or len(text) < 3:
                continue
                
            # ç›¸å¯¾ãƒ‘ã‚¹ã‚’çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
            if href.startswith('/'):
                full_url = f"https://{base_url.split('/')[2]}{href}"
            elif href.startswith('http'):
                full_url = href
            else:
                continue
                
            # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ãƒªãƒ³ã‚¯ã‚’å„ªå…ˆ
            relevance_score = 0
            text_lower = text.lower()
            
            for keyword in keywords:
                if keyword in text_lower:
                    relevance_score += 2
                if keyword in href.lower():
                    relevance_score += 1
            
            # PDFãƒ•ã‚¡ã‚¤ãƒ«ã¯ç‰¹ã«é‡è¦
            if href.endswith('.pdf'):
                relevance_score += 3
                
            if relevance_score > 0:
                subpages.append({
                    'url': full_url,
                    'text': text,
                    'score': relevance_score
                })
        
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        subpages.sort(key=lambda x: x['score'], reverse=True)
        return [page['url'] for page in subpages[:5]]
    
    def explore_subpage(self, url, keywords, question):
        """ã‚µãƒ–ãƒšãƒ¼ã‚¸ã®è©³ç´°æ¢ç´¢"""
        try:
            response = self.session.get(url, timeout=15)
            if response.status_code != 200:
                return None
                
            # PDFãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
            if url.endswith('.pdf'):
                return self.extract_pdf_content(url, keywords, question)
                
            # HTMLãƒšãƒ¼ã‚¸ã®å ´åˆ
            soup = BeautifulSoup(response.text, 'html.parser')
            content = self.extract_relevant_content(soup, keywords, question)
            
            if content:
                return {
                    'url': url,
                    'content': content,
                    'source_type': self.classify_source_type(url),
                    'depth': 'deep'
                }
                
        except:
            pass
            
        return None
    
    def extract_pdf_content(self, pdf_url, keywords, question):
        """PDFæ–‡æ›¸ã‹ã‚‰ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        try:
            response = self.session.get(pdf_url, timeout=20)
            if response.status_code == 200:
                # PDFè§£æã¯è¤‡é›‘ãªã®ã§ã€ã¾ãšã¯PDFã®å­˜åœ¨ã‚’è¨˜éŒ²
                return {
                    'url': pdf_url,
                    'content': f"PDFæ–‡æ›¸ãŒç™ºè¦‹ã•ã‚Œã¾ã—ãŸ: {pdf_url.split('/')[-1]}",
                    'source_type': 'PDFè³‡æ–™',
                    'depth': 'document'
                }
        except:
            pass
        return None
    
    def discover_important_documents(self, domain, keywords, question):
        """é‡è¦æ–‡æ›¸ã®è‡ªå‹•ç™ºè¦‹ï¼ˆä¸€æ¬¡æƒ…å ±å„ªå…ˆï¼‰"""
        documents = []
        
        # ä¸€æ¬¡æƒ…å ±ï¼ˆå…¬å¼é–‹ç¤ºè³‡æ–™ï¼‰ã®å„ªå…ˆçš„ãªæ¤œç´¢ãƒ‘ã‚¹
        primary_document_paths = [
            '/ir/library/',      # IRè³‡æ–™ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
            '/ir/finance/',      # è²¡å‹™æƒ…å ±
            '/ir/brief/',        # æ±ºç®—çŸ­ä¿¡
            '/ir/securities/',   # æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸
            '/ir/results/',      # æ±ºç®—èª¬æ˜è³‡æ–™
            '/ir/plan/',         # ä¸­æœŸçµŒå–¶è¨ˆç”»
            '/ir/annual/',       # ã‚¢ãƒ‹ãƒ¥ã‚¢ãƒ«ãƒ¬ãƒãƒ¼ãƒˆ
            '/ir/disclosure/',   # é–‹ç¤ºæƒ…å ±
            '/investor/library/',
            '/investor/financials/',
            '/company/plan/',
            '/company/management/',
            '/sustainability/report/',
        ]
        
        # ä¸€æ¬¡æƒ…å ±ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚ˆã‚Šå…·ä½“çš„ã«ï¼‰
        primary_keywords = [
            'æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸', 'æ±ºç®—çŸ­ä¿¡', 'æ±ºç®—èª¬æ˜', 'annual report',
            'ä¸­æœŸçµŒå–¶è¨ˆç”»', 'äº‹æ¥­å ±å‘Šæ›¸', 'å››åŠæœŸå ±å‘Š', 'è²¡å‹™è«¸è¡¨',
            'financial results', 'earnings', 'quarterly report',
            'æ¥­ç¸¾èª¬æ˜', 'æŠ•è³‡å®¶èª¬æ˜', 'investor presentation'
        ]
        
        st.write("ğŸ“‹ ä¸€æ¬¡æƒ…å ±ï¼ˆå…¬å¼é–‹ç¤ºè³‡æ–™ï¼‰ã‚’å„ªå…ˆæ¤œç´¢ä¸­...")
        
        for path in primary_document_paths:
            try:
                url = f"https://{domain}{path}"
                response = self.session.get(url, timeout=15)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # ä¸€æ¬¡æƒ…å ±ã‚’å„ªå…ˆçš„ã«ç™ºè¦‹
                    for link in soup.find_all('a', href=True):
                        href = link.get('href')
                        text = link.get_text().strip()
                        
                        if not href or len(text) < 3:
                            continue
                        
                        # ä¸€æ¬¡æƒ…å ±ã®ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼ˆå¤§å¹…å¼·åŒ–ï¼‰
                        relevance_score = 0
                        text_lower = text.lower()
                        
                        # ä¸€æ¬¡æƒ…å ±ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«é«˜ã„ã‚¹ã‚³ã‚¢
                        for keyword in primary_keywords:
                            if keyword in text_lower:
                                relevance_score += 10  # é«˜ã„ã‚¹ã‚³ã‚¢
                        
                        # å¹´åº¦ãƒ»æœŸé–“æƒ…å ±ãŒã‚ã‚Œã°è¿½åŠ ã‚¹ã‚³ã‚¢
                        if any(year in text for year in ['2024', '2023', '2022', '2025']):
                            relevance_score += 5
                        
                        # PDFã¯å…¬å¼è³‡æ–™ã®å¯èƒ½æ€§ãŒé«˜ã„
                        if href.endswith('.pdf'):
                            relevance_score += 8
                        
                        # è³ªå•ã«é–¢é€£ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
                        for keyword in keywords:
                            if keyword.lower() in text_lower:
                                relevance_score += 3
                        
                        if relevance_score >= 8:  # é–¾å€¤ã‚’ä¸Šã’ã¦é«˜å“è³ªãªæƒ…å ±ã®ã¿
                            if href.startswith('/'):
                                full_url = f"https://{domain}{href}"
                            else:
                                full_url = href
                                
                            documents.append({
                                'url': full_url,
                                'content': f"ã€ä¸€æ¬¡æƒ…å ±ã€‘{text}",
                                'source_type': 'å…¬å¼é–‹ç¤ºè³‡æ–™',
                                'depth': 'primary_document',
                                'relevance_score': relevance_score
                            })
                            
                        if len(documents) >= 5:  # ä¸€æ¬¡æƒ…å ±ã¯æœ€å¤§5ä»¶
                            break
                            
            except:
                continue
                
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        documents.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
        
        if documents:
            st.success(f"âœ… {len(documents)}ä»¶ã®ä¸€æ¬¡æƒ…å ±ã‚’ç™ºè¦‹")
        else:
            st.warning("âš ï¸ ä¸€æ¬¡æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                
        return documents[:5]  # æœ€é«˜å“è³ªã®ä¸€æ¬¡æƒ…å ±5ä»¶ã¾ã§
    
    def assess_content_reliability(self, content):
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ä¿¡é ¼æ€§ã‚’è©•ä¾¡ï¼ˆ0-100ã®ã‚¹ã‚³ã‚¢ï¼‰"""
        if not content or len(content) < 10:
            return 0
        
        score = 50  # åŸºæœ¬ã‚¹ã‚³ã‚¢
        content_lower = content.lower()
        
        # ä¸€æ¬¡æƒ…å ±ã®è¨¼æ‹ ãŒã‚ã‚Œã°é«˜ã‚¹ã‚³ã‚¢
        primary_indicators = [
            'æ±ºç®—çŸ­ä¿¡', 'æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸', 'ã‚¢ãƒ‹ãƒ¥ã‚¢ãƒ«ãƒ¬ãƒãƒ¼ãƒˆ', 'æ±ºç®—èª¬æ˜',
            'ä¸­æœŸçµŒå–¶è¨ˆç”»', 'æŠ•è³‡å®¶å‘ã‘', 'irè³‡æ–™', 'å…¬å¼ç™ºè¡¨', 'é–‹ç¤ºæƒ…å ±'
        ]
        
        for indicator in primary_indicators:
            if indicator in content_lower:
                score += 15
        
        # å…·ä½“çš„ãªå¹´åº¦ãƒ»æ•°å€¤ãŒã‚ã‚Œã°ä¿¡é ¼æ€§UP
        if any(year in content for year in ['2024å¹´', '2023å¹´', '2022å¹´', '2025å¹´']):
            score += 10
        
        # æ¨æ¸¬ãƒ»æ›–æ˜§ãªè¡¨ç¾ãŒã‚ã‚Œã°ä¿¡é ¼æ€§DOWN
        unreliable_indicators = [
            'æ¨å®š', 'ä¸€èˆ¬çš„ã«', 'æ¥­ç•Œæ¨™æº–', 'é€šå¸¸', 'å¤šãã®ä¼æ¥­',
            'ã¨æ€ã‚ã‚Œ', 'ã¨è€ƒãˆã‚‰ã‚Œ', 'å¯èƒ½æ€§ãŒã‚', 'äºˆæƒ³ã•ã‚Œã‚‹'
        ]
        
        for indicator in unreliable_indicators:
            if indicator in content_lower:
                score -= 20
        
        # ã€Œå…¬å¼é–‹ç¤ºæƒ…å ±ã§ã¯ç¢ºèªã§ãã¾ã›ã‚“ã€ãªã©ã®æ­£ç›´ãªè¡¨ç¾ã¯ä¿¡é ¼æ€§UP
        honest_indicators = [
            'ç¢ºèªã§ãã¾ã›ã‚“', 'é–‹ç¤ºã•ã‚Œã¦ã„ã¾ã›ã‚“', 'å…¬è¡¨ã•ã‚Œã¦ã„ã¾ã›ã‚“',
            'æƒ…å ±ãŒé™å®šçš„', 'è©³ç´°ã¯ä¸æ˜'
        ]
        
        for indicator in honest_indicators:
            if indicator in content_lower:
                score += 10
        
        return max(0, min(100, score))  # 0-100ã®ç¯„å›²ã«åˆ¶é™
    
    def search_external_sources(self, company_name, industry_keywords):
        """SerpAPIã‚’ä½¿ç”¨ã—ãŸé–¢é€£æ€§ã®é«˜ã„å¤–éƒ¨æƒ…å ±åé›†"""
        external_data = []
        
        st.info("ğŸŒ SerpAPIã§é–¢é€£æ€§ã®é«˜ã„æ¥­ç•Œæƒ…å ±ã‚’å³é¸åé›†ä¸­...")
        
        # SerpAPIã‚­ãƒ¼ã‚’å–å¾—ï¼ˆæ–°ã—ã„ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ï¼‰
        serpapi_key = self.get_serpapi_key()
        if not serpapi_key:
            return self.create_fallback_external_data(company_name, industry_keywords)
        
        # ã‚ˆã‚Šå…·ä½“çš„ã§é–¢é€£æ€§ã®é«˜ã„æ¤œç´¢ã‚¯ã‚¨ãƒª
        search_queries = [
            f'"{company_name}" å¸‚å ´è¦æ¨¡ OR æ¥­ç•Œã‚·ã‚§ã‚¢ OR å£²ä¸Š OR æ¥­ç¸¾ site:nikkei.com OR site:toyokeizai.net OR site:diamond.jp',
            f'"{company_name}" ç«¶åˆ OR ãƒ©ã‚¤ãƒãƒ« OR æ¥­ç•Œåœ°ä½ -æ±‚äºº -è»¢è· site:nikkei.com OR site:itmedia.co.jp'
        ]
        
        for i, query in enumerate(search_queries, 1):
            st.write(f"ğŸ” æ¤œç´¢ {i}/{len(search_queries)}: {query[:60]}...")
            
            try:
                results = self.search_with_serpapi(query, serpapi_key)
                
                if results and 'organic_results' in results:
                    relevant_results = self.filter_relevant_results(results['organic_results'], company_name)
                    
                    for result in relevant_results[:2]:  # é–¢é€£æ€§ã®é«˜ã„ä¸Šä½2ä»¶ã®ã¿
                        external_data.append({
                            'title': result.get('title', ''),
                            'snippet': result.get('snippet', ''),
                            'url': result.get('link', ''),
                            'source': self.extract_domain(result.get('link', '')),
                            'type': 'å¤–éƒ¨è¨˜äº‹',
                            'relevance_score': result.get('relevance_score', 0)
                        })
                    
                    st.success(f"âœ… é–¢é€£æ€§ã®é«˜ã„{len(relevant_results[:2])}ä»¶ã‚’é¸æŠ")
                else:
                    st.warning(f"âš ï¸ æ¤œç´¢ {i}: çµæœãªã—")
                    
            except Exception as e:
                st.warning(f"âš ï¸ æ¤œç´¢ {i} ã‚¨ãƒ©ãƒ¼: {str(e)}")
                continue
        
        # é–¢é€£æ€§ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
        external_data.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
        return external_data[:4]  # æœ€é«˜å“è³ªã®4ä»¶ã®ã¿
    
    def filter_relevant_results(self, results, company_name):
        """æ¤œç´¢çµæœã®é–¢é€£æ€§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        filtered_results = []
        
        # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆãƒã‚¤ã‚ºã«ãªã‚Šã‚„ã™ã„æƒ…å ±ï¼‰
        exclude_keywords = ['æ±‚äºº', 'è»¢è·', 'æ¡ç”¨', 'é¢æ¥', 'å°±æ´»', 'æ–°å’', 'ä¸­é€”', 'å£ã‚³ãƒŸ', 'indeed', 'ãƒªã‚¯ãƒŠãƒ“']
        
        # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆé–¢é€£æ€§åˆ¤å®šç”¨ï¼‰
        important_keywords = ['å¸‚å ´', 'æ¥­ç•Œ', 'å£²ä¸Š', 'åˆ©ç›Š', 'æ¥­ç¸¾', 'ã‚·ã‚§ã‚¢', 'ç«¶åˆ', 'äº‹æ¥­', 'æˆ¦ç•¥', 'åˆ†æ']
        
        for result in results:
            title = result.get('title', '').lower()
            snippet = result.get('snippet', '').lower()
            
            # é™¤å¤–æ¡ä»¶ãƒã‚§ãƒƒã‚¯
            if any(exclude in title or exclude in snippet for exclude in exclude_keywords):
                continue
            
            # ä¼æ¥­åã®è¨€åŠãƒã‚§ãƒƒã‚¯
            if company_name.lower() not in title and company_name.lower() not in snippet:
                continue
            
            # é–¢é€£æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
            relevance_score = 0
            for keyword in important_keywords:
                if keyword in title:
                    relevance_score += 2
                if keyword in snippet:
                    relevance_score += 1
            
            # ä¿¡é ¼ã§ãã‚‹ã‚½ãƒ¼ã‚¹ã‹ãƒã‚§ãƒƒã‚¯
            url = result.get('link', '')
            if any(domain in url for domain in ['nikkei.com', 'toyokeizai.net', 'diamond.jp', 'itmedia.co.jp']):
                relevance_score += 3
            
            # æœ€ä½é–¢é€£æ€§ã‚¹ã‚³ã‚¢ã®é–¾å€¤
            if relevance_score >= 3:
                result['relevance_score'] = relevance_score
                filtered_results.append(result)
        
        return sorted(filtered_results, key=lambda x: x['relevance_score'], reverse=True)
    
    def search_ir_documents_with_serpapi(self, company_name):
        """SerpAPIã‚’ä½¿ç”¨ã—ã¦IRé–¢é€£æ–‡æ›¸ã‚’æ¤œç´¢ãƒ»åé›†"""
        ir_data = []
        
        st.info("ğŸ” SerpAPIã§IRé–¢é€£è³‡æ–™ã‚’æ¤œç´¢ä¸­...")
        
        # SerpAPIã‚­ãƒ¼ã‚’å–å¾—
        serpapi_key = self.get_serpapi_key()
        if not serpapi_key:
            st.warning("âš ï¸ SerpAPIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return []
        
        # IRé–¢é€£ã®æ¤œç´¢ã‚¯ã‚¨ãƒªï¼ˆã‚ˆã‚Šå…·ä½“çš„ï¼‰
        ir_search_queries = [
            f'"{company_name}" æ±ºç®—çŸ­ä¿¡ OR æ±ºç®—èª¬æ˜ä¼š OR æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ filetype:pdf',
            f'"{company_name}" ä¸­æœŸçµŒå–¶è¨ˆç”» OR äº‹æ¥­æˆ¦ç•¥ OR æ¥­ç¸¾ filetype:pdf',
            f'"{company_name}" IRæƒ…å ± OR æŠ•è³‡å®¶å‘ã‘ OR è²¡å‹™æƒ…å ± site:*.co.jp'
        ]
        
        for i, query in enumerate(ir_search_queries, 1):
            st.write(f"ğŸ” IRæ¤œç´¢ {i}/{len(ir_search_queries)}: {query[:60]}...")
            
            try:
                results = self.search_with_serpapi(query, serpapi_key)
                
                if results and 'organic_results' in results:
                    ir_results = self.filter_ir_documents(results['organic_results'], company_name)
                    
                    for result in ir_results[:2]:  # é–¢é€£æ€§ã®é«˜ã„ä¸Šä½2ä»¶
                        ir_data.append({
                            'title': result.get('title', ''),
                            'snippet': result.get('snippet', ''),
                            'url': result.get('link', ''),
                            'source': self.extract_domain(result.get('link', '')),
                            'document_type': self.classify_ir_document(result.get('title', ''), result.get('snippet', '')),
                            'type': 'IRé–¢é€£è³‡æ–™'
                        })
                    
                    st.success(f"âœ… {len(ir_results[:2])}ä»¶ã®IRé–¢é€£è³‡æ–™ã‚’ç™ºè¦‹")
                else:
                    st.warning(f"âš ï¸ IRæ¤œç´¢ {i}: çµæœãªã—")
                    
            except Exception as e:
                st.warning(f"âš ï¸ IRæ¤œç´¢ {i} ã‚¨ãƒ©ãƒ¼: {str(e)}")
                continue
        
        return ir_data[:6]  # æœ€å¤§6ä»¶ã®IRé–¢é€£æƒ…å ±
    
    def filter_ir_documents(self, results, company_name):
        """æ¤œç´¢çµæœã‹ã‚‰IRé–¢é€£æ–‡æ›¸ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        filtered_results = []
        
        # IRé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        ir_keywords = ['æ±ºç®—', 'æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸', 'ä¸­æœŸçµŒå–¶è¨ˆç”»', 'æ¥­ç¸¾', 'IR', 'æŠ•è³‡å®¶', 'è²¡å‹™', 'å£²ä¸Š', 'åˆ©ç›Š', 'æˆ¦ç•¥']
        
        # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        exclude_keywords = ['æ±‚äºº', 'è»¢è·', 'æ¡ç”¨', 'æ–°å’', 'å£ã‚³ãƒŸ', 'indeed', 'ãƒªã‚¯ãƒŠãƒ“', 'ãƒã‚¤ãƒŠãƒ“']
        
        for result in results:
            title = result.get('title', '').lower()
            snippet = result.get('snippet', '').lower()
            
            # é™¤å¤–æ¡ä»¶ãƒã‚§ãƒƒã‚¯
            if any(exclude in title or exclude in snippet for exclude in exclude_keywords):
                continue
            
            # ä¼æ¥­åã®è¨€åŠãƒã‚§ãƒƒã‚¯
            if company_name.lower() not in title and company_name.lower() not in snippet:
                continue
            
            # IRé–¢é€£åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
            ir_score = 0
            for keyword in ir_keywords:
                if keyword in title:
                    ir_score += 3
                if keyword in snippet:
                    ir_score += 1
            
            # PDFæ–‡æ›¸ã¯ã‚¹ã‚³ã‚¢è¿½åŠ 
            url = result.get('link', '')
            if '.pdf' in url or 'filetype:pdf' in url:
                ir_score += 2
            
            # ä¼æ¥­å…¬å¼ã‚µã‚¤ãƒˆã¯ã‚¹ã‚³ã‚¢è¿½åŠ 
            if company_name.lower() in url or '.co.jp' in url:
                ir_score += 2
            
            # æœ€ä½IRé–¢é€£ã‚¹ã‚³ã‚¢ã®é–¾å€¤
            if ir_score >= 3:
                result['ir_score'] = ir_score
                filtered_results.append(result)
        
        return sorted(filtered_results, key=lambda x: x['ir_score'], reverse=True)
    
    def establish_company_fundamentals(self, company_name):
        """ä¼æ¥­åŸºæœ¬æƒ…å ±ã®ç¢ºç«‹ï¼ˆä¸»åŠ›äº‹æ¥­ãƒ»æ¥­ç•Œåˆ†é¡ãƒ»ç«¶åˆã®æ­£ç¢ºãªç‰¹å®šï¼‰"""
        st.info("ğŸ¢ ä¼æ¥­åŸºæœ¬æƒ…å ±ã‚’ç¢ºç«‹ä¸­ï¼ˆä¸»åŠ›äº‹æ¥­ãƒ»æ¥­ç•Œåˆ†é¡ãƒ»ç«¶åˆã®ç‰¹å®šï¼‰...")
        
        # SerpAPIã‚’ä½¿ç”¨ã—ã¦ä¼æ¥­åŸºæœ¬æƒ…å ±ã‚’åé›†
        serpapi_key = self.get_serpapi_key()
        if not serpapi_key:
            st.warning("âš ï¸ SerpAPIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return self.create_fallback_company_fundamentals(company_name)
        
        # ä¼æ¥­åŸºæœ¬æƒ…å ±ã®æ¤œç´¢ã‚¯ã‚¨ãƒª
        fundamental_queries = [
            f'"{company_name}" ä¼šç¤¾æ¦‚è¦ OR ä¼æ¥­æ¦‚è¦ OR äº‹æ¥­å†…å®¹ site:*.co.jp',
            f'"{company_name}" ä¸»åŠ›äº‹æ¥­ OR ã‚³ã‚¢äº‹æ¥­ OR å£²ä¸Šæ§‹æˆ site:*.co.jp',
            f'"{company_name}" æ¥­ç•Œ OR ã‚»ã‚¯ã‚¿ãƒ¼ OR ç«¶åˆä»–ç¤¾'
        ]
        
        company_fundamentals = {
            'company_name': company_name,
            'primary_business': '',
            'industry_classification': '',
            'business_segments': [],
            'competitors': [],
            'confidence_score': 0
        }
        
        for i, query in enumerate(fundamental_queries, 1):
            st.write(f"ğŸ” åŸºæœ¬æƒ…å ±æ¤œç´¢ {i}/{len(fundamental_queries)}: {query[:60]}...")
            
            try:
                results = self.search_with_serpapi(query, serpapi_key)
                
                if results and 'organic_results' in results:
                    filtered_results = self.filter_company_fundamental_results(results['organic_results'], company_name)
                    
                    for result in filtered_results[:2]:
                        self.extract_fundamental_data(result, company_fundamentals)
                    
                    st.success(f"âœ… {len(filtered_results[:2])}ä»¶ã®åŸºæœ¬æƒ…å ±ã‚’æŠ½å‡º")
                else:
                    st.warning(f"âš ï¸ åŸºæœ¬æƒ…å ±æ¤œç´¢ {i}: çµæœãªã—")
                    
            except Exception as e:
                st.warning(f"âš ï¸ åŸºæœ¬æƒ…å ±æ¤œç´¢ {i} ã‚¨ãƒ©ãƒ¼: {str(e)}")
                continue
        
        # åŸºæœ¬æƒ…å ±ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        company_fundamentals = self.validate_company_fundamentals(company_fundamentals)
        
        return company_fundamentals
    
    def filter_company_fundamental_results(self, results, company_name):
        """ä¼æ¥­åŸºæœ¬æƒ…å ±ã®æ¤œç´¢çµæœã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        filtered_results = []
        
        # åŸºæœ¬æƒ…å ±é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        fundamental_keywords = ['ä¼šç¤¾æ¦‚è¦', 'ä¼æ¥­æ¦‚è¦', 'äº‹æ¥­å†…å®¹', 'ä¸»åŠ›äº‹æ¥­', 'æ¥­ç•Œ', 'ç«¶åˆ', 'ã‚»ã‚¯ã‚¿ãƒ¼', 'å£²ä¸Šæ§‹æˆ']
        
        # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        exclude_keywords = ['æ±‚äºº', 'è»¢è·', 'æ¡ç”¨', 'æ–°å’', 'å£ã‚³ãƒŸ', 'indeed', 'ãƒªã‚¯ãƒŠãƒ“', 'ãƒã‚¤ãƒŠãƒ“', 'ã‚¨ãƒ³è»¢è·']
        
        for result in results:
            title = result.get('title', '').lower()
            snippet = result.get('snippet', '').lower()
            
            # é™¤å¤–æ¡ä»¶ãƒã‚§ãƒƒã‚¯
            if any(exclude in title or exclude in snippet for exclude in exclude_keywords):
                continue
            
            # ä¼æ¥­åã®è¨€åŠãƒã‚§ãƒƒã‚¯
            if company_name.lower() not in title and company_name.lower() not in snippet:
                continue
            
            # åŸºæœ¬æƒ…å ±é–¢é€£åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
            fundamental_score = 0
            for keyword in fundamental_keywords:
                if keyword in title:
                    fundamental_score += 3
                if keyword in snippet:
                    fundamental_score += 1
            
            # ä¼æ¥­å…¬å¼ã‚µã‚¤ãƒˆã¯ã‚¹ã‚³ã‚¢è¿½åŠ 
            url = result.get('link', '')
            if company_name.lower() in url or '.co.jp' in url:
                fundamental_score += 3
            
            # æœ€ä½é–¢é€£æ€§ã‚¹ã‚³ã‚¢ã®é–¾å€¤
            if fundamental_score >= 2:
                result['fundamental_score'] = fundamental_score
                filtered_results.append(result)
        
        return sorted(filtered_results, key=lambda x: x['fundamental_score'], reverse=True)
    
    def extract_fundamental_data(self, result, company_fundamentals):
        """æ¤œç´¢çµæœã‹ã‚‰ä¼æ¥­åŸºæœ¬æƒ…å ±ã‚’æŠ½å‡º"""
        title = result.get('title', '')
        snippet = result.get('snippet', '')
        text = (title + ' ' + snippet).lower()
        
        # ä¸»åŠ›äº‹æ¥­ã®æ¨å®š
        if 'äººæ' in text or 'hr' in text or 'è»¢è·' in text or 'æ±‚äºº' in text:
            if not company_fundamentals['primary_business']:
                company_fundamentals['primary_business'] = 'äººæã‚µãƒ¼ãƒ“ã‚¹'
                company_fundamentals['industry_classification'] = 'HRãƒ»äººæã‚µãƒ¼ãƒ“ã‚¹'
                company_fundamentals['competitors'].extend(['ãƒã‚¤ãƒŠãƒ“', 'ã‚¨ãƒ³ãƒ»ã‚¸ãƒ£ãƒ‘ãƒ³', 'ãƒ‘ãƒ¼ã‚½ãƒ«ã‚­ãƒ£ãƒªã‚¢'])
        
        elif 'ä¸å‹•ç”£' in text or 'suumo' in text or 'ä½å®…' in text:
            if 'äººæ' not in company_fundamentals['primary_business']:
                company_fundamentals['business_segments'].append('ä¸å‹•ç”£æƒ…å ±ã‚µãƒ¼ãƒ“ã‚¹')
        
        elif 'it' in text or 'ã‚·ã‚¹ãƒ†ãƒ ' in text or 'ãƒ‡ã‚¸ã‚¿ãƒ«' in text:
            company_fundamentals['business_segments'].append('ITãƒ»ãƒ‡ã‚¸ã‚¿ãƒ«ã‚µãƒ¼ãƒ“ã‚¹')
        
        elif 'åºƒå‘Š' in text or 'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°' in text:
            company_fundamentals['business_segments'].append('åºƒå‘Šãƒ»ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°')
        
        # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®æ›´æ–°
        if '.co.jp' in result.get('link', ''):
            company_fundamentals['confidence_score'] += 10
        
        company_fundamentals['confidence_score'] += result.get('fundamental_score', 0)
    
    def validate_company_fundamentals(self, company_fundamentals):
        """ä¼æ¥­åŸºæœ¬æƒ…å ±ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯"""
        
        # ãƒªã‚¯ãƒ«ãƒ¼ãƒˆã®å ´åˆã®ç‰¹åˆ¥å‡¦ç†ï¼ˆæ—¢çŸ¥ã®æ­£ç¢ºãªæƒ…å ±ã§è£œå®Œï¼‰
        if 'ãƒªã‚¯ãƒ«ãƒ¼ãƒˆ' in company_fundamentals['company_name']:
            company_fundamentals['primary_business'] = 'äººæã‚µãƒ¼ãƒ“ã‚¹ãƒ»HR Tech'
            company_fundamentals['industry_classification'] = 'HRãƒ»äººæã‚µãƒ¼ãƒ“ã‚¹'
            company_fundamentals['business_segments'] = ['äººæã‚µãƒ¼ãƒ“ã‚¹', 'åºƒå‘Šãƒ»ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°', 'ä¸å‹•ç”£æƒ…å ±ã‚µãƒ¼ãƒ“ã‚¹', 'SaaSãƒ»HRãƒ†ãƒƒã‚¯']
            company_fundamentals['competitors'] = ['ãƒã‚¤ãƒŠãƒ“', 'ã‚¨ãƒ³ãƒ»ã‚¸ãƒ£ãƒ‘ãƒ³', 'ãƒ‘ãƒ¼ã‚½ãƒ«ã‚­ãƒ£ãƒªã‚¢', 'ãƒ“ã‚ºãƒªãƒ¼ãƒ']
            company_fundamentals['confidence_score'] = 95
        
        # ä¿¡é ¼åº¦ãŒä½ã„å ´åˆã®è­¦å‘Š
        if company_fundamentals['confidence_score'] < 30:
            st.warning("âš ï¸ ä¼æ¥­åŸºæœ¬æƒ…å ±ã®ä¿¡é ¼åº¦ãŒä½ã„ã§ã™ã€‚åˆ†æçµæœã®ç²¾åº¦ã«å½±éŸ¿ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        
        return company_fundamentals
    
    def create_fallback_company_fundamentals(self, company_name):
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ä¼æ¥­åŸºæœ¬æƒ…å ±"""
        return {
            'company_name': company_name,
            'primary_business': 'ä¸æ˜ï¼ˆè¦ç¢ºèªï¼‰',
            'industry_classification': 'ä¸æ˜ï¼ˆè¦ç¢ºèªï¼‰',
            'business_segments': [],
            'competitors': [],
            'confidence_score': 0
        }
    
    def classify_ir_document(self, title, snippet):
        """IRæ–‡æ›¸ã®ç¨®é¡ã‚’åˆ†é¡"""
        text = (title + ' ' + snippet).lower()
        
        if 'æ±ºç®—çŸ­ä¿¡' in text or 'æ±ºç®—èª¬æ˜' in text:
            return 'æ±ºç®—çŸ­ä¿¡ãƒ»èª¬æ˜è³‡æ–™'
        elif 'æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸' in text or '10-k' in text:
            return 'æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸'
        elif 'ä¸­æœŸçµŒå–¶è¨ˆç”»' in text or 'çµŒå–¶æˆ¦ç•¥' in text:
            return 'ä¸­æœŸçµŒå–¶è¨ˆç”»ãƒ»æˆ¦ç•¥è³‡æ–™'
        elif 'æ¥­ç¸¾' in text or 'è²¡å‹™' in text:
            return 'æ¥­ç¸¾ãƒ»è²¡å‹™è³‡æ–™'
        else:
            return 'IRé–¢é€£è³‡æ–™'
    
    def search_with_serpapi(self, query, api_key):
        """SerpAPIã‚’ä½¿ç”¨ã—ãŸæ¤œç´¢å®Ÿè¡Œ"""
        url = "https://serpapi.com/search"
        params = {
            "q": query,
            "api_key": api_key,
            "engine": "google",
            "num": 5,  # ç„¡æ–™æ ç¯€ç´„
            "hl": "ja",  # æ—¥æœ¬èª
            "gl": "jp"   # æ—¥æœ¬åœ°åŸŸ
        }
        
        response = requests.get(url, params=params, timeout=15)
        
        if response.status_code == 200:
            return response.json()
        else:
            st.warning(f"SerpAPI Error: {response.status_code}")
            return None
    
    def extract_domain(self, url):
        """URLã‹ã‚‰ãƒ‰ãƒ¡ã‚¤ãƒ³åã‚’æŠ½å‡º"""
        if not url:
            return "ä¸æ˜"
        
        try:
            from urllib.parse import urlparse
            domain = urlparse(url).netloc
            
            # æ—¥æœ¬ã®ä¸»è¦ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’è­˜åˆ¥
            if 'nikkei.com' in domain:
                return 'æ—¥æœ¬çµŒæ¸ˆæ–°è'
            elif 'toyokeizai.net' in domain:
                return 'æ±æ´‹çµŒæ¸ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³'
            elif 'diamond.jp' in domain:
                return 'ãƒ€ã‚¤ãƒ¤ãƒ¢ãƒ³ãƒ‰ãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³'
            elif 'itmedia.co.jp' in domain:
                return 'ITmedia'
            else:
                return domain
        except:
            return "ä¸æ˜"
    
    def create_fallback_external_data(self, company_name, industry_keywords):
        """å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æƒ…å ±ç”Ÿæˆ"""
        fallback_data = []
        
        # ä¸€èˆ¬çš„ãªæ¥­ç•Œæƒ…å ±ï¼ˆæ¨å®šãƒ™ãƒ¼ã‚¹ï¼‰
        if 'ãƒªã‚¯ãƒ«ãƒ¼ãƒˆ' in company_name:
            fallback_data = [
                {
                    'title': 'äººæãƒ»ä½å®…æƒ…å ±ã‚µãƒ¼ãƒ“ã‚¹æ¥­ç•Œã®å‹•å‘',
                    'snippet': 'äººææƒ…å ±ã‚µãƒ¼ãƒ“ã‚¹æ¥­ç•Œã¯ç¶™ç¶šçš„ãªæˆé•·ã‚’ç¤ºã—ã¦ãŠã‚Šã€ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–ã¨AIæ´»ç”¨ãŒé€²å±•ã—ã¦ã„ã‚‹ã€‚ä½å®…æƒ…å ±ã‚µãƒ¼ãƒ“ã‚¹ã‚‚åŒæ§˜ã«DXåŒ–ãŒåŠ é€Ÿã€‚',
                    'url': 'https://example.com/industry-trend',
                    'source': 'ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æ',
                    'type': 'æ¨å®šæƒ…å ±'
                },
                {
                    'title': 'æƒ…å ±ã‚µãƒ¼ãƒ“ã‚¹æ¥­ç•Œã®ç«¶åˆçŠ¶æ³',
                    'snippet': 'ä½å®…æƒ…å ±åˆ†é‡ã§ã¯è¤‡æ•°ã®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ãŒç«¶åˆã€‚äººæã‚µãƒ¼ãƒ“ã‚¹ã§ã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«ä¼æ¥­ã¨ã®ç«¶äº‰ãŒæ¿€åŒ–ã€‚',
                    'url': 'https://example.com/competition',
                    'source': 'ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æ',
                    'type': 'æ¨å®šæƒ…å ±'
                }
            ]
        else:
            # æ±ç”¨çš„ãªæ¥­ç•Œæƒ…å ±
            fallback_data = [
                {
                    'title': f'{industry_keywords}æ¥­ç•Œã®å¸‚å ´å‹•å‘',
                    'snippet': 'å½“è©²æ¥­ç•Œã§ã¯æŠ€è¡“é©æ–°ã¨ãƒ‡ã‚¸ã‚¿ãƒ«å¤‰é©ãŒé€²ã‚“ã§ãŠã‚Šã€å¸‚å ´ç’°å¢ƒã¯å¤‰åŒ–ã—ã¦ã„ã‚‹ã€‚',
                    'url': 'https://example.com/market-trend',
                    'source': 'ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æ',
                    'type': 'æ¨å®šæƒ…å ±'
                }
            ]
        
        st.info(f"ğŸ’¡ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æƒ…å ±ã‚’ç”Ÿæˆ: {len(fallback_data)}ä»¶")
        return fallback_data
    
    def extract_search_keywords(self, question):
        """è³ªå•ã‹ã‚‰æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        # ä¼æ¥­åˆ†æã«é–¢é€£ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°
        keyword_mapping = {
            "å£²ä¸Š": ["å£²ä¸Š", "revenue", "æ¥­ç¸¾", "æ±ºç®—"],
            "åˆ©ç›Š": ["åˆ©ç›Š", "profit", "å–¶æ¥­åˆ©ç›Š", "å½“æœŸç´”åˆ©ç›Š"],
            "äº‹æ¥­": ["äº‹æ¥­", "business", "ã‚µãƒ¼ãƒ“ã‚¹", "äº‹æ¥­å†…å®¹"],
            "æ¡ç”¨": ["æ¡ç”¨", "recruit", "æ–°å’", "ä¸­é€”", "æ±‚äºº"],
            "åƒãæ–¹": ["åƒãæ–¹", "work", "ãƒªãƒ¢ãƒ¼ãƒˆ", "åˆ¶åº¦", "ç¦åˆ©åšç”Ÿ"],
            "å°†æ¥": ["å°†æ¥", "future", "æˆ¦ç•¥", "è¨ˆç”»", "ãƒ“ã‚¸ãƒ§ãƒ³"],
            "ç«¶åˆ": ["ç«¶åˆ", "ç«¶äº‰", "ãƒ©ã‚¤ãƒãƒ«", "ã‚·ã‚§ã‚¢", "å¸‚å ´"],
            "æŠ€è¡“": ["æŠ€è¡“", "technology", "IT", "DX", "ã‚·ã‚¹ãƒ†ãƒ "]
        }
        
        question_lower = question.lower()
        found_keywords = []
        
        for category, keywords in keyword_mapping.items():
            for keyword in keywords:
                if keyword in question_lower:
                    found_keywords.extend(keywords)
                    break
        
        return list(set(found_keywords)) if found_keywords else ["ä¼æ¥­æƒ…å ±", "ä¼šç¤¾æ¦‚è¦"]
    
    def extract_relevant_content(self, soup, keywords, question):
        """HTMLã‹ã‚‰è³ªå•ã«é–¢é€£ã™ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ·±åº¦æŠ½å‡º"""
        relevant_texts = []
        
        # ã‚ˆã‚Šè©³ç´°ãªè¦ç´ ã‚’å¯¾è±¡ã«æ‹¡å¼µ
        content_tags = ['h1', 'h2', 'h3', 'h4', 'p', 'div', 'li', 'span', 'td', 'th']
        
        for tag in soup.find_all(content_tags):
            text = tag.get_text().strip()
            
            # ãƒ†ã‚­ã‚¹ãƒˆé•·ã®æ¡ä»¶ã‚’ç·©å’Œï¼ˆçŸ­ã„é‡è¦æƒ…å ±ã‚‚å–å¾—ï¼‰
            if len(text) < 10 or len(text) > 800:
                continue
                
            text_lower = text.lower()
            question_lower = question.lower()
            
            relevance_score = 0
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ï¼ˆé‡ã¿ä»˜ã‘å¼·åŒ–ï¼‰
            for keyword in keywords:
                keyword_lower = keyword.lower()
                if keyword_lower in text_lower:
                    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å®Œå…¨ä¸€è‡´
                    if keyword_lower in text_lower.split():
                        relevance_score += 3
                    else:
                        relevance_score += 2
            
            # è³ªå•ã®å˜èªãƒãƒƒãƒãƒ³ã‚°
            question_words = [w for w in question_lower.split() if len(w) > 2]
            for word in question_words:
                if word in text_lower:
                    relevance_score += 1
            
            # ä¼æ¥­åˆ†æã«é‡è¦ãªç”¨èªã¸ã®è¿½åŠ ã‚¹ã‚³ã‚¢
            important_terms = [
                'å£²ä¸Š', 'åˆ©ç›Š', 'æ¥­ç¸¾', 'æ±ºç®—', 'æˆ¦ç•¥', 'è¨ˆç”»', 'äº‹æ¥­', 'ãƒ“ã‚¸ãƒ§ãƒ³',
                'å¼·ã¿', 'ç‰¹å¾´', 'ç«¶åˆ', 'å¸‚å ´', 'æŠ€è¡“', 'DX', 'AI', 'ã‚µã‚¹ãƒ†ãƒŠãƒ“ãƒªãƒ†ã‚£',
                'æ¡ç”¨', 'äººæ', 'åƒãæ–¹', 'åˆ¶åº¦', 'ç¦åˆ©åšç”Ÿ', 'ãƒŸãƒƒã‚·ãƒ§ãƒ³'
            ]
            
            for term in important_terms:
                if term in text:
                    relevance_score += 1.5
            
            # æ•°å€¤ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯é‡è¦åº¦UP
            if any(char.isdigit() for char in text) and ('å„„' in text or 'ä¸‡' in text or '%' in text):
                relevance_score += 2
            
            if relevance_score > 0:
                relevant_texts.append({
                    'text': text,
                    'score': relevance_score
                })
        
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ã‚’è¿”ã™ï¼ˆæ•°ã‚’å¢—åŠ ï¼‰
        relevant_texts.sort(key=lambda x: x['score'], reverse=True)
        
        # ã‚ˆã‚Šå¤šãã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å«ã‚ã‚‹ï¼ˆä¸Šä½6ä»¶ï¼‰
        top_texts = [item['text'] for item in relevant_texts[:6]]
        
        return '\n\n'.join(top_texts) if top_texts else ""
    
    def classify_source_type(self, url):
        """URLã‹ã‚‰ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã‚’åˆ†é¡"""
        if '/ir/' in url:
            return 'IRæƒ…å ±'
        elif '/news/' in url:
            return 'ãƒ‹ãƒ¥ãƒ¼ã‚¹'
        elif '/recruit/' in url:
            return 'æ¡ç”¨æƒ…å ±'
        elif '/company/' in url:
            return 'ä¼šç¤¾æ¦‚è¦'
        elif '/sustainability/' in url:
            return 'ã‚µã‚¹ãƒ†ãƒŠãƒ“ãƒªãƒ†ã‚£'
        else:
            return 'ä¼æ¥­æƒ…å ±'
    
    def generate_chat_response(self, question, analysis_data, company_info, chat_history):
        """æ‹¡å¼µãƒãƒ£ãƒƒãƒˆè³ªå•ã¸ã®å›ç­”ç”Ÿæˆï¼ˆæ—¢å­˜ã‚½ãƒ¼ã‚¹æ´»ç”¨ï¼‰"""
        
        # Step 1: åŸºæœ¬çš„ãªåˆ†æçµæœã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦æ•´ç†
        base_context = f"""
åˆ†æå¯¾è±¡ä¼æ¥­: {company_info['company_name']}
åˆ†æé‡ç‚¹åˆ†é‡: {company_info['focus_area']}

ã€EVPåˆ†æçµæœã€‘:
{json.dumps(analysis_data.get('evp', {}), ensure_ascii=False, indent=2)}

ã€ãƒ“ã‚¸ãƒã‚¹åˆ†æçµæœã€‘:
{json.dumps(analysis_data.get('business_analysis', {}), ensure_ascii=False, indent=2)}
"""
        
        # Step 2: æ—¢å­˜ã‚½ãƒ¼ã‚¹ã‹ã‚‰è¿½åŠ æƒ…å ±ã‚’æ¤œç´¢
        st.info("ğŸ” ä¼æ¥­ã‚µã‚¤ãƒˆã‚’æ·±åº¦èª¿æŸ»ä¸­...")
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®è¡¨ç¤º
        company_domain = company_info.get('company_domain')
        if company_domain:
            st.write(f"èª¿æŸ»å¯¾è±¡ãƒ‰ãƒ¡ã‚¤ãƒ³: {company_domain}")
        else:
            st.warning("âš ï¸ ä¼æ¥­ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åŸºæœ¬åˆ†æçµæœã®ã¿ã§å›ç­”ã—ã¾ã™ã€‚")
        
        additional_sources = self.search_existing_sources(question, {
            'company_domain': company_domain
        })
        
        # ãƒ‡ãƒãƒƒã‚°: è©³ç´°ãªèª¿æŸ»çµæœã‚’è¡¨ç¤º
        if additional_sources:
            st.success(f"âœ… {len(additional_sources)}ä»¶ã®æ·±åº¦èª¿æŸ»æƒ…å ±ã‚’ç™ºè¦‹")
            with st.expander("ğŸ” æ·±åº¦èª¿æŸ»è©³ç´°"):
                for i, source in enumerate(additional_sources, 1):
                    st.write(f"**{i}. {source.get('source_type', 'N/A')}**")
                    st.write(f"URL: {source.get('url', 'N/A')}")
                    st.write(f"æ·±åº¦: {source.get('depth', 'N/A')}")
                    if source.get('content'):
                        content_preview = source['content'][:200] + "..." if len(source['content']) > 200 else source['content']
                        st.write(f"å†…å®¹: {content_preview}")
                    st.write("---")
        else:
            st.warning("âš ï¸ æ·±åº¦èª¿æŸ»ã§è¿½åŠ æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
        # è¿½åŠ æƒ…å ±ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ
        additional_context = ""
        if additional_sources:
            st.success(f"âœ… {len(additional_sources)}ä»¶ã®è¿½åŠ æƒ…å ±ã‚’ç™ºè¦‹")
            additional_context = "\nã€è¿½åŠ åé›†æƒ…å ±ã€‘:\n"
            for i, source in enumerate(additional_sources, 1):
                additional_context += f"\n{i}. {source['source_type']} ({source['url']}):\n{source['content']}\n"
        else:
            st.info("â„¹ï¸ è¿½åŠ æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚åˆ†æçµæœã®ã¿ã§å›ç­”ã—ã¾ã™ã€‚")
        
        # Step 3: ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®æ•´ç†
        history_context = ""
        if chat_history:
            history_context = "ã€éå»ã®è³ªç–‘å¿œç­”ã€‘:\n"
            for q, a in chat_history[-2:]:  # ç›´è¿‘2ä»¶ã®ã¿
                history_context += f"Q: {q}\nA: {a}\n\n"
        
        # Step 4: æ‹¡å¼µãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆï¼ˆãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³é˜²æ­¢å¼·åŒ–ï¼‰
        enhanced_prompt = f"""
ã‚ãªãŸã¯ä¼æ¥­åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã‚’å³æ ¼ã«å®ˆã£ã¦å›ç­”ã—ã¦ãã ã•ã„ï¼š

ã€CRITICALå›ç­”ãƒ«ãƒ¼ãƒ« - å¿…ãšéµå®ˆã€‘
1. æä¾›ã•ã‚ŒãŸæƒ…å ±ã®ã¿ã‚’ä½¿ç”¨ - æ¨æ¸¬ã‚„ä¸€èˆ¬è«–ã¯ç¦æ­¢
2. æƒ…å ±æºã‚’å¿…ãšæ˜è¨˜ï¼šã€Œåˆ†æçµæœã«ã‚ˆã‚‹ã¨ã€ã€Œä¼æ¥­ã‚µã‚¤ãƒˆã®â—‹â—‹ã«ã‚ˆã‚‹ã¨ã€
3. ãƒ‡ãƒ¼ã‚¿ã«ãªã„æƒ…å ±ã¯çµ¶å¯¾ã«ä½œã‚‰ãšã€Œæä¾›ã•ã‚ŒãŸæƒ…å ±ã«ã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€ã¨æ˜è¨˜
4. å…·ä½“çš„ãªæ•°å€¤ãƒ»æ—¥ä»˜ãƒ»å›ºæœ‰åè©ã¯æä¾›ãƒ‡ãƒ¼ã‚¿ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ã®ã¿ä½¿ç”¨
5. å›ç­”ã¯300-500æ–‡å­—ç¨‹åº¦ã§å…·ä½“çš„ã«ã€ä½†ã—æ ¹æ‹ ãªãæƒ…å ±ã¯ä¸€åˆ‡å«ã‚ãªã„

ã€æä¾›ãƒ‡ãƒ¼ã‚¿ã€‘
{base_context}

{additional_context}

{history_context}

ã€ç¾åœ¨ã®è³ªå•ã€‘: {question}

ã€æŒ‡ç¤ºã€‘
ä¸Šè¨˜ã®æä¾›ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ã€è³ªå•ã«å¯¾ã™ã‚‹å…·ä½“çš„ã§æœ‰ç”¨ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
ãƒ‡ãƒ¼ã‚¿ã«è¨˜è¼‰ã•ã‚Œã¦ã„ãªã„æƒ…å ±ã¯æ¨æ¸¬ã›ãšã€ã€Œæä¾›ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã«ã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€ã¨æ˜è¨˜ã—ã¦ãã ã•ã„ã€‚
æƒ…å ±æºï¼ˆåˆ†æçµæœã¾ãŸã¯è¿½åŠ åé›†æƒ…å ±ï¼‰ã‚’å¿…ãšæ˜è¨˜ã—ã¦ãã ã•ã„ã€‚
"""
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": enhanced_prompt}],
                temperature=0.2,
                max_tokens=500
            )
            
            answer = response.choices[0].message.content.strip()
            
            # å‡ºå…¸æƒ…å ±ã‚’è¿½åŠ 
            if additional_sources:
                answer += "\n\nğŸ“š **å‚ç…§ã—ãŸè¿½åŠ æƒ…å ±:**"
                for source in additional_sources:
                    answer += f"\nâ€¢ {source['source_type']}: {source['url']}"
            
            return answer
            
        except Exception as e:
            return f"âŒ å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\n\nåˆ†æãƒ‡ãƒ¼ã‚¿ã‚’å‚ç…§ã—ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
    
    def get_openai_api_key(self):
        """APIã‚­ãƒ¼å–å¾—ï¼ˆæœ¬ç•ªç’°å¢ƒå¯¾å¿œï¼‰"""
        # Streamlit Cloud ã®Secretsæ©Ÿèƒ½ã‚’å„ªå…ˆ
        if hasattr(st, 'secrets') and "OPENAI_API_KEY" in st.secrets:
            return st.secrets["OPENAI_API_KEY"]
        # ç’°å¢ƒå¤‰æ•°ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        elif os.getenv("OPENAI_API_KEY"):
            return os.getenv("OPENAI_API_KEY")
        else:
            st.error("âš ï¸ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            st.markdown("""
            **è¨­å®šæ–¹æ³•:**
            - Streamlit Cloud: Secretsæ©Ÿèƒ½ã§OPENAI_API_KEYã‚’è¨­å®š
            - ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œ: ç’°å¢ƒå¤‰æ•°ã§OPENAI_API_KEYã‚’è¨­å®š
            """)
            st.stop()
    
    def get_serpapi_key(self):
        """SerpAPI ã‚­ãƒ¼å–å¾—ï¼ˆæœ¬ç•ªç’°å¢ƒå¯¾å¿œï¼‰"""
        # Streamlit Cloud ã®Secretsæ©Ÿèƒ½ã‚’å„ªå…ˆ
        if hasattr(st, 'secrets') and "SERPAPI_KEY" in st.secrets:
            return st.secrets["SERPAPI_KEY"]
        # ç’°å¢ƒå¤‰æ•°ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        elif os.getenv("SERPAPI_KEY"):
            return os.getenv("SERPAPI_KEY")
        else:
            st.warning("âš ï¸ SerpAPI ã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å¤–éƒ¨æ¤œç´¢æ©Ÿèƒ½ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¾ã™ã€‚")
            st.markdown("""
            **SerpAPIè¨­å®šæ–¹æ³•ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰:**
            - 1. https://serpapi.com ã§ã‚¢ã‚«ã‚¦ãƒ³ãƒˆä½œæˆï¼ˆç„¡æ–™æ æœˆ100å›ï¼‰
            - 2. Streamlit Cloud: Secretsæ©Ÿèƒ½ã§SERPAPI_KEYã‚’è¨­å®š
            - 3. ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œ: ç’°å¢ƒå¤‰æ•°ã§SERPAPI_KEYã‚’è¨­å®š
            """)
            return None
    
    def create_research_prompt(self, company_info):
        """å·¥å¤«ã•ã‚ŒãŸãƒªã‚µãƒ¼ãƒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ"""
        prompt = f"""
ã‚ãªãŸã¯ä¼æ¥­ãƒªã‚µãƒ¼ãƒã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ä¼æ¥­ã«ã¤ã„ã¦ã€EVPï¼ˆEmployee Value Propositionï¼‰ã¨ä¼æ¥­åˆ†æã®å„é …ç›®ã‚’è©³ç´°ã«èª¿æŸ»ã—ã€å…·ä½“çš„ãªæƒ…å ±ã‚’åé›†ã—ã¦ãã ã•ã„ã€‚

## èª¿æŸ»å¯¾è±¡ä¼æ¥­
- ä¼æ¥­å: {company_info['company_name']}
- ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸: {company_info.get('website_url', 'ä¸æ˜')}
- é‡ç‚¹åˆ†é‡: {company_info['focus_area']}

### EVPï¼ˆEmployee Value Propositionï¼‰é …ç›®

#### 1. Rewardsï¼ˆå ±é…¬ãƒ»å¾…é‡ï¼‰
- åŸºæœ¬çµ¦ä¸æ°´æº–ã€è³ä¸åˆ¶åº¦ã€ç¦åˆ©åšç”Ÿã€å„ç¨®æ‰‹å½“ã‚„ã‚¤ãƒ³ã‚»ãƒ³ãƒ†ã‚£ãƒ–

#### 2. Opportunityï¼ˆæ©Ÿä¼šãƒ»æˆé•·ï¼‰
- ç ”ä¿®åˆ¶åº¦ã€ã‚­ãƒ£ãƒªã‚¢ãƒ‘ã‚¹ã€æ˜‡é€²åˆ¶åº¦ã€æµ·å¤–é§åœ¨æ©Ÿä¼šã€è³‡æ ¼å–å¾—æ”¯æ´

#### 3. Organizationï¼ˆçµ„ç¹”ãƒ»ä¼æ¥­æ–‡åŒ–ï¼‰
- ä¼æ¥­ç†å¿µã€ãƒŸãƒƒã‚·ãƒ§ãƒ³ã€ä¼æ¥­æ–‡åŒ–ã€ãƒ–ãƒ©ãƒ³ãƒ‰åŠ›ã€çµ„ç¹”é¢¨åœŸ

#### 4. Peopleï¼ˆäººæãƒ»ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆï¼‰
- ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚¿ã‚¤ãƒ«ã€ãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€å¿ƒç†çš„å®‰å…¨æ€§ã€å¤šæ§˜æ€§ãƒ»ã‚¤ãƒ³ã‚¯ãƒ«ãƒ¼ã‚¸ãƒ§ãƒ³

#### 5. Workï¼ˆåƒãæ–¹ãƒ»æ¥­å‹™ï¼‰
- å‹¤å‹™æ™‚é–“ã€ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯ã€ãƒ¯ãƒ¼ã‚¯ãƒ©ã‚¤ãƒ•ãƒãƒ©ãƒ³ã‚¹ã€æ¥­å‹™ã®å°‚é–€æ€§

### ä¼æ¥­åˆ†æé …ç›®ï¼ˆå„é …ç›®ã¯300-400æ–‡å­—ç¨‹åº¦ã§å…·ä½“çš„ã«è¨˜è¼‰ï¼‰

#### 1. æ¥­ç•Œãƒ»å¸‚å ´åˆ†æ
- ä¼æ¥­ãŒå±ã™ã‚‹ä¸»è¦æ¥­ç•Œã®ç‰¹å¾´ã¨å¸‚å ´è¦æ¨¡ï¼ˆå…·ä½“çš„ãªæ•°å€¤å¿…é ˆï¼‰
- æ¥­ç•Œå…¨ä½“ã®å¹´æˆé•·ç‡ã¨éå»3-5å¹´ã®ãƒˆãƒ¬ãƒ³ãƒ‰
- æ¥­ç•Œã®ä¸»è¦ãªæŠ€è¡“é©æ–°ã¨å¤‰åŒ–ï¼ˆå…·ä½“ä¾‹ã‚’å«ã‚€ï¼‰
- å‘ã“ã†5-10å¹´ã®æ¥­ç•Œå°†æ¥æ€§ã¨æˆé•·è¦‹é€šã—
- æ¥­ç•ŒãŒç›´é¢ã—ã¦ã„ã‚‹ä¸»è¦ãªèª²é¡Œã¨æ–°ãŸãªæ©Ÿä¼š
- **å¿…é ˆ**: æ¥­ç•Œå…¨ä½“ã®ç«¶äº‰çŠ¶æ³ã¨å‚å…¥ä¼æ¥­æ•°ã®æ¦‚æ³

#### 2. æ¥­ç•Œå†…ãƒã‚¸ã‚·ãƒ§ãƒ³ï¼ˆç«¶åˆæ¯”è¼ƒå¿…é ˆï¼‰
- å£²ä¸Šé«˜ã®æ¥­ç•Œå†…é †ä½ã¨å…·ä½“çš„ãªå¸‚å ´ã‚·ã‚§ã‚¢ï¼ˆï¼…è¡¨è¨˜ï¼‰
- ä¸»è¦ç«¶åˆä¼æ¥­ã‚’3-5ç¤¾æ˜è¨˜ã—ã€ãã‚Œã‚‰ã¨ã®å£²ä¸Šãƒ»è¦æ¨¡æ¯”è¼ƒ
- å–¶æ¥­åˆ©ç›Šç‡ãƒ»ROEç­‰ã®åç›Šæ€§æŒ‡æ¨™ã¨æ¥­ç•Œå¹³å‡ã¨ã®æ¯”è¼ƒ
- éå»3-5å¹´ã®å£²ä¸Šæˆé•·ç‡ãƒ»åˆ©ç›Šæˆé•·ç‡ã¨ç«¶åˆä»–ç¤¾æ¯”è¼ƒ
- æ ªä¾¡ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»æ™‚ä¾¡ç·é¡ã®æ¥­ç•Œå†…ä½ç½®ã¥ã‘
- **å¿…é ˆ**: ã€Œâ—‹â—‹ç¤¾ã¨æ¯”è¼ƒã—ã¦ã€ã¨ã„ã†å½¢ã§å…·ä½“çš„ç«¶åˆä¼æ¥­åã‚’æŒ™ã’ã¦åˆ†æ

#### 3. ç‹¬è‡ªæ€§ãƒ»å·®åˆ¥åŒ–è¦å› ï¼ˆå…·ä½“çš„äº‹ä¾‹é‡è¦–ï¼‰
- ç«¶åˆä»–ç¤¾ãŒçœŸä¼¼ã§ããªã„ç‹¬è‡ªæŠ€è¡“ãƒ»ã‚µãƒ¼ãƒ“ã‚¹ãƒ»ãƒ“ã‚¸ãƒã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆå…·ä½“åç§°ï¼‰
- ä¿æœ‰ã™ã‚‹ç‰¹è¨±æ•°ãƒ»çŸ¥çš„è²¡ç”£ãƒ»ç‹¬è‡ªãƒã‚¦ãƒã‚¦ã®æ¦‚è¦
- ãƒ–ãƒ©ãƒ³ãƒ‰èªçŸ¥åº¦ãƒ»é¡§å®¢ãƒ­ã‚¤ãƒ¤ãƒªãƒ†ã‚£ã®æ•°å€¤çš„æŒ‡æ¨™ã‚„èª¿æŸ»çµæœ
- R&DæŠ•è³‡é¡ãƒ»æŠ•è³‡æ¯”ç‡ã¨ç«¶åˆä»–ç¤¾ã¨ã®æ¯”è¼ƒ
- å‚å…¥éšœå£ã¨ãªã‚‹ç‹¬è‡ªè³‡ç”£ï¼ˆè¨­å‚™ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€ãƒ‡ãƒ¼ã‚¿ç­‰ï¼‰
- **å¿…é ˆ**: ä»£è¡¨çš„ãªè£½å“ãƒ»ã‚µãƒ¼ãƒ“ã‚¹åã‚’æ˜è¨˜ã—ã€ãªãœãã‚ŒãŒå·®åˆ¥åŒ–è¦å› ãªã®ã‹ã‚’èª¬æ˜

#### 4. äº‹æ¥­ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªåˆ†æï¼ˆå…·ä½“çš„æ•°å€¤ãƒ»äº‹æ¥­åé‡è¦–ï¼‰
- ä¸»è¦äº‹æ¥­ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã¨å£²ä¸Šãƒ»åˆ©ç›Šæ§‹æˆæ¯”ï¼ˆï¼…è¡¨è¨˜å¿…é ˆã€å…·ä½“çš„ãªäº‹æ¥­åæ˜è¨˜ï¼‰
- å„äº‹æ¥­ã®æˆé•·æ€§ï¼šã€Œä¼¸ã³ã¦ã„ã‚‹åˆ†é‡ã€ã®ç‰¹å®šã¨éå»3-5å¹´ã®æˆé•·ç‡ãƒ‡ãƒ¼ã‚¿
- åç›Šæ€§åˆ†æï¼šã€Œé‡‘ã®ãªã‚‹æœ¨ã€äº‹æ¥­ã®ç‰¹å®šã¨å–¶æ¥­åˆ©ç›Šç‡ãƒ»åç›Šè²¢çŒ®åº¦
- æˆ¦ç•¥çš„é‡ç‚¹é ˜åŸŸï¼šã©ã®åˆ†é‡ã¸ã®æŠ•è³‡æ‹¡å¤§ãƒ»M&Aãƒ»æ–°è¦å‚å…¥ã‚’é€²ã‚ã¦ã„ã‚‹ã‹
- äº‹æ¥­é–“ã‚·ãƒŠã‚¸ãƒ¼åŠ¹æœã¨ç›¸äº’ä½œç”¨ã®å…·ä½“ä¾‹ï¼ˆè£½å“ãƒ»æŠ€è¡“ãƒ»é¡§å®¢åŸºç›¤ã®å…±æœ‰ç­‰ï¼‰
- **å¿…é ˆ**: å„äº‹æ¥­ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®å…·ä½“åç§°ã€ä»£è¡¨çš„è£½å“ãƒ»ã‚µãƒ¼ãƒ“ã‚¹åã‚’æ˜è¨˜

## å›ç­”å½¢å¼
JSONå½¢å¼ã§ä»¥ä¸‹ã®é€šã‚Šå›ç­”ã—ã¦ãã ã•ã„ï¼š

```json
{{
  "evp": {{
    "rewards": "å…·ä½“çš„ãªå ±é…¬ãƒ»å¾…é‡æƒ…å ±",
    "opportunity": "å…·ä½“çš„ãªæˆé•·æ©Ÿä¼šæƒ…å ±",
    "organization": "å…·ä½“çš„ãªçµ„ç¹”ãƒ»ä¼æ¥­æ–‡åŒ–æƒ…å ±",
    "people": "å…·ä½“çš„ãªäººæãƒ»ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆæƒ…å ±",
    "work": "å…·ä½“çš„ãªåƒãæ–¹ãƒ»æ¥­å‹™æƒ…å ±"
  }},
  "business_analysis": {{
    "industry_market": "å…·ä½“çš„ãªæ¥­ç•Œãƒ»å¸‚å ´åˆ†æ",
    "market_position": "å…·ä½“çš„ãªæ¥­ç•Œå†…ãƒã‚¸ã‚·ãƒ§ãƒ³",
    "differentiation": "å…·ä½“çš„ãªç‹¬è‡ªæ€§ãƒ»å·®åˆ¥åŒ–è¦å› ",
    "business_portfolio": "å…·ä½“çš„ãªäº‹æ¥­ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªåˆ†æ"
  }}
}}
```

å„é …ç›®ã¯**300-400æ–‡å­—ã§å…·ä½“çš„ã«**è¨˜è¼‰ã—ã€ä»¥ä¸‹ã‚’å³æ ¼ã«éµå®ˆã—ã¦ãã ã•ã„ï¼š

ã€CRITICAL: ãƒ•ã‚¡ã‚¯ãƒˆãƒ™ãƒ¼ã‚¹åˆ†æãƒ«ãƒ¼ãƒ«ã€‘
1. **ä¸€æ¬¡æƒ…å ±ã®ã¿ä½¿ç”¨**: ä¼æ¥­ã®å…¬å¼é–‹ç¤ºè³‡æ–™ï¼ˆæœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ã€æ±ºç®—çŸ­ä¿¡ã€IRè³‡æ–™ï¼‰ã®å†…å®¹ã®ã¿è¨˜è¼‰
2. **æ¨æ¸¬ãƒ»ä¸€èˆ¬è«–ã®ç¦æ­¢**: LLMã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãæ¨æ¸¬ã¯ä¸€åˆ‡ä½¿ç”¨ç¦æ­¢
3. **æ•°å€¤ã®å‡ºå…¸æ˜è¨˜**: ã€Œ2024å¹´æ±ºç®—çŸ­ä¿¡ã«ã‚ˆã‚‹ã¨ã€ã€Œæœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ã®è¨˜è¼‰ã§ã¯ã€ç­‰ã€å¿…ãšå‡ºå…¸ã‚’æ˜è¨˜
4. **æœªç¢ºèªæƒ…å ±ã®åŒºåˆ¥**: å…¬å¼æƒ…å ±ã§ç¢ºèªã§ããªã„å†…å®¹ã¯ã€Œå…¬å¼é–‹ç¤ºæƒ…å ±ã§ã¯ç¢ºèªã§ãã¾ã›ã‚“ã€ã¨æ˜è¨˜
5. **ç«¶åˆåˆ†æã®åˆ¶é™**: ä¼æ¥­ãŒå…¬å¼ã«è¨€åŠã—ãŸç«¶åˆã®ã¿è¨˜è¼‰ï¼ˆæ¨æ¸¬ã«ã‚ˆã‚‹ç«¶åˆãƒªã‚¹ãƒˆã¯ç¦æ­¢ï¼‰

- **æ•°å€¤ãƒ‡ãƒ¼ã‚¿**: å…¬å¼é–‹ç¤ºè³‡æ–™ã®å…·ä½“çš„æ•°å­—ã®ã¿ï¼ˆæ¨å®šå€¤ãƒ»æ¥­ç•Œå¹³å‡ã¯ä½¿ç”¨ç¦æ­¢ï¼‰
- **ç«¶åˆä¼æ¥­å**: å½“è©²ä¼æ¥­ãŒå…¬å¼è³‡æ–™ã§è¨€åŠã—ãŸç«¶åˆã®ã¿è¨˜è¼‰
- **å›ºæœ‰åè©**: å…¬å¼è³‡æ–™ã«è¨˜è¼‰ã•ã‚ŒãŸè£½å“åã€ã‚µãƒ¼ãƒ“ã‚¹åã®ã¿ä½¿ç”¨
- **æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿**: å…¬å¼é–‹ç¤ºã®éå»ãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨
- **æƒ…å ±æºã®æ˜ç¢ºåŒ–**: å…¨ã¦ã®æƒ…å ±ã«ã€Œâ—‹â—‹å¹´â—‹â—‹è³‡æ–™ã«ã‚ˆã‚‹ã€å½¢å¼ã§å‡ºå…¸æ˜è¨˜

**ä½¿ç”¨ç¦æ­¢è¡¨ç¾**: 
- ã€Œä¸€èˆ¬çš„ã«ã€ã€Œæ¨å®šã§ã¯ã€ã€Œæ¥­ç•Œæ¨™æº–ã€ã€Œé€šå¸¸ã€ã€Œå¤šãã®ä¼æ¥­ã€
- å…·ä½“çš„å‡ºå…¸ã®ãªã„æ•°å€¤ãƒ»ã‚·ã‚§ã‚¢ãƒ»æˆé•·ç‡
- ä¼æ¥­ãŒå…¬å¼ç™ºè¡¨ã—ã¦ã„ãªã„ç«¶åˆä»–ç¤¾å
- LLMã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãæ¨æ¸¬æƒ…å ±

**æƒ…å ±ä¸è¶³æ™‚ã®å¯¾å¿œ**: 
å…¬å¼æƒ…å ±ãŒä¸ååˆ†ãªé …ç›®ã¯ã€Œå…¬å¼é–‹ç¤ºæƒ…å ±ãŒé™å®šçš„ã§è©³ç´°åˆ†æå›°é›£ã€ã¨æ­£ç›´ã«è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚
"""
        return prompt
    
    def create_enhanced_research_prompt(self, company_info, external_data):
        """ä¼æ¥­å…¬å¼æƒ…å ±å„ªå…ˆã®å¼·åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ"""
        
        # å¤–éƒ¨æƒ…å ±ã®æ•´ç†ï¼ˆè£œè¶³æƒ…å ±ã¨ã—ã¦ï¼‰
        external_context = ""
        if external_data:
            external_context = "\nã€è£œè¶³ï¼šå¤–éƒ¨å‚è€ƒæƒ…å ±ã€‘:\n"
            for i, item in enumerate(external_data, 1):
                external_context += f"{i}. ã€{item['source']}ã€‘{item['title']}\n"
                external_context += f"   æ¦‚è¦: {item['snippet']}\n\n"
        
        prompt = f"""
ã‚ãªãŸã¯ä¼æ¥­åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ä¼æ¥­å…¬å¼æƒ…å ±ã‚’æœ€å„ªå…ˆã¨ã—ã€å¤–éƒ¨æƒ…å ±ã¯è£œè¶³ã¨ã—ã¦æ´»ç”¨ã—ã¦æ­£ç¢ºãªåˆ†æã‚’è¡Œã£ã¦ãã ã•ã„ã€‚

ã€åˆ†æå¯¾è±¡ä¼æ¥­ã€‘
ä¼æ¥­å: {company_info['company_name']}
åˆ†æé‡ç‚¹åˆ†é‡: {company_info['focus_area']}
ä¼æ¥­ãƒ‰ãƒ¡ã‚¤ãƒ³: {company_info.get('company_domain', 'ä¸æ˜')}

{external_context}

ã€CRITICALåˆ†æãƒ«ãƒ¼ãƒ« - å³æ ¼ã«éµå®ˆã€‘
1. **æƒ…å ±å„ªå…ˆé †ä½**:
   - ç¬¬1å„ªå…ˆ: ä¼æ¥­å…¬å¼ã‚µã‚¤ãƒˆãƒ»IRè³‡æ–™ã®æƒ…å ±
   - ç¬¬2å„ªå…ˆ: ä¼æ¥­é–‹ç¤ºã®å®šé‡ãƒ‡ãƒ¼ã‚¿ï¼ˆå£²ä¸Šã€åˆ©ç›Šã€å¸‚å ´ã‚·ã‚§ã‚¢ç­‰ï¼‰
   - ç¬¬3å„ªå…ˆ: å¤–éƒ¨è¨˜äº‹ã¯ä¼æ¥­æƒ…å ±ã®è£œè¶³ãƒ»æ¤œè¨¼ã¨ã—ã¦æ´»ç”¨
   
2. **å®šé‡åˆ†æã®å¾¹åº•**:
   - å£²ä¸Šé«˜ã€å–¶æ¥­åˆ©ç›Šã€å¾“æ¥­å“¡æ•°ç­‰ã®å…·ä½“çš„æ•°å€¤ã‚’é‡è¦–
   - å¸‚å ´è¦æ¨¡ãƒ»ã‚·ã‚§ã‚¢ã¯å…¬å¼é–‹ç¤ºãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯å¤–éƒ¨èª¿æŸ»ãƒ‡ãƒ¼ã‚¿ã§æ˜è¨˜
   - æˆé•·ç‡ã€ROEç­‰ã®è²¡å‹™æŒ‡æ¨™ã‚’å¯èƒ½ãªé™ã‚Šå«ã‚ã‚‹
   
3. **ç«¶åˆåˆ†æã®ç²¾åº¦**:
   - ä¼æ¥­ãŒå…¬å¼ã«è¨€åŠã™ã‚‹ç«¶åˆã‚’æœ€å„ªå…ˆ
   - åŒä¸€äº‹æ¥­ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ä¼æ¥­ã‚’æ­£ç¢ºã«ç‰¹å®š
   - å¤–éƒ¨è¨˜äº‹ã®ç«¶åˆæƒ…å ±ã¯ã€Œå‚è€ƒæƒ…å ±ã¨ã—ã¦ã€ã§åŒºåˆ¥
   
4. **äº‹æ¥­é ˜åŸŸã®æ­£ç¢ºãªå®šç¾©**:
   - ä¼æ¥­ã®ä¸»åŠ›äº‹æ¥­ã‚’å…¬å¼æƒ…å ±ã‹ã‚‰æ­£ç¢ºã«æŠŠæ¡
   - ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥å£²ä¸Šæ§‹æˆæ¯”ç­‰ã®å®šé‡ãƒ‡ãƒ¼ã‚¿ã‚’æ´»ç”¨
   - é¡ä¼¼æ¥­ç•Œã¨ã®æ··åŒã‚’é¿ã‘ã‚‹
   
5. **ä¿¡é ¼æ€§ã®æ‹…ä¿**:
   - ä¸æ˜ãªæƒ…å ±ã¯ã€Œç¢ºèªã§ãã¾ã›ã‚“ã€ã¨æ˜è¨˜
   - æ¨å®šã¯æ ¹æ‹ ã‚’ç¤ºã—ã€Œæ¨å®šã€ã¨æ˜è¨˜
   - å¤–éƒ¨æƒ…å ±ã¯å‡ºå…¸ã‚’æ˜è¨˜ã—ã€ä¼æ¥­æƒ…å ±ã¨åŒºåˆ¥

JSONå½¢å¼ã§ä»¥ä¸‹ã®é€šã‚Šå›ç­”ã—ã¦ãã ã•ã„ï¼š

```json
{{
  "evp": {{
    "rewards": {{
      "factual_data": "ä¼æ¥­å…¬å¼ã«é–‹ç¤ºã•ã‚ŒãŸå ±é…¬ãƒ»å¾…é‡ã®å…·ä½“çš„æƒ…å ±ï¼ˆå¹´åã€è³ä¸ã€ç¦åˆ©åšç”Ÿåˆ¶åº¦åã€æ•°å€¤ãƒ‡ãƒ¼ã‚¿ç­‰ï¼‰",
      "analytical_insights": "é–‹ç¤ºãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãåˆ†æãƒ»æ¨å®šï¼ˆæ¥­ç•Œæ¯”è¼ƒã€æˆé•·æ€§è©•ä¾¡ç­‰ï¼‰",
      "data_limitations": "æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹é …ç›®ãƒ»æ¨å®šãŒå¿…è¦ãªç®‡æ‰€"
    }},
    "opportunity": {{
      "factual_data": "ä¼æ¥­å…¬å¼ã«æ˜ç¤ºã•ã‚ŒãŸã‚­ãƒ£ãƒªã‚¢ãƒ‘ã‚¹ãƒ»æˆé•·æ©Ÿä¼šï¼ˆåˆ¶åº¦åã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ å†…å®¹ã€å®Ÿç¸¾æ•°å€¤ç­‰ï¼‰",
      "analytical_insights": "åˆ¶åº¦ãƒ»å®Ÿç¸¾ã‹ã‚‰èª­ã¿å–ã‚Œã‚‹æˆé•·å¯èƒ½æ€§ã®åˆ†æ",
      "data_limitations": "è©³ç´°ãŒä¸æ˜ãªåˆ¶åº¦ãƒ»æ¨å®šè¦ç´ "
    }},
    "organization": {{
      "factual_data": "ä¼æ¥­æ–‡åŒ–ãƒ»çµ„ç¹”ã«é–¢ã™ã‚‹å…¬å¼æƒ…å ±ï¼ˆå¾“æ¥­å“¡æ•°ã€çµ„ç¹”æ§‹é€ ã€ä¼æ¥­ç†å¿µã€åˆ¶åº¦ç­‰ã®å…·ä½“çš„å†…å®¹ï¼‰",
      "analytical_insights": "çµ„ç¹”ç‰¹æ€§ã‹ã‚‰æ¨æ¸¬ã•ã‚Œã‚‹è·å ´ç’°å¢ƒãƒ»ä¼æ¥­æ–‡åŒ–ã®åˆ†æ",
      "data_limitations": "å®šæ€§çš„æƒ…å ±ã‚„æ¨æ¸¬ãŒå¿…è¦ãªç®‡æ‰€"
    }},
    "people": {{
      "factual_data": "äººæè‚²æˆãƒ»ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆã®å…¬å¼åˆ¶åº¦ï¼ˆç ”ä¿®åˆ¶åº¦åã€è©•ä¾¡åˆ¶åº¦ã€å®Ÿç¸¾æ•°å€¤ç­‰ï¼‰",
      "analytical_insights": "åˆ¶åº¦ã‹ã‚‰æ¨æ¸¬ã•ã‚Œã‚‹äººææˆ¦ç•¥ãƒ»æˆé•·ç’°å¢ƒã®åˆ†æ",
      "data_limitations": "åˆ¶åº¦è©³ç´°ã‚„åŠ¹æœãŒä¸æ˜ãªç®‡æ‰€"
    }},
    "work": {{
      "factual_data": "åƒãæ–¹ã«é–¢ã™ã‚‹å…¬å¼æƒ…å ±ï¼ˆå‹¤å‹™åˆ¶åº¦ã€æ¥­å‹™å†…å®¹ã€åƒãæ–¹æ”¹é©ã®å…·ä½“çš„å–çµ„ç­‰ï¼‰",
      "analytical_insights": "åˆ¶åº¦ãƒ»å–çµ„ã‹ã‚‰æ¨æ¸¬ã•ã‚Œã‚‹ãƒ¯ãƒ¼ã‚¯ãƒ©ã‚¤ãƒ•ãƒãƒ©ãƒ³ã‚¹ãƒ»æ¥­å‹™ç‰¹æ€§",
      "data_limitations": "å®Ÿæ…‹ã‚„è©³ç´°ãŒä¸æ˜ãªåˆ¶åº¦ãƒ»æ¨å®šè¦ç´ "
    }}
  }},
  "business_analysis": {{
    "industry_market": {{
      "factual_data": "ä¼æ¥­é–‹ç¤ºãƒ»å¤–éƒ¨èª¿æŸ»ã«ã‚ˆã‚‹å¸‚å ´è¦æ¨¡ãƒ»æ¥­ç•Œå‹•å‘ã®å…·ä½“çš„æ•°å€¤ãƒ»ãƒ‡ãƒ¼ã‚¿",
      "analytical_insights": "ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãå¸‚å ´ç’°å¢ƒãƒ»æˆé•·æ€§ã®åˆ†æãƒ»äºˆæ¸¬",
      "data_limitations": "ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹é ˜åŸŸãƒ»æ¨å®šãŒå¿…è¦ãªç®‡æ‰€"
    }},
    "market_position": {{
      "factual_data": "ä¼æ¥­é–‹ç¤ºãƒ»å¤–éƒ¨èª¿æŸ»ã«ã‚ˆã‚‹å¸‚å ´ã‚·ã‚§ã‚¢ãƒ»å£²ä¸Šè¦æ¨¡ç­‰ã®å…·ä½“çš„é †ä½ãƒ»æ•°å€¤",
      "analytical_insights": "æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãç«¶äº‰åŠ›ãƒ»ãƒã‚¸ã‚·ãƒ§ãƒ³åˆ†æ",
      "data_limitations": "æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹ç«¶åˆãƒ»æ¨å®šè¦ç´ "
    }},
    "differentiation": {{
      "factual_data": "ä¼æ¥­å…¬å¼ã«æ˜ç¤ºã•ã‚ŒãŸå·®åˆ¥åŒ–è¦å› ãƒ»ç«¶äº‰å„ªä½æ€§ï¼ˆæŠ€è¡“ã€ã‚µãƒ¼ãƒ“ã‚¹ã€å®Ÿç¸¾ç­‰ï¼‰",
      "analytical_insights": "å…¬å¼æƒ…å ±ã‹ã‚‰æ¨æ¸¬ã•ã‚Œã‚‹æŒç¶šçš„ç«¶äº‰å„ªä½æ€§ã®åˆ†æ",
      "data_limitations": "ç«¶äº‰å„ªä½æ€§ã®æŒç¶šæ€§ãƒ»åŠ¹æœãŒä¸æ˜ãªç®‡æ‰€"
    }},
    "business_portfolio": {{
      "factual_data": "ä¼æ¥­é–‹ç¤ºã«ã‚ˆã‚‹ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥å£²ä¸Šãƒ»åˆ©ç›Šãƒ»æˆé•·ç‡ç­‰ã®å…·ä½“çš„æ•°å€¤",
      "analytical_insights": "æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãäº‹æ¥­ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªãƒ»åç›Šæ§‹é€ ã®åˆ†æ",
      "data_limitations": "è©³ç´°ãªåç›Šæ§‹é€ ã‚„å°†æ¥æ€§ãŒä¸æ˜ãªäº‹æ¥­é ˜åŸŸ"
    }}
  }}
}}
```

å„é …ç›®ã¯ä»¥ä¸‹ã®æ§‹é€ ã§**600-800æ–‡å­—**ã§å…·ä½“çš„ã«è¨˜è¼‰ã—ã¦ãã ã•ã„ï¼š

**ğŸ“Š factual_dataï¼ˆ300-400æ–‡å­—ï¼‰:**
- ä¼æ¥­å…¬å¼é–‹ç¤ºãƒ»å¤–éƒ¨èª¿æŸ»ã®å…·ä½“çš„æ•°å€¤ãƒ»åˆ¶åº¦åãƒ»å®Ÿç¸¾
- å‡ºå…¸ã‚’æ˜è¨˜ï¼ˆã€Œ2024å¹´3æœˆæœŸæ±ºç®—çŸ­ä¿¡ã«ã‚ˆã‚‹ã¨ã€ã€Œæœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ã«ã‚ˆã‚‹ã¨ã€ç­‰ï¼‰
- æ¨æ¸¬ã‚’å«ã¾ãªã„å®¢è¦³çš„äº‹å®Ÿã®ã¿

**ğŸ” analytical_insightsï¼ˆ200-300æ–‡å­—ï¼‰:**
- factual_dataã«åŸºã¥ãåˆ†æãƒ»è©•ä¾¡ãƒ»æ¨æ¸¬
- ã€Œã“ã‚Œã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¨æ¸¬ã™ã‚‹ã¨ã€ã€Œæ¥­ç•Œæ°´æº–ã¨æ¯”è¼ƒã—ã¦ã€ç­‰ã§æ¨æ¸¬ã§ã‚ã‚‹ã“ã¨ã‚’æ˜ç¤º
- æ ¹æ‹ ã¨ãªã‚‹factual_dataã¨ã®é–¢é€£ã‚’æ˜ç¢ºã«ç¤ºã™

**âš ï¸ data_limitationsï¼ˆ100æ–‡å­—ç¨‹åº¦ï¼‰:**
- æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹ç®‡æ‰€ãƒ»æ¨æ¸¬ãŒå¿…è¦ãªç†ç”±ã‚’ç°¡æ½”ã«æ˜è¨˜
- ã€Œè©³ç´°ãªåˆ¶åº¦å†…å®¹ã¯éé–‹ç¤ºã€ã€Œæ¥­ç•Œæ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ãªã—ã€ç­‰

ã€è¨˜è¼‰æ–¹é‡ã€‘
- ä¼æ¥­å…¬å¼æƒ…å ±ã‚’70%ä»¥ä¸Šã€å¤–éƒ¨æƒ…å ±ã¯30%ä»¥ä¸‹ã®æ¯”é‡
- å®šé‡ãƒ‡ãƒ¼ã‚¿ï¼ˆæ•°å€¤ï¼‰ã‚’å¯èƒ½ãªé™ã‚Šå«ã‚ã‚‹
- æƒ…å ±æºã‚’æ˜ç¢ºã«åŒºåˆ¥ï¼šã€Œä¼æ¥­å…¬å¼ã«ã‚ˆã‚‹ã¨ã€ã€Œâ—‹â—‹å¹´åº¦æ±ºç®—è³‡æ–™ã«ã‚ˆã‚‹ã¨ã€ã€Œå‚è€ƒã¨ã—ã¦å¤–éƒ¨èª¿æŸ»ã§ã¯ã€
- æ¨æ¸¬ãƒ»ä¸€èˆ¬è«–ã¯æœ€å°é™ã«æŠ‘åˆ¶

**è¨˜è¼‰å¿…é ˆè¦ç´ **: å…·ä½“çš„æ•°å€¤ã€å…¬å¼åˆ¶åº¦åã€æ­£ç¢ºãªäº‹æ¥­ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåã€é–‹ç¤ºã•ã‚ŒãŸç«¶åˆä¼æ¥­å
**ä½¿ç”¨ç¦æ­¢**: æ ¹æ‹ ãªãæ¨æ¸¬ã€å‡ºå…¸ä¸æ˜ã®æ•°å€¤ã€ä¸æ­£ç¢ºãªç«¶åˆä¼æ¥­å
"""
        return prompt
    
    def create_revolutionary_analysis_prompt(self, company_fundamentals, structured_ir, hierarchical_data, external_data):
        """é©æ–°çš„ãªæ®µéšçš„åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
        
        # ä¼æ¥­åŸºæœ¬æƒ…å ±ã®çµ±åˆ
        company_context = f"""
ã€ç¢ºç«‹æ¸ˆã¿ä¼æ¥­åŸºæœ¬æƒ…å ±ã€‘
ä¼æ¥­å: {company_fundamentals['company_name']}
ä¸»åŠ›äº‹æ¥­: {company_fundamentals['primary_business']} (ä¿¡é ¼åº¦: {company_fundamentals['confidence_score']}%)
æ¥­ç•Œåˆ†é¡: {company_fundamentals['industry_classification']}
äº‹æ¥­ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ: {', '.join(company_fundamentals['business_segments'])}
ç¢ºèªæ¸ˆã¿ç«¶åˆ: {', '.join(company_fundamentals['competitors'])}
"""
        
        # IRæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ
        ir_context = ""
        if structured_ir['financial_data']['revenue']['value']:
            ir_context += f"""
ã€IRé–‹ç¤ºè²¡å‹™æƒ…å ±ã€‘
å£²ä¸Šé«˜: {structured_ir['financial_data']['revenue']['value']} ({structured_ir['financial_data']['revenue']['source']}, {structured_ir['financial_data']['revenue']['year']})
å–¶æ¥­åˆ©ç›Š: {structured_ir['financial_data']['operating_profit']['value']} ({structured_ir['financial_data']['operating_profit']['source']}, {structured_ir['financial_data']['operating_profit']['year']})
å¾“æ¥­å“¡æ•°: {structured_ir['financial_data']['employees']['value']} ({structured_ir['financial_data']['employees']['source']}, {structured_ir['financial_data']['employees']['year']})
"""
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ªæƒ…å ±
        quality_context = f"""
ã€ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡ã€‘
IRæ–‡æ›¸ç™ºè¦‹æ•°: {structured_ir['data_quality']['ir_documents_found']}ä»¶
ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§: {structured_ir['data_quality']['data_completeness']:.1f}%
ä¿¡é ¼æ€§ã‚¹ã‚³ã‚¢: {structured_ir['data_quality']['reliability_score']:.1f}%
å…¨ä½“ä¿¡é ¼æ€§: {hierarchical_data['quality_assessment']['overall_reliability']:.1f}%
"""
        
        # å¤–éƒ¨æƒ…å ±ã®å‚è€ƒãƒ‡ãƒ¼ã‚¿
        external_context = ""
        if external_data:
            external_context = "\nã€å‚è€ƒï¼šå¤–éƒ¨æƒ…å ±ã€‘\n"
            for i, item in enumerate(external_data[:3], 1):
                external_context += f"{i}. ã€{item['source']}ã€‘{item['title']}\n"
                external_context += f"   æ¦‚è¦: {item['snippet'][:100]}...\n\n"
        
        prompt = f"""
ã‚ãªãŸã¯ä¼æ¥­åˆ†æã®æœ€é«˜å³°å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®æ®µéšçš„åˆ†æãƒ—ãƒ­ã‚»ã‚¹ã«å¾“ã„ã€äº‹å®Ÿã¨æ¨æ¸¬ã‚’å³æ ¼ã«åŒºåˆ¥ã—ãŸè¶…é«˜ç²¾åº¦åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

{company_context}
{ir_context}
{quality_context}
{external_context}

ã€REVOLUTIONARY åˆ†æãƒ—ãƒ­ã‚»ã‚¹ - 4æ®µéšã®å³æ ¼ãªæ¤œè¨¼ã€‘

ğŸ” STAGE 1: äº‹å®Ÿç¢ºèªãƒ•ã‚§ãƒ¼ã‚º
- IRé–‹ç¤ºæƒ…å ±ã®æ•°å€¤ã¯çµ¶å¯¾çš„äº‹å®Ÿã¨ã—ã¦æ‰±ã†
- ä¼æ¥­å…¬å¼æƒ…å ±ã®åˆ¶åº¦ãƒ»å–çµ„ã¯äº‹å®Ÿã¨ã—ã¦æ‰±ã†
- å¤–éƒ¨æƒ…å ±ã¯ã€Œå‚è€ƒæƒ…å ±ã€ã¨ã—ã¦æ˜è¨˜ã—ã€æ¨æ¸¬ã®ææ–™ã¨ã™ã‚‹
- ä¸æ˜ãªæƒ…å ±ã¯ã€ŒIRé–‹ç¤ºã§ã¯ç¢ºèªã§ããšã€ã¨æ˜è¨˜

ğŸ§® STAGE 2: æ•°å€¤å¦¥å½“æ€§æ¤œè¨¼ãƒ•ã‚§ãƒ¼ã‚º
- æ¥­ç•Œå¸¸è­˜ã¨ã®ç…§åˆï¼ˆäººæã‚µãƒ¼ãƒ“ã‚¹æ¥­ç•Œã®å£²ä¸Šè¦æ¨¡ç­‰ï¼‰
- ç«¶åˆä»–ç¤¾ã¨ã®è¦æ¨¡æ¯”è¼ƒã®å¦¥å½“æ€§ç¢ºèª
- ç•°å¸¸å€¤ã®æ¤œå‡ºã¨ç†ç”±èª¬æ˜

ğŸ† STAGE 3: ç«¶åˆãƒ»æ¥­ç•Œæ•´åˆæ€§ãƒ•ã‚§ãƒ¼ã‚º
- ç¢ºç«‹æ¸ˆã¿ç«¶åˆä¼æ¥­ãƒªã‚¹ãƒˆã¨ã®æ•´åˆæ€§ç¢ºèª
- æ¥­ç•Œåˆ†é¡ã€Œ{company_fundamentals['industry_classification']}ã€ã«åŸºã¥ãåˆ†æ
- ä»–æ¥­ç•Œä¼æ¥­ã®èª¤èªè­˜é˜²æ­¢

âš ï¸ STAGE 4: ã‚¨ãƒ©ãƒ¼é˜²æ­¢æœ€çµ‚ãƒã‚§ãƒƒã‚¯ãƒ•ã‚§ãƒ¼ã‚º
- ä½å®…æ¥­ç•Œã¨ã®æ··åŒãƒã‚§ãƒƒã‚¯
- æ ¹æ‹ ãªãå…·ä½“çš„æ•°å€¤ã®æ’é™¤
- æ¨æ¸¬ã®æ˜ç¤ºã¨æ ¹æ‹ èª¬æ˜

ã€CRITICAL å‡ºå…¸æ˜è¨˜ãƒ«ãƒ¼ãƒ«ã€‘
- ã€Œ2024å¹´3æœˆæœŸæ±ºç®—çŸ­ä¿¡ã«ã‚ˆã‚‹ã¨ã€
- ã€Œ2023å¹´æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ã«ã‚ˆã‚‹ã¨ã€
- ã€ŒIRé–‹ç¤ºæƒ…å ±ã§ã¯ç¢ºèªã§ããªã„ãŸã‚æ¨æ¸¬ã™ã‚‹ã¨ã€
- ã€Œæ—¥æœ¬çµŒæ¸ˆæ–°èã®å ±é“ã«ã‚ˆã‚‹ã¨ï¼ˆå‚è€ƒæƒ…å ±ï¼‰ã€

ã€ç¦æ­¢äº‹é …ã€‘
âŒ æ ¹æ‹ ãªãå…·ä½“çš„æ•°å€¤ï¼ˆã€Œå¸‚å ´è¦æ¨¡10å…†å††ã€ç­‰ï¼‰
âŒ æ¥­ç•Œèª¤èªè­˜ï¼ˆäººæã‚µãƒ¼ãƒ“ã‚¹ä¼æ¥­ã‚’ä½å®…ä¼æ¥­ã¨ã—ã¦åˆ†æï¼‰
âŒ ä¸æ­£ç¢ºãªç«¶åˆä¼æ¥­åï¼ˆæ¥­ç•Œå¤–ä¼æ¥­ã®åˆ—æŒ™ï¼‰
âŒ å‡ºå…¸ä¸æ˜ã®åˆ¶åº¦åãƒ»å–çµ„å

JSONå½¢å¼ã§ä»¥ä¸‹ã®é€šã‚Šå›ç­”ã—ã¦ãã ã•ã„ï¼š

```json
{{
  "evp": {{
    "rewards": {{
      "factual_data": "IRé–‹ç¤ºãƒ»ä¼æ¥­å…¬å¼ã®å ±é…¬å¾…é‡æƒ…å ±ï¼ˆå…·ä½“çš„æ•°å€¤ãƒ»åˆ¶åº¦åãƒ»å‡ºå…¸æ˜è¨˜ï¼‰",
      "analytical_insights": "é–‹ç¤ºãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãæ¥­ç•Œæ¯”è¼ƒãƒ»æˆé•·æ€§åˆ†æï¼ˆæ¨æ¸¬ç®‡æ‰€ã¯ã€Œæ¨æ¸¬ã€æ˜è¨˜ï¼‰",
      "data_limitations": "IRé–‹ç¤ºã§ç¢ºèªã§ããªã„é …ç›®ã®æ˜ç¢ºãªæŒ‡æ‘˜"
    }},
    "opportunity": {{
      "factual_data": "IRé–‹ç¤ºãƒ»ä¼æ¥­å…¬å¼ã®ã‚­ãƒ£ãƒªã‚¢åˆ¶åº¦ï¼ˆåˆ¶åº¦åãƒ»å®Ÿç¸¾æ•°å€¤ãƒ»å‡ºå…¸æ˜è¨˜ï¼‰",
      "analytical_insights": "åˆ¶åº¦ã‹ã‚‰æ¨æ¸¬ã•ã‚Œã‚‹æˆé•·ç’°å¢ƒï¼ˆæ¨æ¸¬æ ¹æ‹ ã‚’æ˜ç¤ºï¼‰",
      "data_limitations": "è©³ç´°ãŒä¸æ˜ãªåˆ¶åº¦ãƒ»æ¨æ¸¬ãŒå¿…è¦ãªç®‡æ‰€"
    }},
    "organization": {{
      "factual_data": "IRé–‹ç¤ºãƒ»ä¼æ¥­å…¬å¼ã®çµ„ç¹”æƒ…å ±ï¼ˆå¾“æ¥­å“¡æ•°ãƒ»çµ„ç¹”æ§‹é€ ãƒ»ç†å¿µãƒ»å‡ºå…¸æ˜è¨˜ï¼‰",
      "analytical_insights": "çµ„ç¹”ç‰¹æ€§ã‹ã‚‰æ¨æ¸¬ã•ã‚Œã‚‹è·å ´ç’°å¢ƒï¼ˆæ¨æ¸¬æ ¹æ‹ ã‚’æ˜ç¤ºï¼‰",
      "data_limitations": "å®šæ€§çš„æƒ…å ±ã§æ¨æ¸¬ãŒå¿…è¦ãªç®‡æ‰€"
    }},
    "people": {{
      "factual_data": "IRé–‹ç¤ºãƒ»ä¼æ¥­å…¬å¼ã®äººæåˆ¶åº¦ï¼ˆç ”ä¿®åˆ¶åº¦åãƒ»è©•ä¾¡åˆ¶åº¦ãƒ»å®Ÿç¸¾ãƒ»å‡ºå…¸æ˜è¨˜ï¼‰",
      "analytical_insights": "åˆ¶åº¦ã‹ã‚‰æ¨æ¸¬ã•ã‚Œã‚‹äººææˆ¦ç•¥ï¼ˆæ¨æ¸¬æ ¹æ‹ ã‚’æ˜ç¤ºï¼‰",
      "data_limitations": "åˆ¶åº¦è©³ç´°ã‚„åŠ¹æœãŒä¸æ˜ãªç®‡æ‰€"
    }},
    "work": {{
      "factual_data": "IRé–‹ç¤ºãƒ»ä¼æ¥­å…¬å¼ã®åƒãæ–¹æƒ…å ±ï¼ˆå‹¤å‹™åˆ¶åº¦ãƒ»åƒãæ–¹æ”¹é©å–çµ„ãƒ»å‡ºå…¸æ˜è¨˜ï¼‰",
      "analytical_insights": "åˆ¶åº¦ã‹ã‚‰æ¨æ¸¬ã•ã‚Œã‚‹æ¥­å‹™ç‰¹æ€§ï¼ˆæ¨æ¸¬æ ¹æ‹ ã‚’æ˜ç¤ºï¼‰",
      "data_limitations": "å®Ÿæ…‹ã‚„è©³ç´°ãŒä¸æ˜ãªåˆ¶åº¦"
    }}
  }},
  "business_analysis": {{
    "industry_market": {{
      "factual_data": "IRé–‹ç¤ºãƒ»å¤–éƒ¨èª¿æŸ»ã®å¸‚å ´ãƒ‡ãƒ¼ã‚¿ï¼ˆ{company_fundamentals['industry_classification']}æ¥­ç•Œã®å…·ä½“çš„æ•°å€¤ãƒ»å‡ºå…¸æ˜è¨˜ï¼‰",
      "analytical_insights": "ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãå¸‚å ´ç’°å¢ƒåˆ†æï¼ˆæ¨æ¸¬ç®‡æ‰€ã¯æ ¹æ‹ æ˜ç¤ºï¼‰",
      "data_limitations": "ãƒ‡ãƒ¼ã‚¿ä¸è¶³é ˜åŸŸã®æ˜ç¢ºãªæŒ‡æ‘˜"
    }},
    "market_position": {{
      "factual_data": "IRé–‹ç¤ºãƒ»å¤–éƒ¨èª¿æŸ»ã®ãƒã‚¸ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚·ã‚§ã‚¢ãƒ»å£²ä¸Šé †ä½ãƒ»å‡ºå…¸æ˜è¨˜ï¼‰",
      "analytical_insights": "ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãç«¶äº‰åŠ›åˆ†æï¼ˆç¢ºç«‹æ¸ˆã¿ç«¶åˆ: {', '.join(company_fundamentals['competitors'])}ã¨ã®æ¯”è¼ƒï¼‰",
      "data_limitations": "æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹ç«¶åˆãƒ»æ¨æ¸¬è¦ç´ "
    }},
    "differentiation": {{
      "factual_data": "IRé–‹ç¤ºãƒ»ä¼æ¥­å…¬å¼ã®å·®åˆ¥åŒ–è¦å› ï¼ˆæŠ€è¡“ãƒ»ã‚µãƒ¼ãƒ“ã‚¹ãƒ»å®Ÿç¸¾ãƒ»å‡ºå…¸æ˜è¨˜ï¼‰",
      "analytical_insights": "å…¬å¼æƒ…å ±ã‹ã‚‰æ¨æ¸¬ã•ã‚Œã‚‹ç«¶äº‰å„ªä½æ€§ï¼ˆæ¨æ¸¬æ ¹æ‹ ã‚’æ˜ç¤ºï¼‰",
      "data_limitations": "å„ªä½æ€§ã®æŒç¶šæ€§ãƒ»åŠ¹æœãŒä¸æ˜ãªç®‡æ‰€"
    }},
    "business_portfolio": {{
      "factual_data": "IRé–‹ç¤ºã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥æ•°å€¤ï¼ˆå£²ä¸Šãƒ»åˆ©ç›Šãƒ»æˆé•·ç‡ãƒ»å‡ºå…¸æ˜è¨˜ï¼‰",
      "analytical_insights": "æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãäº‹æ¥­ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªåˆ†æï¼ˆæ¨æ¸¬æ ¹æ‹ ã‚’æ˜ç¤ºï¼‰",
      "data_limitations": "è©³ç´°ãªåç›Šæ§‹é€ ãŒä¸æ˜ãªäº‹æ¥­é ˜åŸŸ"
    }}
  }}
}}
```

å„factual_dataã¯400æ–‡å­—ã€analytical_insightsã¯300æ–‡å­—ã€data_limitationsã¯100æ–‡å­—ã§è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚
"""
        return prompt
    
    def create_ir_integrated_prompt(self, company_info, ir_data, external_data):
        """IRæƒ…å ±çµ±åˆå‹ã®é«˜ç²¾åº¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ"""
        
        # IRæƒ…å ±ã®æ•´ç†
        ir_context = ""
        if ir_data:
            ir_context = "\nã€é‡è¦ï¼šIRé–‹ç¤ºæƒ…å ±ã€‘:\n"
            for i, item in enumerate(ir_data, 1):
                ir_context += f"{i}. ã€{item.get('type', 'IRè³‡æ–™')}ã€‘{item['title']}\n"
                ir_context += f"   æ—¥ä»˜: {item.get('date', 'ä¸æ˜')}\n"
                ir_context += f"   å†…å®¹æŠœç²‹: {item.get('content', '')[:300]}...\n\n"
        
        # å¤–éƒ¨æƒ…å ±ã®æ•´ç†ï¼ˆè£œè¶³æƒ…å ±ã¨ã—ã¦ï¼‰
        external_context = ""
        if external_data:
            external_context = "\nã€è£œè¶³ï¼šå¤–éƒ¨å‚è€ƒæƒ…å ±ã€‘:\n"
            for i, item in enumerate(external_data, 1):
                external_context += f"{i}. ã€{item['source']}ã€‘{item['title']}\n"
                external_context += f"   æ¦‚è¦: {item['snippet']}\n\n"
        
        prompt = f"""
ã‚ãªãŸã¯ä¼æ¥­åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚IRé–‹ç¤ºæƒ…å ±ã‚’æœ€å„ªå…ˆã¨ã—ã€äº‹å®Ÿã¨æ¨æ¸¬ã‚’æ˜ç¢ºã«åŒºåˆ¥ã—ãŸæ§‹é€ åŒ–åˆ†æã‚’è¡Œã£ã¦ãã ã•ã„ã€‚

ã€åˆ†æå¯¾è±¡ä¼æ¥­ã€‘
ä¼æ¥­å: {company_info['company_name']}
åˆ†æé‡ç‚¹åˆ†é‡: {company_info['focus_area']}
ä¼æ¥­ãƒ‰ãƒ¡ã‚¤ãƒ³: {company_info.get('company_domain', 'ä¸æ˜')}

{ir_context}
{external_context}

ã€CRITICALåˆ†æãƒ«ãƒ¼ãƒ« - å³æ ¼ã«éµå®ˆã€‘

1. **æƒ…å ±å„ªå…ˆé †ä½ï¼ˆçµ¶å¯¾éµå®ˆï¼‰**:
   - ç¬¬1å„ªå…ˆ: IRé–‹ç¤ºæƒ…å ±ï¼ˆæ±ºç®—çŸ­ä¿¡ã€æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ã€ä¸­æœŸçµŒå–¶è¨ˆç”»ç­‰ï¼‰
   - ç¬¬2å„ªå…ˆ: ä¼æ¥­å…¬å¼ã‚µã‚¤ãƒˆæƒ…å ±
   - ç¬¬3å„ªå…ˆ: å¤–éƒ¨è¨˜äº‹ã¯è£œè¶³ãƒ»æ¤œè¨¼ã¨ã—ã¦æœ€å°é™æ´»ç”¨

2. **äº‹å®Ÿã¨æ¨æ¸¬ã®å³æ ¼ãªåŒºåˆ¥**:
   - factual_data: IRé–‹ç¤ºãƒ»ä¼æ¥­å…¬å¼ã®å…·ä½“çš„æ•°å€¤ãƒ»åˆ¶åº¦ãƒ»å®Ÿç¸¾ã®ã¿
   - analytical_insights: ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãåˆ†æãƒ»æ¨æ¸¬ï¼ˆæ ¹æ‹ ã‚’æ˜ç¤ºï¼‰
   - data_limitations: æƒ…å ±ä¸è¶³ç®‡æ‰€ã®æ˜ç¢ºãªæŒ‡æ‘˜

3. **å¿…é ˆè¨˜è¼‰è¦ç´ **:
   - å£²ä¸Šé«˜ã€å–¶æ¥­åˆ©ç›Šã€å¾“æ¥­å“¡æ•°ç­‰ã®å…·ä½“çš„æ•°å€¤ï¼ˆæœ€æ–°æœŸï¼‹éå»2-3å¹´æ¨ç§»ï¼‰
   - ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥æ¥­ç¸¾ï¼ˆå£²ä¸Šæ§‹æˆæ¯”ã€åˆ©ç›Šç‡ç­‰ï¼‰
   - ç«¶åˆä»–ç¤¾ã¨ã®å®šé‡æ¯”è¼ƒï¼ˆã‚·ã‚§ã‚¢ã€è¦æ¨¡ç­‰ï¼‰
   - å…¬å¼ã«é–‹ç¤ºã•ã‚ŒãŸåˆ¶åº¦ãƒ»å–çµ„ã®å…·ä½“å

4. **å“è³ªæ‹…ä¿ãƒ«ãƒ¼ãƒ«**:
   - å‡ºå…¸ã‚’å¿…ãšæ˜è¨˜ï¼šã€Œ2024å¹´3æœˆæœŸæ±ºç®—çŸ­ä¿¡ã€ã€Œ2023å¹´æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ã€ç­‰
   - æ¨æ¸¬ã¯æ ¹æ‹ ã‚’ç¤ºã—ã€Œï½ã‹ã‚‰æ¨æ¸¬ã•ã‚Œã‚‹ã€ã€Œï½ã¨è€ƒãˆã‚‰ã‚Œã‚‹ã€ã§æ˜ç¤º
   - ä¸æ˜ãªæƒ…å ±ã¯ã€Œé–‹ç¤ºæƒ…å ±ã§ã¯ç¢ºèªã§ããšã€ã¨æ­£ç›´ã«è¨˜è¼‰

5. **è¨˜è¼‰åˆ†é‡ãƒ»æ§‹é€ **:
   - å„é …ç›®600-800æ–‡å­—ï¼ˆfactual_data: 400æ–‡å­—ã€analytical_insights: 300æ–‡å­—ã€data_limitations: 100æ–‡å­—ï¼‰
   - å®šé‡ãƒ‡ãƒ¼ã‚¿ã‚’å¯èƒ½ãªé™ã‚Šå¤šãå«ã‚ã‚‹
   - å…·ä½“çš„åˆ¶åº¦åãƒ»ãƒ—ãƒ­ã‚°ãƒ©ãƒ åãƒ»æ•°å€¤ã‚’å„ªå…ˆ

JSONå½¢å¼ã§ä»¥ä¸‹ã®é€šã‚Šå›ç­”ã—ã¦ãã ã•ã„ï¼š

```json
{{
  "evp": {{
    "rewards": {{
      "factual_data": "IRé–‹ç¤ºãƒ»ä¼æ¥­å…¬å¼ã«æ˜ç¤ºã•ã‚ŒãŸå ±é…¬ãƒ»å¾…é‡ã®å…·ä½“çš„æƒ…å ±ï¼ˆå¹´åã€è³ä¸ã€ç¦åˆ©åšç”Ÿåˆ¶åº¦åã€æ•°å€¤ãƒ‡ãƒ¼ã‚¿ç­‰ï¼‰",
      "analytical_insights": "é–‹ç¤ºãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãåˆ†æãƒ»æ¨å®šï¼ˆæ¥­ç•Œæ¯”è¼ƒã€æˆé•·æ€§è©•ä¾¡ç­‰ï¼‰",
      "data_limitations": "æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹é …ç›®ãƒ»æ¨å®šãŒå¿…è¦ãªç®‡æ‰€"
    }},
    "opportunity": {{
      "factual_data": "IRé–‹ç¤ºãƒ»ä¼æ¥­å…¬å¼ã«æ˜ç¤ºã•ã‚ŒãŸã‚­ãƒ£ãƒªã‚¢ãƒ‘ã‚¹ãƒ»æˆé•·æ©Ÿä¼šï¼ˆåˆ¶åº¦åã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ å†…å®¹ã€å®Ÿç¸¾æ•°å€¤ç­‰ï¼‰",
      "analytical_insights": "åˆ¶åº¦ãƒ»å®Ÿç¸¾ã‹ã‚‰èª­ã¿å–ã‚Œã‚‹æˆé•·å¯èƒ½æ€§ã®åˆ†æ",
      "data_limitations": "è©³ç´°ãŒä¸æ˜ãªåˆ¶åº¦ãƒ»æ¨å®šè¦ç´ "
    }},
    "organization": {{
      "factual_data": "IRé–‹ç¤ºãƒ»ä¼æ¥­æ–‡åŒ–ãƒ»çµ„ç¹”ã«é–¢ã™ã‚‹å…¬å¼æƒ…å ±ï¼ˆå¾“æ¥­å“¡æ•°ã€çµ„ç¹”æ§‹é€ ã€ä¼æ¥­ç†å¿µã€åˆ¶åº¦ç­‰ã®å…·ä½“çš„å†…å®¹ï¼‰",
      "analytical_insights": "çµ„ç¹”ç‰¹æ€§ã‹ã‚‰æ¨æ¸¬ã•ã‚Œã‚‹è·å ´ç’°å¢ƒãƒ»ä¼æ¥­æ–‡åŒ–ã®åˆ†æ",
      "data_limitations": "å®šæ€§çš„æƒ…å ±ã‚„æ¨æ¸¬ãŒå¿…è¦ãªç®‡æ‰€"
    }},
    "people": {{
      "factual_data": "IRé–‹ç¤ºãƒ»äººæè‚²æˆãƒ»ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆã®å…¬å¼åˆ¶åº¦ï¼ˆç ”ä¿®åˆ¶åº¦åã€è©•ä¾¡åˆ¶åº¦ã€å®Ÿç¸¾æ•°å€¤ç­‰ï¼‰",
      "analytical_insights": "åˆ¶åº¦ã‹ã‚‰æ¨æ¸¬ã•ã‚Œã‚‹äººææˆ¦ç•¥ãƒ»æˆé•·ç’°å¢ƒã®åˆ†æ",
      "data_limitations": "åˆ¶åº¦è©³ç´°ã‚„åŠ¹æœãŒä¸æ˜ãªç®‡æ‰€"
    }},
    "work": {{
      "factual_data": "IRé–‹ç¤ºãƒ»åƒãæ–¹ã«é–¢ã™ã‚‹å…¬å¼æƒ…å ±ï¼ˆå‹¤å‹™åˆ¶åº¦ã€æ¥­å‹™å†…å®¹ã€åƒãæ–¹æ”¹é©ã®å…·ä½“çš„å–çµ„ç­‰ï¼‰",
      "analytical_insights": "åˆ¶åº¦ãƒ»å–çµ„ã‹ã‚‰æ¨æ¸¬ã•ã‚Œã‚‹ãƒ¯ãƒ¼ã‚¯ãƒ©ã‚¤ãƒ•ãƒãƒ©ãƒ³ã‚¹ãƒ»æ¥­å‹™ç‰¹æ€§",
      "data_limitations": "å®Ÿæ…‹ã‚„è©³ç´°ãŒä¸æ˜ãªåˆ¶åº¦ãƒ»æ¨å®šè¦ç´ "
    }}
  }},
  "business_analysis": {{
    "industry_market": {{
      "factual_data": "IRé–‹ç¤ºãƒ»å¤–éƒ¨èª¿æŸ»ã«ã‚ˆã‚‹å¸‚å ´è¦æ¨¡ãƒ»æ¥­ç•Œå‹•å‘ã®å…·ä½“çš„æ•°å€¤ãƒ»ãƒ‡ãƒ¼ã‚¿",
      "analytical_insights": "ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãå¸‚å ´ç’°å¢ƒãƒ»æˆé•·æ€§ã®åˆ†æãƒ»äºˆæ¸¬",
      "data_limitations": "ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹é ˜åŸŸãƒ»æ¨å®šãŒå¿…è¦ãªç®‡æ‰€"
    }},
    "market_position": {{
      "factual_data": "IRé–‹ç¤ºãƒ»å¤–éƒ¨èª¿æŸ»ã«ã‚ˆã‚‹å¸‚å ´ã‚·ã‚§ã‚¢ãƒ»å£²ä¸Šè¦æ¨¡ç­‰ã®å…·ä½“çš„é †ä½ãƒ»æ•°å€¤",
      "analytical_insights": "æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãç«¶äº‰åŠ›ãƒ»ãƒã‚¸ã‚·ãƒ§ãƒ³åˆ†æ",
      "data_limitations": "æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹ç«¶åˆãƒ»æ¨å®šè¦ç´ "
    }},
    "differentiation": {{
      "factual_data": "IRé–‹ç¤ºãƒ»ä¼æ¥­å…¬å¼ã«æ˜ç¤ºã•ã‚ŒãŸå·®åˆ¥åŒ–è¦å› ãƒ»ç«¶äº‰å„ªä½æ€§ï¼ˆæŠ€è¡“ã€ã‚µãƒ¼ãƒ“ã‚¹ã€å®Ÿç¸¾ç­‰ï¼‰",
      "analytical_insights": "å…¬å¼æƒ…å ±ã‹ã‚‰æ¨æ¸¬ã•ã‚Œã‚‹æŒç¶šçš„ç«¶äº‰å„ªä½æ€§ã®åˆ†æ",
      "data_limitations": "ç«¶äº‰å„ªä½æ€§ã®æŒç¶šæ€§ãƒ»åŠ¹æœãŒä¸æ˜ãªç®‡æ‰€"
    }},
    "business_portfolio": {{
      "factual_data": "IRé–‹ç¤ºã«ã‚ˆã‚‹ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥å£²ä¸Šãƒ»åˆ©ç›Šãƒ»æˆé•·ç‡ç­‰ã®å…·ä½“çš„æ•°å€¤",
      "analytical_insights": "æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãäº‹æ¥­ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªãƒ»åç›Šæ§‹é€ ã®åˆ†æ",
      "data_limitations": "è©³ç´°ãªåç›Šæ§‹é€ ã‚„å°†æ¥æ€§ãŒä¸æ˜ãªäº‹æ¥­é ˜åŸŸ"
    }}
  }}
}}
```

å„é …ç›®ã¯ä»¥ä¸‹ã®æ§‹é€ ã§**600-800æ–‡å­—**ã§å…·ä½“çš„ã«è¨˜è¼‰ã—ã¦ãã ã•ã„ï¼š

**ğŸ“Š factual_dataï¼ˆ400æ–‡å­—ï¼‰:** IRé–‹ç¤ºãƒ»ä¼æ¥­å…¬å¼ã®å…·ä½“çš„æ•°å€¤ãƒ»åˆ¶åº¦åãƒ»å®Ÿç¸¾ã®ã¿
**ğŸ” analytical_insightsï¼ˆ300æ–‡å­—ï¼‰:** factual_dataã«åŸºã¥ãåˆ†æãƒ»è©•ä¾¡ãƒ»æ¨æ¸¬
**âš ï¸ data_limitationsï¼ˆ100æ–‡å­—ï¼‰:** æƒ…å ±ä¸è¶³ç®‡æ‰€ã®æ˜ç¢ºãªæŒ‡æ‘˜
"""
        return prompt
    
    def research_company(self, company_info):
        """Deep IRæƒ…å ±çµ±åˆå‹ä¼æ¥­èª¿æŸ»ï¼ˆIRæ·±å±¤åé›† + å¤–éƒ¨æƒ…å ±è£œè¶³ï¼‰"""
        
        # Step 1: IRæƒ…å ±ã®æ·±å±¤åé›†ï¼ˆå†æœ‰åŠ¹åŒ–ï¼‰
        st.info("ï¿½ Step 1: IRæƒ…å ±ã‚’æ·±å±¤åé›†ä¸­ï¼ˆæ±ºç®—æ›¸ãƒ»æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ãƒ»ä¸­æœŸçµŒå–¶è¨ˆç”»ï¼‰...")
        ir_data = []
        
        try:
            ir_data = self.search_ir_documents_with_serpapi(company_info['company_name'])
            
            if ir_data:
                st.success(f"âœ… {len(ir_data)}ä»¶ã®IRæƒ…å ±ã‚’åé›†ã—ã¾ã—ãŸ")
                
                # IRæƒ…å ±ã®è©³ç´°è¡¨ç¤º
                with st.expander("ğŸ“Š ç™ºè¦‹ã—ãŸIRé–¢é€£æƒ…å ±ã®è©³ç´°"):
                    for i, item in enumerate(ir_data, 1):
                        st.write(f"**{i}. {item['title']}**")
                        st.write(f"ç¨®é¡: {item.get('document_type', 'IRé–¢é€£è³‡æ–™')}")
                        st.write(f"ã‚½ãƒ¼ã‚¹: {item.get('source', 'ä¸æ˜')}")
                        st.write(f"æ¦‚è¦: {item.get('snippet', '')[:200]}...")
                        st.write(f"URL: {item.get('url', '')}")
                        st.write("---")
            else:
                st.warning("âš ï¸ IRé–¢é€£æƒ…å ±ã®ç™ºè¦‹ãŒã§ãã¾ã›ã‚“ã§ã—ãŸ")
                st.info("ğŸ’¡ ä¼æ¥­å…¬å¼ã‚µã‚¤ãƒˆã®åŸºæœ¬æƒ…å ±ã§åˆ†æã‚’ç¶™ç¶šã—ã¾ã™")
                
        except Exception as e:
            st.warning(f"âš ï¸ IRæƒ…å ±æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            st.info("ğŸ’¡ ä¼æ¥­å…¬å¼ã‚µã‚¤ãƒˆã®åŸºæœ¬æƒ…å ±ã§åˆ†æã‚’ç¶™ç¶šã—ã¾ã™")
            ir_data = []
        
        # Step 2: ä¼æ¥­å…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰ã®åŸºå¹¹æƒ…å ±åé›†
        st.info("ğŸ¢ Step 2: ä¼æ¥­å…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰åŸºå¹¹æƒ…å ±ã‚’åé›†ä¸­...")
        
        # Step 3: å¤–éƒ¨æƒ…å ±ã«ã‚ˆã‚‹è£œè¶³ãƒ»æ¤œè¨¼ï¼ˆæœ€å°é™ï¼‰
        st.info("ğŸŒ Step 3: å¤–éƒ¨æƒ…å ±ã«ã‚ˆã‚‹è£œè¶³ãƒ»æ¤œè¨¼ä¸­...")
        try:
            industry_keywords = company_info.get('focus_area', '').replace('åˆ†é‡', '').replace('é ˜åŸŸ', '')
            external_data = self.search_external_sources(company_info['company_name'], industry_keywords)
            
            if external_data:
                st.success(f"âœ… {len(external_data)}ä»¶ã®è£œè¶³æƒ…å ±ã‚’åé›†")
                
                # å¤–éƒ¨æƒ…å ±ã®ç°¡æ½”è¡¨ç¤º
                with st.expander("ğŸ” è£œè¶³æƒ…å ±ï¼ˆå¤–éƒ¨ã‚½ãƒ¼ã‚¹ï¼‰"):
                    for i, item in enumerate(external_data, 1):
                        st.write(f"**{i}. {item['source']}**: {item['title'][:80]}...")
            else:
                st.info("â„¹ï¸ å¤–éƒ¨è£œè¶³æƒ…å ±ãªã— - ä¼æ¥­å…¬å¼æƒ…å ±ã§åˆ†æç¶™ç¶š")
                external_data = []
                
        except Exception as e:
            st.warning(f"âš ï¸ å¤–éƒ¨æƒ…å ±åé›†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            st.info("ğŸ’¡ ä¼æ¥­å…¬å¼æƒ…å ±ã‚’é‡è¦–ã—ãŸåˆ†æã‚’ç¶™ç¶š")
            external_data = []
        
        # Step 4: IRæƒ…å ±çµ±åˆå‹ã®é«˜ç²¾åº¦åˆ†æ
        st.info("ğŸ§  Step 4: IRæƒ…å ±ã‚’çµ±åˆã—ãŸé«˜ç²¾åº¦åˆ†æå®Ÿè¡Œä¸­...")
        prompt = self.create_ir_integrated_prompt(company_info, ir_data, external_data)
        temperature = 0.05  # ã‚ˆã‚Šä¿å®ˆçš„ã§æ­£ç¢ºæ€§é‡è¦–
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ä¼æ¥­åˆ†æã®å°‚é–€å®¶ã¨ã—ã¦ã€IRé–‹ç¤ºæƒ…å ±ã‚’æœ€å„ªå…ˆã¨ã—ã€äº‹å®Ÿã¨æ¨æ¸¬ã‚’æ˜ç¢ºã«åŒºåˆ¥ã—ãŸæ§‹é€ åŒ–åˆ†æã‚’JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=8000,  # ã‚ˆã‚Šè©³ç´°ãªåˆ†æã®ãŸã‚å¢—é‡
                temperature=temperature
            )
            
            content = response.choices[0].message.content.strip()
            
            # JSONãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            if "```json" in content:
                json_start = content.find("```json") + 7
                json_end = content.find("```", json_start)
                json_content = content[json_start:json_end].strip()
            else:
                json_content = content
            
            research_data = json.loads(json_content)
            
            # IRæƒ…å ±ã‚’åˆ†æçµæœã«å«ã‚ã‚‹
            if ir_data:
                research_data['ir_summary'] = {
                    'documents_analyzed': len(ir_data),
                    'key_documents': [item['title'] for item in ir_data[:5]],
                    'coverage_period': f"éå»3å¹´é–“ã®IRæƒ…å ±"
                }
            
            return research_data
            
        except Exception as e:
            st.error(f"AIåˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return None
    
    
    def calculate_analysis_quality_score(self, evp_data, hierarchical_data):
        """åˆ†æå“è³ªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        base_score = 60
        
        # ãƒ‡ãƒ¼ã‚¿ä¿¡é ¼æ€§ãƒœãƒ¼ãƒŠã‚¹
        reliability_bonus = hierarchical_data['quality_assessment']['overall_reliability'] * 0.3
        
        # IRé–‹ç¤ºã‚«ãƒãƒ¬ãƒƒã‚¸ãƒœãƒ¼ãƒŠã‚¹
        ir_bonus = hierarchical_data['quality_assessment']['ir_coverage'] * 0.2
        
        # ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯æ•´åˆæ€§ãƒœãƒ¼ãƒŠã‚¹
        logic_bonus = hierarchical_data['quality_assessment']['business_logic_consistency'] * 0.2
        
        # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§ãƒœãƒ¼ãƒŠã‚¹
        completeness_bonus = 10 if hierarchical_data['quality_assessment']['fact_based_ratio'] > 70 else 5
        
        total_score = base_score + reliability_bonus + ir_bonus + logic_bonus + completeness_bonus
        return min(100, int(total_score))
    
    def extract_structured_ir_data(self, company_name):
        """Phase 2: IRæƒ…å ±ã®æ§‹é€ åŒ–æŠ½å‡ºï¼ˆç²¾åº¦å‘ä¸Šç‰ˆï¼‰"""
        # æœ€æ–°IRæ–‡æ›¸ã®å„ªå…ˆæ¤œç´¢ã‚¯ã‚¨ãƒªï¼ˆå¤ã„æƒ…å ±æ··å…¥é˜²æ­¢ï¼‰
        current_year = 2024
        previous_year = 2023
        
        priority_ir_queries = [
            f"{company_name} æ±ºç®—çŸ­ä¿¡ {current_year}å¹´ 3æœˆæœŸ",
            f"{company_name} æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ {previous_year}å¹´åº¦", 
            f"{company_name} å››åŠæœŸå ±å‘Šæ›¸ {current_year}",
            f"{company_name} IR æœ€æ–° å£²ä¸Š å–¶æ¥­åˆ©ç›Š {current_year}",
            f'site:ir.{company_name.lower()}.co.jp OR site:{company_name.lower()}.co.jp/ir æ±ºç®— {current_year}'
        ]
        
        all_ir_data = []
        
        # å„ªå…ˆåº¦é †ã«æ¤œç´¢ï¼ˆæœ€æ–°æƒ…å ±å„ªå…ˆï¼‰
        for query in priority_ir_queries:
            search_results = self.serp_search(query)
            
            # æ¤œç´¢çµæœã‚’å¹´æ¬¡ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆ2022å¹´ä»¥é™ã®æƒ…å ±ã®ã¿ï¼‰
            filtered_results = []
            for result in search_results:
                content = f"{result.get('title', '')} {result.get('snippet', '')}"
                # å¤ã„å¹´æ¬¡æƒ…å ±ã‚’é™¤å¤–
                if any(old_year in content for old_year in ['2021', '2020', '2019', '2018']):
                    continue
                # æœ€æ–°å¹´æ¬¡æƒ…å ±ã‚’å„ªå…ˆ
                if any(recent_year in content for recent_year in [str(current_year), str(previous_year)]):
                    result['priority'] = 'high'
                else:
                    result['priority'] = 'medium'
                filtered_results.append(result)
            
            all_ir_data.extend(filtered_results[:2])  # å„ã‚¯ã‚¨ãƒªã‹ã‚‰æœ€æ–°2ä»¶
        
        # å„ªå…ˆåº¦ã§ã‚½ãƒ¼ãƒˆï¼ˆé«˜å„ªå…ˆåº¦ã‚’å…ˆã«å‡¦ç†ï¼‰
        all_ir_data.sort(key=lambda x: 0 if x.get('priority') == 'high' else 1)
        
        # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã®åˆæœŸåŒ–
        structured_ir = {
            'financial_data': {
                'revenue': {'value': None, 'source': '', 'year': '', 'confidence': 0},
                'operating_profit': {'value': None, 'source': '', 'year': '', 'confidence': 0},
                'employees': {'value': None, 'source': '', 'year': '', 'confidence': 0}
            },
            'business_strategy': {
                'key_strategies': [],
                'growth_areas': [],
                'challenges': []
            },
            'data_quality': {
                'ir_documents_found': len(all_ir_data),
                'data_completeness': 0,
                'reliability_score': 0,
                'latest_year_coverage': False
            }
        }
        
        # æ”¹è‰¯ã•ã‚ŒãŸè²¡å‹™ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã‚ˆã‚Šç²¾å¯†ï¼‰
        revenue_patterns = [
            rf'(?:{current_year}|{previous_year})å¹´.*?å£²ä¸Šé«˜[ï¼š:\s]*([0-9,]+(?:\.[0-9]+)?)\s*å„„å††',
            rf'å£²ä¸Šé«˜[ï¼š:\s]*([0-9,]+(?:\.[0-9]+)?)\s*å„„å††.*?(?:{current_year}|{previous_year})',
            r'å£²ä¸Šåç›Š[ï¼š:\s]*([0-9,]+(?:\.[0-9]+)?)\s*å„„å††',
            r'Revenue[ï¼š:\s]*([0-9,]+(?:\.[0-9]+)?)\s*billion',
        ]
        
        profit_patterns = [
            rf'(?:{current_year}|{previous_year})å¹´.*?å–¶æ¥­åˆ©ç›Š[ï¼š:\s]*([0-9,]+(?:\.[0-9]+)?)\s*å„„å††',
            rf'å–¶æ¥­åˆ©ç›Š[ï¼š:\s]*([0-9,]+(?:\.[0-9]+)?)\s*å„„å††.*?(?:{current_year}|{previous_year})',
            r'Operating Income[ï¼š:\s]*([0-9,]+(?:\.[0-9]+)?)\s*billion',
        ]
        
        employee_patterns = [
            rf'(?:{current_year}|{previous_year})å¹´.*?å¾“æ¥­å“¡æ•°[ï¼š:\s]*([0-9,]+)\s*[äººå]',
            rf'å¾“æ¥­å“¡æ•°[ï¼š:\s]*([0-9,]+)\s*[äººå].*?(?:{current_year}|{previous_year})',
            r'ç¤¾å“¡æ•°[ï¼š:\s]*([0-9,]+)\s*[äººå]',
        ]
        
        # å„IRæ–‡æ›¸ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼ˆå„ªå…ˆåº¦é †ï¼‰
        for item in all_ir_data:
            content = f"{item.get('title', '')} {item.get('snippet', '')}"
            source = item.get('source', 'IRæ–‡æ›¸')
            priority = item.get('priority', 'medium')
            
            # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’å„ªå…ˆåº¦ã«åŸºã¥ã„ã¦è¨­å®š
            confidence = 90 if priority == 'high' else 70
            
            # å¹´æ¬¡æƒ…å ±ã®æŠ½å‡º
            year_match = None
            for year in [current_year, previous_year]:
                if str(year) in content:
                    year_match = str(year)
                    structured_ir['data_quality']['latest_year_coverage'] = True
                    break
            
            # å£²ä¸Šé«˜ã®æŠ½å‡ºï¼ˆé«˜ä¿¡é ¼åº¦ã®ã‚‚ã®ã‚’å„ªå…ˆï¼‰
            if not structured_ir['financial_data']['revenue']['value'] or confidence > structured_ir['financial_data']['revenue']['confidence']:
                for pattern in revenue_patterns:
                    match = re.search(pattern, content)
                    if match:
                        structured_ir['financial_data']['revenue'] = {
                            'value': f"{match.group(1)}å„„å††",
                            'source': source,
                            'year': year_match or f'{previous_year}-{current_year}',
                            'confidence': confidence
                        }
                        break
            
            # å–¶æ¥­åˆ©ç›Šã®æŠ½å‡º
            if not structured_ir['financial_data']['operating_profit']['value'] or confidence > structured_ir['financial_data']['operating_profit']['confidence']:
                for pattern in profit_patterns:
                    match = re.search(pattern, content)
                    if match:
                        structured_ir['financial_data']['operating_profit'] = {
                            'value': f"{match.group(1)}å„„å††",
                            'source': source,
                            'year': year_match or f'{previous_year}-{current_year}',
                            'confidence': confidence
                        }
                        break
            
            # å¾“æ¥­å“¡æ•°ã®æŠ½å‡º
            if not structured_ir['financial_data']['employees']['value'] or confidence > structured_ir['financial_data']['employees']['confidence']:
                for pattern in employee_patterns:
                    match = re.search(pattern, content)
                    if match:
                        structured_ir['financial_data']['employees'] = {
                            'value': f"{match.group(1).replace(',', '')}äºº",
                            'source': source,
                            'year': year_match or f'{previous_year}-{current_year}',
                            'confidence': confidence
                        }
                        break
        
        # ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§ã®è¨ˆç®—ï¼ˆä¿¡é ¼åº¦åŠ é‡ï¼‰
        data_fields = ['revenue', 'operating_profit', 'employees']
        weighted_completeness = 0
        total_weight = 0
        
        for field in data_fields:
            if structured_ir['financial_data'][field]['value']:
                confidence = structured_ir['financial_data'][field]['confidence']
                weighted_completeness += confidence
                total_weight += 100
            else:
                total_weight += 100
        
        structured_ir['data_quality']['data_completeness'] = (weighted_completeness / total_weight) * 100 if total_weight > 0 else 0
        
        # ä¿¡é ¼æ€§ã‚¹ã‚³ã‚¢ã®è¨ˆç®—ï¼ˆæœ€æ–°æƒ…å ±é‡è¦–ï¼‰
        base_score = min(len(all_ir_data) * 8, 40)  # æœ€å¤§40ç‚¹
        completeness_score = structured_ir['data_quality']['data_completeness'] * 0.4  # æœ€å¤§40ç‚¹
        latest_bonus = 20 if structured_ir['data_quality']['latest_year_coverage'] else 0  # æœ€æ–°å¹´æ¬¡ãƒœãƒ¼ãƒŠã‚¹
        
        structured_ir['data_quality']['reliability_score'] = base_score + completeness_score + latest_bonus
        
        return structured_ir
    
    def validate_data_reliability(self, company_fundamentals, structured_ir):
        """Phase 3: ãƒ‡ãƒ¼ã‚¿ä¿¡é ¼æ€§ã®æ¤œè¨¼ï¼ˆæ–°4æ®µéšã‚·ã‚¹ãƒ†ãƒ ç”¨ï¼‰"""
        validated_data = {
            'financial_validation': {
                'revenue_plausible': True,
                'profit_margin_reasonable': True,
                'employee_count_realistic': True,
                'business_logic_consistent': True
            },
            'industry_alignment': {
                'sector_appropriate': True,
                'competitor_consistent': True,
                'market_size_aligned': True
            },
            'data_conflicts': [],
            'quality_scores': {
                'fact_based_ratio': 0,
                'ir_coverage': 0,
                'business_logic_consistency': 0
            }
        }
        
        # ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯æ¤œè¨¼
        industry = company_fundamentals.get('industry_classification', '')
        
        # æ¥­ç•Œæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        if 'HRãƒ»äººæã‚µãƒ¼ãƒ“ã‚¹' in industry:
            # äººæã‚µãƒ¼ãƒ“ã‚¹æ¥­ç•Œã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
            if structured_ir['financial_data']['revenue']['value']:
                revenue_str = structured_ir['financial_data']['revenue']['value']
                revenue_match = re.search(r'([0-9,]+(?:\.[0-9]+)?)', revenue_str)
                if revenue_match:
                    revenue_num = float(revenue_match.group(1).replace(',', ''))
                    if revenue_num < 100 or revenue_num > 50000:  # 100å„„ã€œ5å…†å††ã®ç¯„å›²
                        validated_data['data_conflicts'].append("å£²ä¸Šè¦æ¨¡ãŒäººæã‚µãƒ¼ãƒ“ã‚¹æ¥­ç•Œã®ä¸€èˆ¬çš„ç¯„å›²ã‚’å¤–ã‚Œã¦ã„ã¾ã™")
                        validated_data['financial_validation']['business_logic_consistent'] = False
        
        # IRé–‹ç¤ºã‚«ãƒãƒ¬ãƒƒã‚¸ã®è¨ˆç®—
        ir_fields = ['revenue', 'operating_profit', 'employees']
        ir_covered = sum(1 for field in ir_fields if structured_ir['financial_data'][field]['value'])
        validated_data['quality_scores']['ir_coverage'] = (ir_covered / len(ir_fields)) * 100
        
        # äº‹å®Ÿãƒ™ãƒ¼ã‚¹å‰²åˆï¼ˆIRæ–‡æ›¸ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®å‰²åˆï¼‰
        total_data_points = 3  # revenue, profit, employees
        ir_data_points = ir_covered
        validated_data['quality_scores']['fact_based_ratio'] = (ir_data_points / total_data_points) * 100
        
        # ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯æ•´åˆæ€§ã‚¹ã‚³ã‚¢
        logic_checks = [
            validated_data['financial_validation']['business_logic_consistent'],
            validated_data['industry_alignment']['sector_appropriate'],
            validated_data['industry_alignment']['competitor_consistent']
        ]
        validated_data['quality_scores']['business_logic_consistency'] = (sum(logic_checks) / len(logic_checks)) * 100
        
        return validated_data
    
    def create_data_source_hierarchy(self, validated_data):
        """Phase 3è£œå®Œ: ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹éšå±¤ã®ä½œæˆï¼ˆæ–°4æ®µéšã‚·ã‚¹ãƒ†ãƒ ç”¨ï¼‰"""
        hierarchical_data = {
            'data_sources_by_tier': {
                'Tier 1 (IRé–‹ç¤º)': [],
                'Tier 2 (ä¼æ¥­å…¬å¼)': [],
                'Tier 3 (å¤–éƒ¨è¨˜äº‹)': [],
                'Tier 4 (æ¨å®š)': []
            },
            'quality_assessment': {
                'overall_reliability': validated_data['quality_scores']['ir_coverage'],
                'ir_coverage': validated_data['quality_scores']['ir_coverage'],
                'fact_based_ratio': validated_data['quality_scores']['fact_based_ratio'],
                'business_logic_consistency': validated_data['quality_scores']['business_logic_consistency']
            },
            'error_prevention': {
                'industry_misclassification_risk': 'Low' if validated_data['industry_alignment']['sector_appropriate'] else 'High',
                'competitor_confusion_risk': 'Low' if validated_data['industry_alignment']['competitor_consistent'] else 'High',
                'data_hallucination_risk': 'Low' if validated_data['quality_scores']['fact_based_ratio'] > 70 else 'High'
            }
        }
        
        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®éšå±¤åˆ†é¡
        if validated_data['quality_scores']['ir_coverage'] > 0:
            hierarchical_data['data_sources_by_tier']['Tier 1 (IRé–‹ç¤º)'].append('æ±ºç®—çŸ­ä¿¡ãƒ»æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸')
        
        return hierarchical_data

    def save_results(self, company_info, research_data):
        """çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"research_{company_info['company_name']}_{timestamp}.json"
        filepath = self.results_dir / filename
        
        save_data = {
            "company_info": company_info,
            "research_results": research_data,
            "generated_at": datetime.now().isoformat()
        }
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            return filepath, save_data
        except:
            # æœ¬ç•ªç’°å¢ƒã§ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã«å¤±æ•—ã—ãŸå ´åˆã¯çµæœã®ã¿è¿”ã™
            return None, save_data

def main():
    st.title("ğŸ¢ AIä¼æ¥­åˆ†æã‚·ã‚¹ãƒ†ãƒ ")
    st.markdown("### ä¼æ¥­ã®EVPãƒ»ãƒ“ã‚¸ãƒã‚¹åˆ†æã‚’è‡ªå‹•åŒ–ã™ã‚‹AIã‚·ã‚¹ãƒ†ãƒ ")
    
    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
    with st.expander("â„¹ï¸ ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±", expanded=False):
        st.markdown("""
        **æ©Ÿèƒ½:**
        - **EVPåˆ†æ**: Rewards, Opportunity, Organization, People, Work
        - **ãƒ“ã‚¸ãƒã‚¹åˆ†æ**: æ¥­ç•Œåˆ†æ, ç«¶åˆæ¯”è¼ƒ, ç‹¬è‡ªæ€§, äº‹æ¥­ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ª
        - **AIè‡ªå‹•èª¿æŸ»**: OpenAI GPT-4o-miniä½¿ç”¨
        - **çµæœä¿å­˜**: JSONå½¢å¼ã§ã®è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ
        
        **å…¥åŠ›é …ç›®:**
        - ä¼æ¥­åï¼ˆå¿…é ˆï¼‰
        - ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸URLï¼ˆä»»æ„ï¼‰
        - åˆ†æé‡ç‚¹åˆ†é‡ï¼ˆå¿…é ˆï¼‰
        """)
    
    # å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
    with st.form("company_form"):
        col1, col2 = st.columns(2)
        
        with col1:
            company_name = st.text_input(
                "ğŸ¢ ä¼æ¥­å *", 
                placeholder="ä¾‹: ãƒˆãƒ¨ã‚¿è‡ªå‹•è»Šã€ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯ã€ãƒªã‚¯ãƒ«ãƒ¼ãƒˆ",
                help="åˆ†æå¯¾è±¡ã®ä¼æ¥­åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"
            )
            
            # ä¼æ¥­åˆ†æã®ç¯„å›²èª¬æ˜
            st.info("ğŸ¢ **ä¼æ¥­å…¨ä½“ã®åŒ…æ‹¬çš„åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™**")
            st.write("- äº‹æ¥­ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªå…¨ä½“ã®åˆ†æ")
            st.write("- ä¸»åŠ›äº‹æ¥­é ˜åŸŸã®è©³ç´°è©•ä¾¡") 
            st.write("- æ¥­ç•Œå†…ç«¶åˆåˆ†æãƒ»å¸‚å ´ãƒã‚¸ã‚·ãƒ§ãƒ³")
            st.write("- IRé–‹ç¤ºæƒ…å ±ã«åŸºã¥ãæ­£ç¢ºãªè²¡å‹™åˆ†æ")
        
        with col2:
            # åˆ†æãƒ¬ãƒ™ãƒ«é¸æŠ
            analysis_level = st.selectbox(
                "ğŸ“Š åˆ†æãƒ¬ãƒ™ãƒ«",
                ["æ¨™æº–åˆ†æ", "è©³ç´°åˆ†æ"],
                help="è©³ç´°åˆ†æã§ã¯æ›´ã«æ·±ã„èª¿æŸ»ã‚’å®Ÿæ–½ã—ã¾ã™"
            )
        
        # è©³ç´°è¨­å®šï¼ˆä¸Šç´šè€…å‘ã‘ï¼‰
        with st.expander("âš™ï¸ è©³ç´°è¨­å®š", expanded=False):
            col3, col4 = st.columns(2)
            with col3:
                date_range = st.selectbox("æƒ…å ±ç¯„å›²", ["3å¹´ä»¥å†…", "2å¹´ä»¥å†…", "1å¹´ä»¥å†…"], index=0)
                enable_chat = st.checkbox("åˆ†æå¾Œãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½", value=True, help="åˆ†æçµæœã«é–¢ã™ã‚‹è¿½åŠ è³ªå•ãŒå¯èƒ½ã«ãªã‚Šã¾ã™")
            with col4:
                # IRé–¢é€£è¨­å®šã¯éè¡¨ç¤ºï¼ˆå°†æ¥ã®æ‹¡å¼µç”¨ï¼‰
                max_crawl_depth = 2  # å›ºå®šå€¤
                enable_hallucination_check = False  # IRæ©Ÿèƒ½ç„¡åŠ¹æ™‚ã¯OFF
        
        st.markdown("---")
        submitted = st.form_submit_button("ğŸ” AIåˆ†æé–‹å§‹", type="primary", use_container_width=True)
    
    # ãƒ•ã‚©ãƒ¼ãƒ é€ä¿¡æ™‚ã®å‡¦ç†
    if submitted:
        if not company_name:
            st.error("ğŸš¨ ä¼æ¥­åã¯å¿…é ˆå…¥åŠ›ã§ã™ã€‚")
            return
        
        # èª¿æŸ»ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å…ˆã«åˆæœŸåŒ–
        researcher = StreamlitCompanyResearcher()
        
        # ä¼šç¤¾æƒ…å ±ã®æº–å‚™ï¼ˆä¼æ¥­åˆ†æã«çµ±ä¸€ï¼‰
        company_info = {
            "company_name": company_name,
            "analysis_level": analysis_level,
            "max_crawl_depth": max_crawl_depth,
            "date_range": date_range,
            "enable_hallucination_check": enable_hallucination_check,
            "enable_chat": enable_chat,
            "timestamp": datetime.now().isoformat()
        }
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ä»˜ãã§å®Ÿè¡Œ
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        status_text.text("ğŸ” AIåˆ†æã‚’é–‹å§‹ã—ã¦ã„ã¾ã™...")
        progress_bar.progress(20)
        
        with st.spinner("ğŸ¤– AIåˆ†æä¸­... (30-60ç§’ç¨‹åº¦ãŠå¾…ã¡ãã ã•ã„)"):
            progress_bar.progress(50)
            research_data = researcher.research_company(company_info)
            progress_bar.progress(80)
        
        if research_data:
            # çµæœä¿å­˜
            filepath, save_data = researcher.save_results(company_info, research_data)
            progress_bar.progress(100)
            status_text.text("âœ… åˆ†æå®Œäº†ï¼")
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«åˆ†æçµæœã‚’ä¿å­˜
            st.session_state.analysis_results = {
                "research_data": research_data,
                "company_info": company_info,
                "save_data": save_data,
                "filepath": filepath,
                "researcher": researcher
            }
        else:
            progress_bar.progress(0)
            status_text.text("âŒ åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ")
            st.error("âŒ AIåˆ†æã«å¤±æ•—ã—ã¾ã—ãŸã€‚APIã‚­ãƒ¼ã®è¨­å®šã¾ãŸã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    
    # åˆ†æçµæœã®è¡¨ç¤ºï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‹ã‚‰ï¼‰
    if 'analysis_results' in st.session_state:
        results = st.session_state.analysis_results
        research_data = results["research_data"]
        company_info = results["company_info"]
        save_data = results["save_data"]
        filepath = results["filepath"]
        researcher = results["researcher"]
        
        # åˆ†æçµæœã®æ§‹é€ ç¢ºèªï¼ˆç°¡ç´ åŒ–ï¼‰
        st.info("ğŸ“ åˆ†æçµæœ:")
        st.json({
            "ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¼": list(research_data.keys()),
            "EVPé …ç›®æ•°": len(research_data.get('evp', {})),
            "ãƒ“ã‚¸ãƒã‚¹åˆ†æé …ç›®æ•°": len(research_data.get('business_analysis', {}))
        })
        
        # çµæœè¡¨ç¤º
        st.success("ğŸ‰ AIåˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        
        # åŸºæœ¬æƒ…å ±è¡¨ç¤º
        st.markdown("---")
        st.subheader("ğŸ“Š åˆ†æçµæœã‚µãƒãƒªãƒ¼")
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ğŸ¢ ä¼æ¥­å", company_info["company_name"])
        with col2:
            st.metric("ğŸ¯ é‡ç‚¹åˆ†é‡", company_info["focus_area"])
        with col3:
            st.metric("ğŸ“Š åˆ†æãƒ¬ãƒ™ãƒ«", company_info["analysis_level"])
        with col4:
            st.metric("ğŸ• åˆ†ææ—¥æ™‚", company_info['timestamp'][:19])
        
        st.markdown("---")
            
        
        # ã‚¿ãƒ–å½¢å¼ã§çµæœè¡¨ç¤º
        tab1, tab2, tab3 = st.tabs(["ğŸ“ˆ EVPåˆ†æ", "ğŸ† ãƒ“ã‚¸ãƒã‚¹åˆ†æ", "ğŸ“„ JSONå‡ºåŠ›"])
        
        with tab1:
            st.subheader("ğŸ“ˆ EVPï¼ˆEmployee Value Propositionï¼‰åˆ†æ")
            
            evp_labels = {
                "rewards": "ğŸ’° Rewardsï¼ˆå ±é…¬ãƒ»å¾…é‡ï¼‰",
                "opportunity": "ğŸš€ Opportunityï¼ˆæ©Ÿä¼šãƒ»æˆé•·ï¼‰",
                "organization": "ğŸ¢ Organizationï¼ˆçµ„ç¹”ãƒ»ä¼æ¥­æ–‡åŒ–ï¼‰",
                "people": "ğŸ‘¥ Peopleï¼ˆäººæãƒ»ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆï¼‰",
                "work": "ğŸ’¼ Workï¼ˆåƒãæ–¹ãƒ»æ¥­å‹™ï¼‰"
            }
            
            # EVPåˆ†æçµæœã®å®‰å…¨ãªè¡¨ç¤º
            evp_data = research_data.get('evp', {})
            if evp_data:
                for key, label in evp_labels.items():
                    with st.expander(label, expanded=True):
                        content = evp_data.get(key, "åˆ†æãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
                        st.write(content)
            else:
                st.warning("EVPåˆ†æãƒ‡ãƒ¼ã‚¿ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                st.json(research_data)  # ãƒ‡ãƒãƒƒã‚°ç”¨
        
        with tab2:
            st.subheader("ğŸ† ãƒ“ã‚¸ãƒã‚¹åˆ†æ")
            
            business_labels = {
                "industry_market": "ğŸ“ˆ æ¥­ç•Œãƒ»å¸‚å ´åˆ†æ",
                "market_position": "ğŸ† æ¥­ç•Œå†…ãƒã‚¸ã‚·ãƒ§ãƒ³",
                "differentiation": "â­ ç‹¬è‡ªæ€§ãƒ»å·®åˆ¥åŒ–è¦å› ",
                "business_portfolio": "ğŸ—ï¸ äº‹æ¥­ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªåˆ†æ"
            }
            
            # ãƒ“ã‚¸ãƒã‚¹åˆ†æçµæœã®ä¿¡é ¼æ€§ãƒã‚§ãƒƒã‚¯ä»˜ãè¡¨ç¤º
            business_data = research_data.get('business_analysis', {})
            if business_data:
                # ä¿¡é ¼æ€§è­¦å‘Šã®è¡¨ç¤º
                st.warning("""
                âš ï¸ **æƒ…å ±ã®ä¿¡é ¼æ€§ã«ã¤ã„ã¦**
                - âœ… **ä¸€æ¬¡æƒ…å ±**: ä¼æ¥­å…¬å¼é–‹ç¤ºè³‡æ–™ï¼ˆæœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ã€æ±ºç®—çŸ­ä¿¡ç­‰ï¼‰ã«åŸºã¥ãå†…å®¹
                - âš ï¸ **è¦æ¤œè¨¼**: æ¨å®šå€¤ã‚„ç¬¬ä¸‰è€…æƒ…å ±ã«åŸºã¥ãå†…å®¹
                - âŒ **ç¢ºèªä¸å¯**: å…¬å¼æƒ…å ±ã§è£ä»˜ã‘ãŒå–ã‚Œãªã„å†…å®¹
                
                å„é …ç›®ã§æƒ…å ±æºãŒæ˜è¨˜ã•ã‚Œã¦ã„ãªã„æ•°å€¤ãƒ»ã‚·ã‚§ã‚¢ãƒ»ç«¶åˆæƒ…å ±ã¯æ…é‡ã«ã”åˆ¤æ–­ãã ã•ã„ã€‚
                """)
                
                # ä¿¡é ¼æ€§è©•ä¾¡ã®ãŸã‚ã«ç ”ç©¶è€…ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
                temp_researcher = StreamlitCompanyResearcher()
                
                for key, label in business_labels.items():
                    with st.expander(label, expanded=True):
                        content = business_data.get(key, "åˆ†æãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
                        
                        # ä¿¡é ¼æ€§ãƒã‚§ãƒƒã‚¯
                        reliability_score = temp_researcher.assess_content_reliability(content)
                        
                        if reliability_score >= 80:
                            st.success("âœ… é«˜ä¿¡é ¼æ€§: ä¸€æ¬¡æƒ…å ±ã«åŸºã¥ãåˆ†æ")
                        elif reliability_score >= 60:
                            st.warning("âš ï¸ ä¸­ä¿¡é ¼æ€§: ä¸€éƒ¨æ¨å®šã‚’å«ã‚€å¯èƒ½æ€§")
                        else:
                            st.error("âŒ ä½ä¿¡é ¼æ€§: è¦æ¤œè¨¼æƒ…å ±ã‚’å«ã‚€")
                        
                        st.write(content)
            else:
                st.warning("ãƒ“ã‚¸ãƒã‚¹åˆ†æãƒ‡ãƒ¼ã‚¿ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                st.json(research_data)  # ãƒ‡ãƒãƒƒã‚°ç”¨
        
        with tab3:
            st.subheader("ğŸ“„ JSONå½¢å¼ã®åˆ†æçµæœ")
            st.markdown("åˆ†æçµæœã‚’JSONå½¢å¼ã§è¡¨ç¤ºã—ã¾ã™ã€‚ã‚³ãƒ”ãƒ¼ã—ã¦ä»–ã®ã‚·ã‚¹ãƒ†ãƒ ã§ã‚‚æ´»ç”¨ã§ãã¾ã™ã€‚")
            
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
            json_output = json.dumps(save_data, ensure_ascii=False, indent=2)
            st.download_button(
                label="ğŸ’¾ JSONçµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=json_output,
                file_name=f"research_{company_info['company_name']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json"
            )
            
            # JSONè¡¨ç¤º
            st.code(json_output, language="json")
            
            if filepath:
                st.info(f"ğŸ’¾ çµæœã¯ã‚µãƒ¼ãƒãƒ¼ã«ã‚‚ä¿å­˜ã•ã‚Œã¾ã—ãŸ: {filepath}")
        
        # ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ï¼ˆåˆ†æçµæœå¾Œã®ã¿è¡¨ç¤ºï¼‰
        if company_info.get('enable_chat', True):
            st.markdown("---")
            st.subheader("ğŸ’¬ åˆ†æçµæœã«é–¢ã™ã‚‹è¿½åŠ è³ªå•")
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
            if 'chat_history' not in st.session_state:
                st.session_state.chat_history = []
            if 'analysis_context' not in st.session_state:
                st.session_state.analysis_context = None
            
            # åˆ†æçµæœã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ä¿å­˜
            if st.session_state.analysis_context != research_data:
                st.session_state.analysis_context = research_data
                st.session_state.chat_history = []  # æ–°ã—ã„åˆ†ææ™‚ã¯ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆ
            
            # åˆ†æçµæœãƒ™ãƒ¼ã‚¹ã®è­¦å‘Š
            st.warning("âš ï¸ ã“ã®è³ªå•æ©Ÿèƒ½ã¯åˆ†æçµæœã«åŸºã¥ã„ã¦å›ç­”ã—ã¾ã™ã€‚åˆ†æãƒ‡ãƒ¼ã‚¿ä»¥å¤–ã®æƒ…å ±ã¯æä¾›ã§ãã¾ã›ã‚“ã€‚")
            
            # ãƒãƒ£ãƒƒãƒˆå±¥æ­´è¡¨ç¤º
            for i, (question, answer) in enumerate(st.session_state.chat_history):
                with st.chat_message("user"):
                    st.write(question)
                with st.chat_message("assistant"):
                    st.write(answer)
            
            # è³ªå•å…¥åŠ›
            user_question = st.chat_input("åˆ†æçµæœã«ã¤ã„ã¦è³ªå•ã—ã¦ãã ã•ã„...")
            
            if user_question:
                # è³ªå•ã‚’å±¥æ­´ã«è¿½åŠ 
                with st.chat_message("user"):
                    st.write(user_question)
                
                # AIå›ç­”ç”Ÿæˆ
                with st.chat_message("assistant"):
                    with st.spinner("å›ç­”ã‚’ç”Ÿæˆä¸­..."):
                        answer = researcher.generate_chat_response(
                            user_question, 
                            research_data, 
                            company_info,
                            st.session_state.chat_history
                        )
                        st.write(answer)
                
                # å±¥æ­´ã«è¿½åŠ ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’æ›´æ–°ï¼‰
                st.session_state.chat_history.append((user_question, answer))
            
            # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ãƒªã‚»ãƒƒãƒˆãƒœã‚¿ãƒ³
            if st.button("ğŸ—‘ï¸ ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆ"):
                st.session_state.chat_history = []
                st.success("ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸã€‚")
                
        # æ–°ã—ã„åˆ†æã‚’é–‹å§‹ã™ã‚‹ãƒœã‚¿ãƒ³
        if st.button("ğŸ”„ æ–°ã—ã„åˆ†æã‚’é–‹å§‹"):
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢
            for key in ['analysis_results', 'chat_history', 'analysis_context']:
                if key in st.session_state:
                    del st.session_state[key]
            st.rerun()

    # ãƒ•ãƒƒã‚¿ãƒ¼
    st.markdown("---")
    st.markdown(
        """
        <div style="text-align: center; color: #666;">
            ğŸ” AIä¼æ¥­åˆ†æã‚·ã‚¹ãƒ†ãƒ  | Powered by OpenAI GPT-4o-mini
        </div>
        """, 
        unsafe_allow_html=True
    )

    def extract_structured_ir_data(self, ir_data, company_fundamentals):
        """IRæƒ…å ±ã‹ã‚‰æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        st.info("ğŸ“Š IRæƒ…å ±ã‹ã‚‰æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºä¸­...")
        
        structured_ir = {
            'financial_data': {
                'revenue': {'value': None, 'source': '', 'year': ''},
                'operating_profit': {'value': None, 'source': '', 'year': ''},
                'employees': {'value': None, 'source': '', 'year': ''},
                'segments': []
            },
            'business_strategy': {
                'medium_term_plan': '',
                'key_initiatives': [],
                'growth_targets': []
            },
            'competitive_landscape': {
                'market_position': '',
                'competitive_advantages': [],
                'disclosed_competitors': []
            },
            'data_quality': {
                'ir_documents_found': len(ir_data),
                'data_completeness': 0,
                'reliability_score': 0
            }
        }
        
        # IRæ–‡æ›¸ã‹ã‚‰æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        for ir_item in ir_data:
            self.extract_financial_metrics(ir_item, structured_ir)
            self.extract_business_strategy(ir_item, structured_ir)
            self.extract_competitive_info(ir_item, structured_ir)
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡
        structured_ir = self.assess_ir_data_quality(structured_ir)
        
        return structured_ir
    
    def extract_financial_metrics(self, ir_item, structured_ir):
        """IRæƒ…å ±ã‹ã‚‰è²¡å‹™æŒ‡æ¨™ã‚’æŠ½å‡º"""
        title = ir_item.get('title', '')
        snippet = ir_item.get('snippet', '')
        text = (title + ' ' + snippet).lower()
        
        # å£²ä¸Šé«˜ã®æŠ½å‡º
        import re
        
        # å£²ä¸Šé«˜ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå…†å††ã€å„„å††ã€ç™¾ä¸‡å††ï¼‰
        revenue_patterns = [
            r'å£²ä¸Šé«˜[ï¼š:\s]*(\d+[,ï¼Œ]?\d*)[^\d]*å„„å††',
            r'å£²ä¸Š[ï¼š:\s]*(\d+[,ï¼Œ]?\d*)[^\d]*å„„å††',
            r'revenue[ï¼š:\s]*(\d+[,ï¼Œ]?\d*)[^\d]*å„„å††'
        ]
        
        for pattern in revenue_patterns:
            match = re.search(pattern, text)
            if match and not structured_ir['financial_data']['revenue']['value']:
                structured_ir['financial_data']['revenue'] = {
                    'value': match.group(1).replace(',', '').replace('ï¼Œ', '') + 'å„„å††',
                    'source': ir_item.get('document_type', 'IRè³‡æ–™'),
                    'year': self.extract_year_from_text(text)
                }
                break
        
        # å–¶æ¥­åˆ©ç›Šã®æŠ½å‡º
        profit_patterns = [
            r'å–¶æ¥­åˆ©ç›Š[ï¼š:\s]*(\d+[,ï¼Œ]?\d*)[^\d]*å„„å††',
            r'å–¶æ¥­ç›Š[ï¼š:\s]*(\d+[,ï¼Œ]?\d*)[^\d]*å„„å††'
        ]
        
        for pattern in profit_patterns:
            match = re.search(pattern, text)
            if match and not structured_ir['financial_data']['operating_profit']['value']:
                structured_ir['financial_data']['operating_profit'] = {
                    'value': match.group(1).replace(',', '').replace('ï¼Œ', '') + 'å„„å††',
                    'source': ir_item.get('document_type', 'IRè³‡æ–™'),
                    'year': self.extract_year_from_text(text)
                }
                break
        
        # å¾“æ¥­å“¡æ•°ã®æŠ½å‡º
        employee_patterns = [
            r'å¾“æ¥­å“¡æ•°[ï¼š:\s]*(\d+[,ï¼Œ]?\d*)[^\d]*äºº',
            r'ç¤¾å“¡æ•°[ï¼š:\s]*(\d+[,ï¼Œ]?\d*)[^\d]*äºº'
        ]
        
        for pattern in employee_patterns:
            match = re.search(pattern, text)
            if match and not structured_ir['financial_data']['employees']['value']:
                structured_ir['financial_data']['employees'] = {
                    'value': match.group(1).replace(',', '').replace('ï¼Œ', '') + 'äºº',
                    'source': ir_item.get('document_type', 'IRè³‡æ–™'),
                    'year': self.extract_year_from_text(text)
                }
                break
    
    def extract_business_strategy(self, ir_item, structured_ir):
        """IRæƒ…å ±ã‹ã‚‰äº‹æ¥­æˆ¦ç•¥ã‚’æŠ½å‡º"""
        title = ir_item.get('title', '')
        snippet = ir_item.get('snippet', '')
        text = title + ' ' + snippet
        
        # ä¸­æœŸçµŒå–¶è¨ˆç”»ã®æŠ½å‡º
        if 'ä¸­æœŸçµŒå–¶è¨ˆç”»' in text or 'ä¸­æœŸè¨ˆç”»' in text or 'çµŒå–¶æˆ¦ç•¥' in text:
            if not structured_ir['business_strategy']['medium_term_plan']:
                structured_ir['business_strategy']['medium_term_plan'] = snippet[:200] + '...'
        
        # é‡ç‚¹æ–½ç­–ã®æŠ½å‡º
        strategy_keywords = ['DXæ¨é€²', 'ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–', 'AIæ´»ç”¨', 'æ–°è¦äº‹æ¥­', 'ã‚°ãƒ­ãƒ¼ãƒãƒ«å±•é–‹', 'M&A']
        for keyword in strategy_keywords:
            if keyword in text and keyword not in structured_ir['business_strategy']['key_initiatives']:
                structured_ir['business_strategy']['key_initiatives'].append(keyword)
    
    def extract_competitive_info(self, ir_item, structured_ir):
        """IRæƒ…å ±ã‹ã‚‰ç«¶åˆæƒ…å ±ã‚’æŠ½å‡º"""
        title = ir_item.get('title', '')
        snippet = ir_item.get('snippet', '')
        text = title + ' ' + snippet
        
        # å¸‚å ´ãƒã‚¸ã‚·ãƒ§ãƒ³ã®æŠ½å‡º
        if 'ã‚·ã‚§ã‚¢' in text or 'ãƒã‚¸ã‚·ãƒ§ãƒ³' in text or 'å¸‚å ´' in text:
            if not structured_ir['competitive_landscape']['market_position']:
                structured_ir['competitive_landscape']['market_position'] = snippet[:150] + '...'
        
        # ç«¶äº‰å„ªä½æ€§ã®æŠ½å‡º
        advantage_keywords = ['æŠ€è¡“åŠ›', 'ãƒ–ãƒ©ãƒ³ãƒ‰åŠ›', 'ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯', 'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹', 'ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ']
        for keyword in advantage_keywords:
            if keyword in text and keyword not in structured_ir['competitive_landscape']['competitive_advantages']:
                structured_ir['competitive_landscape']['competitive_advantages'].append(keyword)
    
    def extract_year_from_text(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å¹´åº¦ã‚’æŠ½å‡º"""
        import re
        year_match = re.search(r'20(\d{2})', text)
        return f"20{year_match.group(1)}å¹´" if year_match else "ä¸æ˜"
    
    def assess_ir_data_quality(self, structured_ir):
        """IRæƒ…å ±ã®å“è³ªè©•ä¾¡"""
        completeness_score = 0
        total_fields = 8  # ä¸»è¦ãªæƒ…å ±é …ç›®æ•°
        
        # è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯
        if structured_ir['financial_data']['revenue']['value']:
            completeness_score += 1
        if structured_ir['financial_data']['operating_profit']['value']:
            completeness_score += 1
        if structured_ir['financial_data']['employees']['value']:
            completeness_score += 1
        
        # æˆ¦ç•¥æƒ…å ±ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯
        if structured_ir['business_strategy']['medium_term_plan']:
            completeness_score += 1
        if structured_ir['business_strategy']['key_initiatives']:
            completeness_score += 1
        
        # ç«¶åˆæƒ…å ±ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯
        if structured_ir['competitive_landscape']['market_position']:
            completeness_score += 1
        if structured_ir['competitive_landscape']['competitive_advantages']:
            completeness_score += 1
        
        # IRæ–‡æ›¸æ•°ã«ã‚ˆã‚‹ä¿¡é ¼æ€§ã‚¹ã‚³ã‚¢
        ir_count = structured_ir['data_quality']['ir_documents_found']
        if ir_count >= 3:
            completeness_score += 1
        
        structured_ir['data_quality']['data_completeness'] = (completeness_score / total_fields) * 100
        structured_ir['data_quality']['reliability_score'] = min(ir_count * 20, 100)
        
        return structured_ir
    
    def validate_data_reliability(self, data_item, source_type, company_fundamentals):
        """ãƒ‡ãƒ¼ã‚¿ã®ä¿¡é ¼æ€§ã‚’æ¤œè¨¼"""
        reliability_score = 0
        validation_notes = []
        
        # ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã«ã‚ˆã‚‹åŸºç¤ã‚¹ã‚³ã‚¢
        source_scores = {
            'IRé–‹ç¤º': 90,
            'æ±ºç®—çŸ­ä¿¡ãƒ»èª¬æ˜è³‡æ–™': 85,
            'æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸': 95,
            'ä¸­æœŸçµŒå–¶è¨ˆç”»ãƒ»æˆ¦ç•¥è³‡æ–™': 80,
            'ä¼æ¥­å…¬å¼ã‚µã‚¤ãƒˆ': 70,
            'å¤–éƒ¨èª¿æŸ»ãƒ¬ãƒãƒ¼ãƒˆ': 60,
            'æ—¥æœ¬çµŒæ¸ˆæ–°è': 75,
            'æ±æ´‹çµŒæ¸ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³': 65,
            'æ¨å®š': 20
        }
        
        reliability_score = source_scores.get(source_type, 30)
        
        # æ•°å€¤ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        if isinstance(data_item, dict) and 'value' in data_item:
            value = data_item['value']
            
            # å£²ä¸Šè¦æ¨¡ã®å¸¸è­˜çš„ç¯„å›²ãƒã‚§ãƒƒã‚¯
            if 'å„„å††' in str(value):
                try:
                    amount = float(str(value).replace('å„„å††', '').replace(',', ''))
                    if amount > 100000:  # 10å…†å††è¶…ã¯è¦æ³¨æ„
                        reliability_score -= 30
                        validation_notes.append("å£²ä¸Šè¦æ¨¡ãŒç•°å¸¸ã«å¤§ãã„å¯èƒ½æ€§")
                    elif amount < 1:  # 1å„„å††æœªæº€ã¯è¦æ³¨æ„
                        reliability_score -= 20
                        validation_notes.append("å£²ä¸Šè¦æ¨¡ãŒç•°å¸¸ã«å°ã•ã„å¯èƒ½æ€§")
                except:
                    reliability_score -= 40
                    validation_notes.append("æ•°å€¤å½¢å¼ãŒä¸æ­£")
        
        # ä¼æ¥­è¦æ¨¡ã¨ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        if company_fundamentals['primary_business'] == 'äººæã‚µãƒ¼ãƒ“ã‚¹ãƒ»HR Tech':
            # äººæã‚µãƒ¼ãƒ“ã‚¹æ¥­ç•Œã®ä¸€èˆ¬çš„ç¯„å›²
            if 'ä½å®…' in str(data_item) or 'ä¸å‹•ç”£é–‹ç™º' in str(data_item):
                reliability_score -= 50
                validation_notes.append("æ¥­ç•Œåˆ†é¡ã¨ã®ä¸æ•´åˆ")
        
        return {
            'reliability_score': max(0, reliability_score),
            'validation_notes': validation_notes,
            'source_type': source_type
        }
    
    def create_data_source_hierarchy(self, structured_ir, external_data, company_fundamentals):
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹éšå±¤ã®ä½œæˆ"""
        hierarchical_data = {
            'tier_1_ir_disclosed': {},
            'tier_2_official_company': {},
            'tier_3_external_verified': {},
            'tier_4_estimated': {},
            'quality_assessment': {
                'overall_reliability': 0,
                'data_conflicts': [],
                'missing_critical_data': []
            }
        }
        
        # Tier 1: IRé–‹ç¤ºæƒ…å ±
        if structured_ir['financial_data']['revenue']['value']:
            hierarchical_data['tier_1_ir_disclosed']['revenue'] = {
                'value': structured_ir['financial_data']['revenue']['value'],
                'source': structured_ir['financial_data']['revenue']['source'],
                'validation': self.validate_data_reliability(
                    structured_ir['financial_data']['revenue'], 
                    structured_ir['financial_data']['revenue']['source'],
                    company_fundamentals
                )
            }
        
        if structured_ir['financial_data']['operating_profit']['value']:
            hierarchical_data['tier_1_ir_disclosed']['operating_profit'] = {
                'value': structured_ir['financial_data']['operating_profit']['value'],
                'source': structured_ir['financial_data']['operating_profit']['source'],
                'validation': self.validate_data_reliability(
                    structured_ir['financial_data']['operating_profit'], 
                    structured_ir['financial_data']['operating_profit']['source'],
                    company_fundamentals
                )
            }
        
        # Tier 3: å¤–éƒ¨æ¤œè¨¼æ¸ˆã¿æƒ…å ±
        for ext_item in external_data:
            hierarchical_data['tier_3_external_verified'][f"external_{len(hierarchical_data['tier_3_external_verified'])}"] = {
                'title': ext_item.get('title', ''),
                'source': ext_item.get('source', ''),
                'snippet': ext_item.get('snippet', ''),
                'validation': self.validate_data_reliability(
                    ext_item, 
                    ext_item.get('source', 'å¤–éƒ¨è¨˜äº‹'),
                    company_fundamentals
                )
            }
        
        # å…¨ä½“å“è³ªè©•ä¾¡
        all_scores = []
        for tier in ['tier_1_ir_disclosed', 'tier_3_external_verified']:
            for item in hierarchical_data[tier].values():
                if 'validation' in item:
                    all_scores.append(item['validation']['reliability_score'])
        
        hierarchical_data['quality_assessment']['overall_reliability'] = sum(all_scores) / len(all_scores) if all_scores else 0
        
        # é‡è¦ãƒ‡ãƒ¼ã‚¿ã®æ¬ æãƒã‚§ãƒƒã‚¯
        critical_data = ['revenue', 'operating_profit', 'employees']
        for data_key in critical_data:
            if data_key not in hierarchical_data['tier_1_ir_disclosed']:
                hierarchical_data['quality_assessment']['missing_critical_data'].append(data_key)
        
        return hierarchical_data

if __name__ == "__main__":
    main()