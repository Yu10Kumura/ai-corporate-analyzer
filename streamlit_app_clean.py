#!/usr/bin/env python3
"""
ğŸ¢ ä¼æ¥­ãƒ“ã‚¸ãƒã‚¹åˆ†æã‚·ã‚¹ãƒ†ãƒ  - Streamlit Webç‰ˆ
EVPæ©Ÿèƒ½ã‚’å‰Šé™¤ã—ã€ä¼æ¥­ã®ãƒ“ã‚¸ãƒã‚¹åˆ†æã«ç‰¹åŒ–ã—ãŸAIã‚·ã‚¹ãƒ†ãƒ 
"""

import streamlit as st
import os
import json
import datetime
import time
import requests
from pathlib import Path
from openai import OpenAI
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from datetime import datetime, timedelta

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ğŸ¢ ä¼æ¥­ãƒ“ã‚¸ãƒã‚¹åˆ†æã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# è¨­å®šå®šæ•°
CONFIG = {
    'MAX_CRAWL_DEPTH': 2,
    'DATE_LIMIT_YEARS': 3,
    'MAX_SOURCES': 10,
    'USER_AGENT': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
}

class IRDataCollector:
    """IRæƒ…å ±åé›†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆç°¡ç´ åŒ–ç‰ˆï¼‰"""
    
    def __init__(self, company_domain):
        self.company_domain = company_domain
        self.session = requests.Session()
        self.session.headers.update({'User-Agent': CONFIG['USER_AGENT']})
    
    def collect_basic_ir_info(self):
        """åŸºæœ¬çš„ãªIRæƒ…å ±ã‚’åé›†"""
        try:
            ir_patterns = [
                f"https://{self.company_domain}/ir/",
                f"https://{self.company_domain}/investor/",
                f"https://ir.{self.company_domain}/"
            ]
            
            collected_data = []
            
            for url in ir_patterns:
                try:
                    st.info(f"ğŸ” IRæƒ…å ±ã‚’æ¢ç´¢ä¸­: {url}")
                    response = self.session.get(url, timeout=10)
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        content = response.text[:2000]  # æœ€åˆã®2000æ–‡å­—
                        
                        collected_data.append({
                            'url': url,
                            'content': content,
                            'title': soup.title.string if soup.title else f"IRæƒ…å ± - {self.company_domain}"
                        })
                        
                        st.success(f"âœ… IRæƒ…å ±ã‚’å–å¾—: {url}")
                        break  # 1ã¤æˆåŠŸã—ãŸã‚‰çµ‚äº†
                        
                except requests.exceptions.RequestException:
                    continue
            
            return collected_data
            
        except Exception as e:
            st.warning(f"âš ï¸ IRæƒ…å ±ã®åé›†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []

class BusinessAnalyzer:
    """ä¼æ¥­ãƒ“ã‚¸ãƒã‚¹åˆ†æã‚·ã‚¹ãƒ†ãƒ ï¼ˆäº‹æ¥­åˆ†æç‰¹åŒ–ï¼‰"""
    
    def __init__(self):
        self.client = OpenAI(api_key=self._get_api_key())
        
    def _get_api_key(self):
        """APIã‚­ãƒ¼ã‚’å–å¾—"""
        # Streamlit Cloudã®å ´åˆ
        if hasattr(st, 'secrets') and 'OPENAI_API_KEY' in st.secrets:
            return st.secrets["OPENAI_API_KEY"]
        
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            st.error("âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            st.stop()
        
        return api_key
    
    def create_analysis_prompt(self, company_name, ir_data=None):
        """çµ±ä¸€ã•ã‚ŒãŸåˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆäº‹æ¥­åˆ†æã®ã¿ï¼‰"""
        
        ir_content = ""
        if ir_data:
            ir_content = "\n".join([
                f"ã€IRæƒ…å ±ã€‘: {item['title']}\nå†…å®¹: {item['content'][:800]}...\n"
                for item in ir_data[:3]
            ])
        
        prompt = f"""
ã‚ãªãŸã¯ä¼æ¥­ãƒ“ã‚¸ãƒã‚¹åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ä¼æ¥­ã«ã¤ã„ã¦ã€äº‹æ¥­åˆ†æã®ã¿ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

ã€åˆ†æå¯¾è±¡ä¼æ¥­ã€‘: {company_name}

ã€åˆ©ç”¨å¯èƒ½ãªæƒ…å ±ã€‘:
{ir_content if ir_content else "å…¬é–‹æƒ…å ±ã«åŸºã¥ãåˆ†æã‚’å®Ÿè¡Œ"}

ã€é‡è¦åˆ¶ç´„ã€‘:
1. EVPåˆ†æã¯å®Ÿè¡Œã—ãªã„
2. äº‹æ¥­åˆ†æã®4é …ç›®ã®ã¿ã«ç‰¹åŒ–
3. æ¨æ¸¬ã®å ´åˆã¯æ˜ç¢ºã«ã€Œæ¨å®šã€ã¨è¨˜è¼‰
4. ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ã€Œæƒ…å ±ä¸è¶³ã€ã¨æ˜è¨˜
5. å‡ºå…¸ãŒã‚ã‚‹å ´åˆã¯å¿…ãšæ˜è¨˜

ã€åˆ†æé …ç›®ã€‘:
1. industry_market: æ¥­ç•Œãƒ»å¸‚å ´åˆ†æï¼ˆæ‰€å±æ¥­ç•Œã€å¸‚å ´è¦æ¨¡ã€æˆé•·æ€§ï¼‰
2. market_position: æ¥­ç•Œå†…ãƒã‚¸ã‚·ãƒ§ãƒ³ï¼ˆå£²ä¸Šè¦æ¨¡ã€å¸‚å ´ã‚·ã‚§ã‚¢ã€ç«¶åˆæ¯”è¼ƒï¼‰
3. differentiation: ç‹¬è‡ªæ€§ãƒ»å·®åˆ¥åŒ–è¦å› ï¼ˆæŠ€è¡“åŠ›ã€ãƒ–ãƒ©ãƒ³ãƒ‰åŠ›ã€äº‹æ¥­ãƒ¢ãƒ‡ãƒ«ï¼‰
4. business_portfolio: äº‹æ¥­ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªåˆ†æï¼ˆä¸»åŠ›äº‹æ¥­ã€åç›Šæ§‹é€ ã€äº‹æ¥­é ˜åŸŸï¼‰

ã€å‡ºåŠ›å½¢å¼ã€‘:
ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š

{{
  "business_analysis": {{
    "industry_market": "è©³ç´°ãªæ¥­ç•Œãƒ»å¸‚å ´åˆ†æï¼ˆ800æ–‡å­—ç¨‹åº¦ï¼‰",
    "market_position": "æ¥­ç•Œå†…ãƒã‚¸ã‚·ãƒ§ãƒ³ã®åˆ†æï¼ˆ800æ–‡å­—ç¨‹åº¦ï¼‰", 
    "differentiation": "ç‹¬è‡ªæ€§ãƒ»å·®åˆ¥åŒ–è¦å› ã®åˆ†æï¼ˆ800æ–‡å­—ç¨‹åº¦ï¼‰",
    "business_portfolio": "äº‹æ¥­ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®åˆ†æï¼ˆ800æ–‡å­—ç¨‹åº¦ï¼‰"
  }},
  "analysis_metadata": {{
    "company_name": "{company_name}",
    "analysis_date": "{datetime.now().strftime('%Y-%m-%d')}",
    "data_sources": ["ä¼æ¥­å…¬å¼æƒ…å ±", "IRé–‹ç¤ºè³‡æ–™"],
    "reliability_score": 85
  }}
}}
"""
        return prompt
    
    def analyze_company(self, company_name, company_url=None):
        """ä¼æ¥­ã®äº‹æ¥­åˆ†æã‚’å®Ÿè¡Œ"""
        
        # IRæƒ…å ±åé›†
        ir_data = []
        if company_url:
            domain = self._extract_domain(company_url)
            if domain:
                collector = IRDataCollector(domain)
                ir_data = collector.collect_basic_ir_info()
        
        # åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        prompt = self.create_analysis_prompt(company_name, ir_data)
        
        try:
            st.info("ğŸ¤– AIåˆ†æã‚’å®Ÿè¡Œä¸­...")
            
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                max_tokens=4000
            )
            
            result_text = response.choices[0].message.content
            
            # JSONè§£æ
            try:
                # JSONéƒ¨åˆ†ã‚’æŠ½å‡º
                json_start = result_text.find('{')
                json_end = result_text.rfind('}') + 1
                json_text = result_text[json_start:json_end]
                
                result = json.loads(json_text)
                return result
                
            except json.JSONDecodeError:
                st.error("âŒ AIå¿œç­”ã®JSONè§£æã«å¤±æ•—ã—ã¾ã—ãŸ")
                st.code(result_text)
                return None
                
        except Exception as e:
            st.error(f"âŒ AIåˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None
    
    def _extract_domain(self, url):
        """URLã‹ã‚‰ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’æŠ½å‡º"""
        try:
            if not url.startswith(('http://', 'https://')):
                url = 'https://' + url
            
            parsed = urlparse(url)
            domain = parsed.netloc
            if domain.startswith('www.'):
                domain = domain[4:]
            return domain
        except:
            return None
    
    def save_results(self, company_name, analysis_data):
        """åˆ†æçµæœã‚’ä¿å­˜"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"business_analysis_{company_name}_{timestamp}.json"
            
            save_data = {
                "analysis_results": analysis_data,
                "generated_at": datetime.now().isoformat(),
                "system_info": {
                    "version": "2.0_business_focused",
                    "analysis_type": "business_only"
                }
            }
            
            # JSONæ–‡å­—åˆ—ã¨ã—ã¦è¿”ã™ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã¯ç’°å¢ƒã«ã‚ˆã‚Šç•°ãªã‚‹ï¼‰
            return filename, save_data
            
        except Exception as e:
            st.warning(f"âš ï¸ çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None, analysis_data

def main():
    st.title("ğŸ¢ ä¼æ¥­ãƒ“ã‚¸ãƒã‚¹åˆ†æã‚·ã‚¹ãƒ†ãƒ ")
    st.markdown("### ä¼æ¥­ã®äº‹æ¥­æˆ¦ç•¥ãƒ»ç«¶åˆåˆ†æãƒ»å¸‚å ´ãƒã‚¸ã‚·ãƒ§ãƒ³ã‚’è‡ªå‹•åˆ†æ")
    
    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
    with st.expander("â„¹ï¸ ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±", expanded=False):
        st.markdown("""
        **åˆ†æå†…å®¹:**
        - ğŸ“ˆ **æ¥­ç•Œãƒ»å¸‚å ´åˆ†æ**: æ‰€å±æ¥­ç•Œã¨å¸‚å ´è¦æ¨¡ãƒ»æˆé•·æ€§
        - ğŸ† **æ¥­ç•Œå†…ãƒã‚¸ã‚·ãƒ§ãƒ³**: å£²ä¸Šè¦æ¨¡ãƒ»å¸‚å ´ã‚·ã‚§ã‚¢ãƒ»ç«¶åˆæ¯”è¼ƒ
        - â­ **ç‹¬è‡ªæ€§ãƒ»å·®åˆ¥åŒ–**: æŠ€è¡“åŠ›ãƒ»ãƒ–ãƒ©ãƒ³ãƒ‰åŠ›ãƒ»äº‹æ¥­ãƒ¢ãƒ‡ãƒ«
        - ğŸ—ï¸ **äº‹æ¥­ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ª**: ä¸»åŠ›äº‹æ¥­ãƒ»åç›Šæ§‹é€ ãƒ»äº‹æ¥­é ˜åŸŸ
        
        **ç‰¹å¾´:**
        - EVPåˆ†æã‚’å»ƒæ­¢ã—ã€äº‹æ¥­åˆ†æã«ç‰¹åŒ–
        - IRæƒ…å ±ã«åŸºã¥ãå®¢è¦³çš„åˆ†æ
        - 800æ–‡å­—ã®è©³ç´°åˆ†æ
        - JSONå½¢å¼ã§ã®çµæœå‡ºåŠ›
        """)
    
    # å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
    with st.form("analysis_form"):
        col1, col2 = st.columns(2)
        
        with col1:
            company_name = st.text_input(
                "ğŸ¢ ä¼æ¥­å *", 
                placeholder="ä¾‹: ãƒˆãƒ¨ã‚¿è‡ªå‹•è»Šã€ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯ã€ãƒªã‚¯ãƒ«ãƒ¼ãƒˆ",
                help="åˆ†æå¯¾è±¡ã®ä¼æ¥­åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"
            )
        
        with col2:
            company_url = st.text_input(
                "ğŸŒ ä¼æ¥­URLï¼ˆä»»æ„ï¼‰",
                placeholder="https://www.company.co.jp",
                help="ã‚ˆã‚Šè©³ç´°ãªåˆ†æã®ãŸã‚ã«ä¼æ¥­URLã‚’å…¥åŠ›ï¼ˆä»»æ„ï¼‰"
            )
        
        st.markdown("---")
        submitted = st.form_submit_button("ğŸ” äº‹æ¥­åˆ†æé–‹å§‹", type="primary", use_container_width=True)
    
    # åˆ†æå®Ÿè¡Œ
    if submitted:
        if not company_name:
            st.error("ğŸš¨ ä¼æ¥­åã¯å¿…é ˆå…¥åŠ›ã§ã™ã€‚")
            return
        
        analyzer = BusinessAnalyzer()
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        status_text.text("ğŸ” ä¼æ¥­æƒ…å ±ã‚’åé›†ä¸­...")
        progress_bar.progress(25)
        
        with st.spinner("ğŸ¤– AIåˆ†æä¸­... (30-60ç§’ç¨‹åº¦ãŠå¾…ã¡ãã ã•ã„)"):
            progress_bar.progress(50)
            analysis_result = analyzer.analyze_company(company_name, company_url)
            progress_bar.progress(80)
        
        if analysis_result:
            # çµæœä¿å­˜
            filename, save_data = analyzer.save_results(company_name, analysis_result)
            progress_bar.progress(100)
            status_text.text("âœ… åˆ†æå®Œäº†ï¼")
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
            st.session_state.analysis_results = {
                "data": analysis_result,
                "company_name": company_name,
                "save_data": save_data,
                "filename": filename
            }
        else:
            progress_bar.progress(0)
            status_text.text("âŒ åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ")
            st.error("âŒ åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸã€‚APIã‚­ãƒ¼ã¾ãŸã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    
    # çµæœè¡¨ç¤º
    if 'analysis_results' in st.session_state:
        results = st.session_state.analysis_results
        analysis_data = results["data"]
        company_name = results["company_name"]
        save_data = results["save_data"]
        filename = results["filename"]
        
        st.success("ğŸ‰ äº‹æ¥­åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        
        # åŸºæœ¬æƒ…å ±
        st.markdown("---")
        st.subheader("ğŸ“Š åˆ†æçµæœã‚µãƒãƒªãƒ¼")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ğŸ¢ ä¼æ¥­å", company_name)
        with col2:
            st.metric("ğŸ¯ åˆ†æã‚¿ã‚¤ãƒ—", "äº‹æ¥­åˆ†æç‰¹åŒ–")
        with col3:
            metadata = analysis_data.get('analysis_metadata', {})
            st.metric("ğŸ“ˆ ä¿¡é ¼æ€§ã‚¹ã‚³ã‚¢", f"{metadata.get('reliability_score', 'N/A')}/100")
        
        st.markdown("---")
        
        # ã‚¿ãƒ–å½¢å¼ã§çµæœè¡¨ç¤º
        tab1, tab2 = st.tabs(["ğŸ† äº‹æ¥­åˆ†æçµæœ", "ğŸ“„ JSONå‡ºåŠ›"])
        
        with tab1:
            st.subheader("ğŸ† ä¼æ¥­ãƒ“ã‚¸ãƒã‚¹åˆ†æ")
            
            business_labels = {
                "industry_market": "ğŸ“ˆ æ¥­ç•Œãƒ»å¸‚å ´åˆ†æ",
                "market_position": "ğŸ† æ¥­ç•Œå†…ãƒã‚¸ã‚·ãƒ§ãƒ³",
                "differentiation": "â­ ç‹¬è‡ªæ€§ãƒ»å·®åˆ¥åŒ–è¦å› ",
                "business_portfolio": "ğŸ—ï¸ äº‹æ¥­ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªåˆ†æ"
            }
            
            business_data = analysis_data.get('business_analysis', {})
            if business_data:
                for key, label in business_labels.items():
                    with st.expander(label, expanded=True):
                        content = business_data.get(key, "åˆ†æãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
                        st.write(content)
            else:
                st.warning("äº‹æ¥­åˆ†æãƒ‡ãƒ¼ã‚¿ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
        
        with tab2:
            st.subheader("ğŸ“„ JSONå½¢å¼ã®åˆ†æçµæœ")
            
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
            json_output = json.dumps(save_data, ensure_ascii=False, indent=2)
            st.download_button(
                label="ğŸ’¾ JSONçµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=json_output,
                file_name=f"business_analysis_{company_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json"
            )
            
            # JSONè¡¨ç¤º
            st.code(json_output, language="json")
        
        # æ–°ã—ã„åˆ†æãƒœã‚¿ãƒ³
        if st.button("ğŸ”„ æ–°ã—ã„åˆ†æã‚’é–‹å§‹"):
            if 'analysis_results' in st.session_state:
                del st.session_state.analysis_results
            st.rerun()

    # ãƒ•ãƒƒã‚¿ãƒ¼
    st.markdown("---")
    st.markdown(
        """
        <div style="text-align: center; color: #666;">
            ğŸ“Š ä¼æ¥­ãƒ“ã‚¸ãƒã‚¹åˆ†æã‚·ã‚¹ãƒ†ãƒ  v2.0 | Powered by OpenAI GPT-4o-mini
        </div>
        """, 
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()